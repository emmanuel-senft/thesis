%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Background} \label{chap:background}
\glsresetall

This chapter describes social \gls{hri} and presents related research in agent and robot control. The first section introduces the different fields of application of social \gls{hri} and then draws from them requirements for controlling a robot interacting with humans. The second section provides the current state of the art in robot control for \gls{hri} and analyses it through the requirements presented in Section \ref{ssec:back_constraints}. And finally, the third section presents \gls{iml}, an alternative method to teach agents how to interact and how it could be applied to \gls{hri}.

\section{Social Human-Robot Interaction}

\subsection{Fields of application}

\gls{hri} covers the full spectrum of interactions between humans and robots ranging from physical assistance and teleoperation to social companions. However, as this thesis is focused on teaching robots to interact socially with humans, we used the following criteria on the interactions to select the subfields of \gls{hri} relevant to our research:
\begin{itemize}
\item presence of an interaction between a robot and a human: both the human and the robot are influencing each other's behaviour
\item the social side of the interaction is key, the interaction involves a socially interactive robot as defined by \citet{Fong2003} (not physical human-robot interaction, such as an exoskeleton, physical rehabilitation or pure teleoperation as in robotic assisted surgery).
\end{itemize}

\subsubsection{Socially Assistive Robotics}

	\gls{sar} is a term coined by \cite{feil2005defining} and refers to a robot providing assistance to human users through social interaction and has been defined by \cite{tapus2007socially} as one of the grand challenges of robotics.
	
	One of the main applications of \gls{sar} is care for the elderlies. Ageing of population is a known challenge for today and the near future: the \cite{united2017world} reports than `population aged 60 or over is growing faster than all younger age groups'. This will decrease the support ratio (number of worker per retiree) forcing society to find ways to provide care to an increasing number of persons using a decreasing staff. Robots represent a unique opportunity to provide for this lacking workforce potentially allowing elderlies to stay at home rather than joining elderly care centres \citep{di2014web} or simply to support the nursing home staff \citep{wada2004effects}.
	
	%For this reason, robots for the elderlies is one of the major robotic market today and for the next decades as demonstrated by the number of projects all around the globe targeting this issue: Robot-Era \citep{bevilacqua2012robot}, Accompany project \citep{amirabdollahian2013accompany}, CompanionAble \citep{badii2009companionable}  and the Robear project in Japan    \footnote{\url{http://www.riken.jp/en/pr/press/2015/20150223_2/}} to cite     only a small subset of them.	
	
    However, the acceptance of these robots in elderly care facilities or at their homes is still a complex task. Multiple studies report a good acceptance of robot and positive effects on stress in home cares both for the elderly and the nursing staff \citep{wada2004effects,tamura2004entertainment}, but as mentioned by \citet{broadbent2009acceptance}, this acceptance could be increased by matching more closely the behaviours of the robots to the actual patients' needs and expectations.
		
	The second main application of \gls{sar} is \gls{rat}. In addition to providing social support, robots are also involved in therapies, following a patient during their rehabilitation or treatment to improve their health and their acceptance in society or when recovering from an accident. In therapies, robots have first been used as physical platform to help patient in rehabilitation therapy in the 80s \citep{harwin1988robot}. They were primally used as mechatronic tools helping humans to accomplish repetitive task. But in the late 90s, robots have started to be used for their social capabilities. For example the AuRoRA Project \citep{dautenhahn1999robots} started in 1998 to explore the use of robot as therapeutic tools for children with \gls{asd}. Since AuRoRA, many projects have started all around the world to use robot to help patient with ASD, as shown by the review presented by \cite{diehl2012clinical} or more recently the \gls{dream} \citep{esteban2017build}. \gls{rat} is not limited to \gls{asd} only, the use of robots is also explored in hospitals, for example to support children with diabetes \citep{belpaeme2012multimodal}, to support elderly with dementia \citep{wada2005psychological} or to provide encouragements and monitor stroke \citep{mataric2007socially} or cardiac rehabilitation \citep{lara2017human}.	
	
\subsubsection{Education} 
	Social robots are also being used in education, supporting teachers to provide learning content to children. As presented by \cite{mubin2013review}, the robot can take multiple roles such as peer, tutor or tool. Mubin et al. stress that the intentions of the robotic community is not to replace teachers by robots, but provide them with a new form of technological teaching-aid. Nevertheless, \cite{verner2016science} presented positive results from a study using a tall humanoid robot to deliver a science lesson to children. Due to the lack of a social component when interacting with a tool robot, this overview will discuss the roles of robotic tutors and peers.
		
	\paragraph{Robotics tutors providing tailored teaching content in 1 to 1 interaction.} 
	Studies reported that individualised feedback increase the performance of students \citep{cohen1982educational,bloom19842}, but due to the large number of students supervised by a single teacher in classes today, tutoring is complex to apply. Robotic tutors could provide this powerful one to one tailored interaction not available in current classroom without increasing the workload on teachers and leading to higher learning gain for the children \citep{kanda2004interactive,leyzberg2012physical,kennedy2016social,gordon2016affective}. In addition to be used in the classroom, robots can support children at home and have been shown to elicit advantages over web or paper based instructions \citep{han2005educational}. However as pointed by \citet{kennedy2015robot}, the robot's social behaviours have to be carefully managed as a robot too social could decrease the learning for the children compared to a less social robot. 
	
	\paragraph{Peer robots learning alongside the children.}
	A peer robot does not mentor a child to learn new concepts, but learns alongside or from the children. Unlike other types of agents in education, peer robots have the opportunity to fit new roles non-existent in education today, the role of care-receiver rather than care-giver \cite{tanaka2012children}. For example, in Co-Writer \citep{hood2015children}, the child has to teach a robot how to write, and as the child demonstrates correct handwriting to the robot, they improves their own. Peer robots are able to leverage the concept of learning by teaching \citep{frager1970learning} in a way hardly matched by humans. The robot can take the role of a less knowledgeable agent with endless patience and encourage the student to perform repetitive tasks such as handwriting and improve, where an adult would not be a believable agent requiring learning and younger students might not have the compliance and the patience to learn from another child.
	
\subsubsection{Search and rescue, and military} 
	
    Robots are already deployed in the real world, outside of labs and used during search and rescue missions: after a natural or artificial catastrophe, robots have been sent to analyse the damage area and report or rescue the surviving victims of the incident \citep{murphy2008search}. During these missions, robots have to interact socially with two kinds of human partners: the survivors and the rescue team. In both cases the social component of the interaction is key: the survivor is probably in a shocked state and the robot could be the only link they have with the external world after the accident. In this case, a social response is expected from the robot and it has to be carefully controlled. On the other side, the rescue team monitoring the robots is under high pressure to act quickly and faces traumatic events too. Even if the robot does not display social behaviour, rescuers interacting with it might develop some feeling toward the robots they are using during these tense moments and this has to be taken into account when designing the robot and its behaviour \citep{fincannon2004evidence}.
	
    Similar human behaviours (emotional bonding with a robot) have also been observed in the army. Robots have been deployed as teleoperated drones and ground units alongside soldier to complete scouting or demining tasks. By interacting with a robot for extensive periods, soldiers developed feelings toward this robot they used in a daily basis: taking pictures with it and introducing it to their friends. This relation have gone as far as soldier risking their own life to in order to save the robot used by their squad \citep{singer2009wired}. This demonstrates that the social side of the robot has to be carefully managed to prevent the robot to have an opposite effect that the one desired: preventing the waste of human lives.
		
\subsubsection{Hospitality and Entertainment} 
	
	Robots are also interacting with humans in hotels around the world. The Relay robot (Savioke\footnote{\url{http://www.savioke.com/}}) delivers commands directly to the guests' room. Whilst the social interaction is still minimal today, these robots interact everyday with humans and evoke social reactions from them. On the research side, scientists have explored a robot as a receptionist in a hall at Carnegie Melon University \citep{gockley2005designing} and robots as museums and exhibitions guide since the late 90s \citep{thrun1999minerva,burgard1999experiences} and now, continue to explore how to improve the social interaction between tourists and these guide robots \citep{bennewitz2005towards}. Similarly, robots guide and advice humans in shops and shopping mall. Long term studies explored how humans perceive and interact with robots in this environment \citep{kanda2009affective} and how robots should behave with clients \citep{kanda2008will}.

    Robots have also entered homes and family circles: twenty years ago, Sony created Aibo, a robotic dog to be used as pet in Japanese families and a new version is released in 2018. An analysis of online discussions of owners published 6 years after Aibo's first introduction gives insights on the relationship that owners created with their robots \citep{friedman2003hardware}. For example 42\% of the members assigned intentionality to the robot, like preferences, emotions or even feelings. Similar behaviours have also been observed when the robot is not presented as a pet, but even just a tool. \cite{fink2013living} report that one of their participants was worried that their Roomba would feel lonely when they would be in holidays. 
    
    More recently, the Pepper robot has been sold to family in Japan, however, as of early 2018 no study in English has reported results of the interaction with families or long term use and acceptance.

\subsubsection{Collaborative Robots in Industry}

	In industry, robots used to be locked behind cages to prevent humans to interact with them and getting hurt. However recently, social robots such as Baxter \citep{guizzo2012rethink} have been designed to collaborate with humans and share the same workspace interacting physically and socially with factory workers.	As these robots need to be safe to human interacting with them, they are often softly actuated, using active or passive compliance to avoid the risk related to collision between a stiff robot and a human. Additionally, when collaborating with people, the legibility of motion is fundamental. To interact efficiently and safely with humans, robots need to make their intentions clear to humans surrounding them \citep{dragan2013legibility} and reciprocally, they also need to interpret human social cue to infer their goals and intentions.
	
	Beyond the safety and legibility, another key challenge in \gls{hrc} is task assignment: if a goal has to be achieved by a human-robot team, the repartition of tasks should be carefully managed to  optimise the end result in term of task performance, but also to ensure comfort for the human. Implicit rules describing the role and behaviours of each participant have to be taken into account in a human-robot collaboration context, and as such, the task repartition system should be aware of them and follow them \citep{montreuil2007planning}. More recently, work done in the 3\textsuperscript{rd} Hand project \footnote{\url{http://3rdhandrobot.eu/}} explored how a robot should support a human in a collaborative assembly task by adapting its behaviour to this human's personal preferences \citep{munzer2017efficient}.
	
	A last challenged faced by \gls{ai} with the recent advances in \gls{ml}, and especially with the omnipresence of neural networks and deep learning, is \gls{xai}. As agents learning to interact will make mistakes and behave unexpectedly from times to times, they have to be able to provide explanations for these errors in a way understandable by humans. This challenge also applies to \gls{hri}, especially in \gls{hrc} where both humans and robots aim to collaborate to complete a task. \cite{hayes2017improving} propose to achieve transparency through policy explanation, by providing robots with a way to answer questions and explain their behaviour by self observation and logic deduction.

\subsection{Requirements on Robots Interacting with Humans} \label{ssec:back_constraints}

    The previous section demonstrated that robots are already interacting with sensitive populations: young children, elderlies, persons with handicap or in a stressful situations (victims of catastrophes, soldier or persons requiring healthcare for example). And, as pointed previously in the case of robots for the elderlies, this number of human-robot interactions is likely to rise for all the other categories too. In this context, failure to meet expectations or lack of social norms or awareness might lead to physical injuries to surrounding humans or potentially to offence, anger, frustration or boredom. As such, the behaviour of robots interacting with humans need to be carefully managed to meet expectations and robots need to behave in socially acceptable manner avoiding as much as possible any inappropriate or confusing behaviours.
    
    These undesired behaviours may come from different origins: lack of sensory capabilities to identify necessary environmental features, lack of knowledge to interpret human behaviours appropriately, failure to convey intentions, impossibility to execute the required action or incorrect action policy. Due to the wide range of origins of these potential social faux-pas, this research focuses only on the last point, obtaining a correct action policy: assuming a set of inputs, how a robot should select the most correct action. The other issues are either orthogonal and would lead to failure even with an `optimal' action policy as external factors prevent the robot to solve a problem or can be handled by having a better action policy, generalising more efficiently, selecting suboptimal actions when the optimal ones are not available, or reacting correctly to human behaviours without having to interpret them on a higher level. 
    
    Appropriate actions are highly dependent on the interaction context: they could aim to match users' expectations of a robot behaviour or complete a specific task; and actions correct in a context might be inappropriate in another one. Nonetheless, the intention behind being \textit{appropriate} here is that actions executed should be guaranteed not to present risks for the humans involved in the interaction, for example, preventing physical harm or mental distress, while helping the robot to move toward its goal and achieving its objectives.

    Additionally, interactions with humans `in the wild' \citep{belpaeme2012multimodal} do not happen in well defined environments or rigid laboratory setups: in the real world, robots have to interact in diverse environments, with a large number of different persons, during extended periods of time or with initial incomplete or incorrect knowledge. As such, the action policy also needs to be adaptable to the context, to the users and evolve over time. Robots need to be adaptive, to react to changing environments, cover a larger field of application and improve their action policy over time.

    Lastly, in many cases today, interactive robots are not autonomous but partially controlled by a human operator. We argue that to have a real \gls{hri}, the robot needs to be as autonomous as possible. As pointed out in \citet{baxter2016characterising}, by relying too much on a human to control a robot, we are shifting from a human-robot to a human-human interaction using a robot as proxy. As a community, \gls{hri} should strive toward more autonomy for robots interacting with humans.
    
    We define three axes to analyse a robot behaviour and evaluate how suited its behaviour is to interact with humans:
    \begin{itemize}
    	\item Appropriateness of actions
    	\item Adaptivity
    	\item Autonomy
    \end{itemize}
    And we will use these axes to analyse current robotic controller types in Section \ref{sec:back_behaviour}.    
    
    \gls{hri} being a large field, other research axes are equally important such as the complexity or the depth of the interaction, the constraints put on the environment, the ability of the robot to set its own goals, the dependence and knowledge of social rules or the range of application to cite only a few. However, these axes are more influenced by the goal and the context of the specific human-robot interaction taking place rather than the action policy itself, so this research focuses on the three axis mentioned previously. Additionally, a robot scoring high on all the axes presented before should be able to safely and autonomously learn to interact in deeper and more complex interactions and learn to extend its abilities beyond the ones it initially started with.

	To be able to sustain meaningful social interactions, we argue that a robot should score highly in the three axes presented before, following these three principles:
	\begin{enumerate}
		\item Only execute appropriate actions.
		\item Have a high level of adaptivity and learn.
		\item Have a high level of autonomy.
	\end{enumerate}

\subsubsection{Appropriateness of Actions} \label{ssec:appropriateness} %Similar to safety
    As argued previously, much of social human-robot interaction takes place in  stressful or at least sensitive environments where humans have particular expectations about the robot behaviour or needs from the robot. Additionally, even in less critical situations, human-human interactions are subject to a large set of social norms and conventions resulting from precise expectations of the interacting partners \citep{sherif1936psychology}. And some of these expectations are also transferred to interactions with robots \citep{bartneck2004design}.
	
    Failing to produce appropriate actions, for example by not matching the users' expectations, may have a negative impact on the interaction, potentially compromising future interactions if the human feel disrespected, confused or annoyed. Similarly failing to behave appropriately can harm the persons interacting with the robot: not reminding an elderly to take their medication, not taking into account the state of mind of survivors after a disaster or behaving inconsistently with children with \gls{asd} might lead to dramatic consequences.  We argue that robots require a way to ensure that all the actions they execute do not present risks to the humans involved in the interaction while moving the robot closer to its goal.
	
	%TODO: check if worth mentioning Asimov
    %Surprise is an important element in social interaction, as it can revitalise the interaction and increase the engagement of users \citep{lemaignan2014dynamics}. As such, a robot does not need to be consistently fully predictable, but in any contexts of interaction the robot should not present danger for humans it is interacting with. This requirement is similar to the first law of Asimov \citep{asimov1942runaround}, but not as a law that the robot should follow (as it would be impossible to implement it that way, and that the point of Asimov was that these laws are not sufficient), but as a design principle for the humans developing a robot behaviour or as defined by \cite{murphy2009beyond}: ``A human may not deploy a robot without the human-robot work system meeting the highest legal and professional standards of safety and ethics.''
    
    Being able to behave appropriately is a tremendous challenge: real interactions involve a large sensory space, with the people participating often being unpredictable or at least highly stochastic. In addition, social interaction is grounded by a large number of often implicit norms, with expectations being highly dependent on the context of interaction. It seems unlikely that every possible case of interaction and reactions from the humans interacting with the robot will be anticipated beforehand, but regardless of the complexity of the interaction, the robot's behaviour needs to be constantly correct. Social robots need an action policy able to generate the appropriate reactions for the expected states and human behaviours, but also be able to manage uncertainty, selecting a correct action even when facing a sensory state not explicitly anticipated by the designers.
    
    For the review in the next section, the appropriateness of actions axis is a continuous spectrum characterising how much the system controlling the robot ensures that the robot acts in a safe and useful way for the users at any moment of the interaction. For example, a robot selecting its action randomly has a low appropriateness as no mechanism prevent the execution of unexpected or undesired action. On the other hand, a robot continuously selecting the action that a human expert would select has a high value as domain experts have the knowledge of which action is the correct one in this application domain.


\subsubsection{Adaptivity}	\label{ssec:adap}
	%TODO: Probably better, but not quite yet
    As stated before, humans are complex, indeterministic and unpredictable agents, as such an optimal robot behaviour is not likely to be known in advance and programmable by hand \citep{dautenhahn2004robots,argall2009survey}. End users will express behaviours not anticipated by the designers, the interaction environment is often not perfectly definable and the desired behaviour might also need to be customisable by or for the end user or evolve over time. For these reasons, robots interacting with people need to be able to update their action policy and improve their behaviour over time. We use the term \emph{adaptivity} to represent this ability to express a behaviour suited to different conditions and refine it over time. 
    
    While many studies in \gls{hri} do not use adaptive robots, to interact meaningfully outside of lab settings or scientific studies, robots need to possess this adaptivity to extend their range of application and improve their interactions with users.  
    
    We propose two components for this adaptivity: the basic component is the adaptivity to the environment, i.e. the generalisation of the behaviour (reacting accordingly to different types of inputs) and the second component is the adaptivity in time which is in essence learning (the possibility to enrich and refine the action policy over time). 
	
    The same robot might be expected to interact in different environments or the environment where it evolves might change with time. For example a robot used as an assistant for elderly people will have to interact in the home of the owner, but also have to follow the owner in the street or in a supermarket. In these different environments, different behaviour will be expected from the robot, as such to be able to behave accordingly, the robot has either to be programmed to be adaptive to the environment or be able to learn how to interact in these different locations.

    Additionally, in most of the application fields presented earlier, robots have to interact with a large number of users; and often, these interaction partners are not know in advance, have different roles and will change over time. In education the name and particularities of each child is cumbersome to specify in advance and an autonomous robot would also have to interact both with the students and the teacher, which will require different action policies. In entertainment or search and rescue, none of the users are known beforehand but providing a personalised behaviour adapted to the context may impact significantly the outcomes of the interaction. Adaptivity provides a way to identify the diverse users, their preferences and their needs, and adopt an action policy that suits the current user more specifically. %Additionally, when provided with learning, the robot can update its behaviour according to this user's desires, preferences and capabilities.
    
    When deployed in the wild, robots will be expected to interact over extensive periods of times with the same user, e.g. companion robots for the elderly, military robots for a squad or robots used in \gls{rat} \citep{leite2013social}. With these long-term social interactions, a key aspect in the engagement and efficiency is  the co-adaptation between the user and the robot. Learning allows the robot to tailor its behaviour to the current user and track the changes of preferences and desires occurring over long-term interaction. This adaptivity in time additionally let the robot learn from its errors and improve its action policy over time. 

	Furthermore, providing a robot with learning enables it to be used by non-experts in robotics, granting them a way to design their own human-robot interactions, and making use of their expertise and knowledge to have the robot interacting as they desire. This is crucial as many application of social robotics, such as \gls{rat}, happen in environment where non-technical persons possess the domain expertise required to ensure that the robot is efficient. And as stated by \cite{amershi2014power}, this reduces the requirements on expensive and time consuming rounds-trips between domain-experts and engineers and additionally decreases the risks of confusion between these two communities.

    For this review, adaptivity is a continuous scale ranging from no adaptivity at all: the robot has a linear script that it follows in all the interactions, to high adaptivity: the robot dynamically changes its action policy during an interaction, learns new actions and tasks and improves its action policy over time. 
    %As this adaptivity is over three axes, some robots can  have a high adaptivity in users (by adapting their behaviour to the actions of their interactants), but not in time (if the same inputs always trigger the same output), and not in space (if only one specific context of interaction is taken into account). In that case, the controller will receive a relatively low adaptivity rating.

\subsubsection{Autonomy}
	%TODO Reread
	
	Today, many experiments in \gls{hri} are conducted using a robot tele-operated by a human \citep{riek2012wizard,baxter2016characterising}, and whilst having a human controlling the robot presents many advantages (e.g. the human provides the knowledge and the adaptivity required and has sensing and reasoning capabilities not yet implemented on the robot), multiple reasons push us away from this type of interaction \citep{Thill2013}. It is not suited for deploying robots in the real world: it does not scale to interact for long periods of time or in many places, the human-robot interaction tends to become a human-human interaction using a robot as proxy \citep{baxter2016characterising} and it might introduce multiple biases in the robot behaviour \citep{howley2014effects}. For these reasons, among many, we argue that a robot used in social \gls{hri} should be as autonomous as possible. A limited human supervision could support the robot and be used to improve its behaviour, but the robot should not rely on humans for its action selection during the main parts of the interaction. 
		
    The third axis of this literature review is the autonomy. As stated by \citet{beer2014toward}, autonomy is organised following a spectrum of different levels of autonomy from no autonomy at all: a human is totally controlling the robot (doing sensory perception, analysis and action selection) to a full autonomy: the robot senses and acts on its environment without relying on human inputs. Levels exist between these extremes where a human and a robot share perception, decision or action: for example the robot can request information from a supervisor or the supervisor can override the action or goal being executed
    
%    Some systems use a hybrid combination of human control and autonomy. In these shared autonomy system, the boundary between the autonomous control and the human can happen on several levels: from labelling of sensory inputs \citep{depalma2016nimbus} to assigning reward to action to teach the robot an action policy \citep{thomaz2008teachable}. Similarly, this help can be triggered by the robot or the human, at specific stages of the interaction or at any time or can be a simple guidance or a command. 

\section{Current robot behaviours in HRI} \label{sec:back_behaviour}

    This section presents high level categories of robot control currently used in \gls{hri} to define a robot behaviour. 
    %As the number of individual techniques is too large for an exhaustive review, we organised the literature into broader categories. 
    For each category, we will present the corresponding approach, indicate representative works done in this direction and qualitatively rate it on the three axes defined in the previous section.
	
\subsection{Fixed preprogrammed behaviour}

    One of the simplest ways to have a robot interacting with a human is to have an explicit fixed behaviour. The robot is fully autonomous and follows a script or a finite state machine for action selection. This approach is dependent on having a well defined and predictable environment to have the interaction running smoothly. If the interaction modalities (possible range of behaviours and goals) are limited enough, a constant action policy can be sufficient to handle all (sensible) human actions. 
    
    This approach is followed in a large number of research in \gls{hri}: many studies being human-centred, the focus is not in the complexity of the robot's behaviour but on how different humans would interact with and react to a robot displaying a fixed behaviour, which also allows to strictly compare conditions. While having advantages when exploring people's reactions to robots, this method can hardly be used to deploy robots to interact with humans on a daily basis due to the lack of well defined environments in real world applications, the limited application of such fixed behaviour in day to day interactions and the high risk of boredom or lack of interest of users over repeated interactions.

    By essence, this type of controller has a no adaptivity as the robot is following a preprogrammed script, but scores highly on autonomy as no external human is required to control the robot and when the application domain is highly specified, the behaviour can be mostly appropriate.

\subsection{Adaptive preprogrammed behaviour}
	
	To go beyond a script, robot can also be programmed to react to expected human actions. By adaptive preprogrammed behaviour, we denote a behaviour programmed before the interaction, but explicitly (or implicitly) including ways to adjust the action policy in reaction to anticipated human behaviours. This preprogrammed adaptation takes two forms: either having a fixed number of variables impacted by the actions of the partner and guiding the action policy, or explicitly planning for specific behaviours to be produced if predefined conditions are met.
	
	%Robots have been programmed with different behaviours, selecting the one corresponding to the current interaction following a set of rules given prior the interaction. 
	
	Homeostasis, the tendency to keep multiple elements at equilibrium, is constantly used by living systems to survive and is also a good example of the first case of preprogrammed adaptation used in social \gls{hri}. \citet{breazeal1998motivational} used a set of drives (social, stimulation, security and fatigue) which are represented by a variable each and have to be kept within a predefined range to represent a `healthy' situation. If these values are outside the desired homeostatic range, the robot is either over or under-stimulated and this will affect its emotion status and it will display an emotion accordingly. Homeostasis approaches have also been extended to robotic pets \citep{arkin2003ethological} or \gls{rat} \citep{cao2017collaborative}.
	
	On the other hand, a case of planned adaptation is clearly presented in \citet{leyzberg2014personalizing}. Participants have to play a cognitive game,  and a robot delivers predefined advises on strategies depending on the performance and the current lack of knowledge of the participant. With these anticipated human behaviours, the robot can proposed personalised support as long as the participants behave within expectations. 

	%Could talk  about other adaptations: tutoring (bielefeld) or chess or other / aibo?
	
	Due to the implicit description of behaviours, homeostasis-based methods are more robust in unconstrained environments than a purely scripted controller, while remaining totally autonomous. However the action policy is not adaptive in time and as the behaviours are not totally defined and controlled, there is no guarantee against the robot acting in inconsistent way in some specific cases which limits the appropriateness of actions. Similarly, planned adaptation provides adaptivity to the environment but only in highly limited cases expected by the designers. This limit the adaptability of such an approach as the robot does not learn and may face situations not expected by the designers, also reducing the maximum appropriateness of actions.

	%Efforts have been made to extend the homeostasis approaches beyond purely reactive systems with the use of hormone models \citep{Lones2014}. This method allows the previous experiences of the robot to impact the behaviour expressed providing limited adaptivity in time to the robot. However this approach was only applied to a robot interacting in a non-social environment and the robot behaviour was biased rather than fully adaptive.
	
	Both predefined adaptation and homeostasis-based methods score highly in autonomy and can have a moderate to high level of appropriateness, but the adaptivity is low as they can only adapt to the environment within predefined, anticipated and limited boundaries and the robot does not learn.

\subsection{Wizard of Oz} \label{subsec:WoZ}

	\acrfull{woz} is a specific case of tele-operation where the robot is not autonomous but at least partially controlled by an external human operator to create the illusion of autonomy in an interaction with a user. It outsources the difficulty of action selection and/or sensory interpretation to a human operator. This technique has emerged from the \gls{hci} field \citep{kelley1983empirical} and is today common practice in \gls{hri} \citep{riek2012wizard}. Similarly to scripted behaviours, \gls{woz} is highly used in human-centred studies to explore how humans react to robot and not as a realistic way to control robots in the wild. A second use of \gls{woz} is to safely gather data to develop a robot controller from human demonstrations (cf. Section \ref{ssec:back_lfd}).
	
	Multiple ways exist to combine the autonomous component of the robot controller and the wizard. \cite{baxter2016characterising} define two levels of \gls{woz} related to the levels of autonomy presented by \cite{beer2014toward} and corresponding to the involvement of the human in the action selection process. In perceptual \gls{woz}, the human only replaces sensory system and feeds information to the robot controller, while in cognitive \gls{woz}, the human directly makes decisions about what the robot should do next. Typically, perceptual \gls{woz} replaces challenging features of the controller required for a study, but not relevant to the research question. One typical example is \gls{nlp}, despite all the progress made in speech recognition, \gls{nlp} is still a challenge in \gls{hri}, especially when interacting with children \citep{kennedy2017child}. And as some studies require a limited speech recognition element to test an hypothesis, using a human for that part of the interaction allows to run study without having to solve complex technical challenges, as used in \cite{cakmak2010designing}.

	This level of human control impacts the autonomy of a system: a robot relying on human only to do perception has a higher autonomy than a robot fully controlled by an operator. Similarly to the different levels of autonomy presented earlier, systems can also combine human control and predefined autonomous behaviour in mixed systems. For example, \citet{shiomi2008semi} propose a semi-autonomous informative robot being mainly autonomous, but with the ability to make explicit request to a human supervisor in predefined cases where the sensory input is not clear enough to make a decision. %The human can also provide additional information on the state of the world to inform the robot of events or do natural language recognition, to inform an autonomous system about the sentences said by the users. %MIght need to add other ref
	
	With \gls{woz}, the adaptivity and the appropriateness are provided almost exclusively by the human, so these characteristics are dependent of the human expertise but are generally high. However, due to the reliance on human supervision to control the robot, the autonomy is low. For semi-autonomous robots, the picture is more complex: as explained by \cite{beer2014toward}, the initiative, the human's role and the quantity of information and control shared influence the level of autonomy. For example, in \citet{shiomi2008semi} the robot explicitly makes requests to the human, but the human cannot take the initiative to step in the interaction limiting the adaptivity (especially as the robot policy is fixed). And as the human only has limited control over the robot behaviour, no mechanism prevents the robot to make undesired decision, leading to a higher autonomous, but a lower appropriateness of actions and adaptivity compared to classical \gls{woz}.

\subsection{Learning from Demonstration} \label{ssec:back_lfd}
%mention that its applicable to end users and allow designers to have to implement the behaviour and all the specific cases
	%motivation for lfd
	%behaviour too complex to be coded or requirement of end user knowledge
	%MIght require ref
	As stated by numerous researchers, explicitly defining a robot behaviour and manually implementing it on a robot can take a prohibitive amount of time or not be possible at all \citep{argall2009survey,billard2008robot}. This statement applies both to manipulation tasks and social interaction. However in both cases, humans have some knowledge or expertise that should be transferred to the robot. However in social robotics, experts of the field often do not have the technical knowledge to implement efficient behaviours on a robot, which results in numerous design iteration between the users and engineers to reach a consensus. 
	
	%As when interacting it might be complicated and time consuming to acquire data points for learning, offline learning methods are mainly inspired from the Learning from Demonstration framework (LfD) \citep{argall2009survey}. With LfD, the idea is to take inspiration from human demonstrations to accelerate the learning for an agent or to teach tasks that could not be preprogrammed manually \citep{billard2013robot}. The classical approach starts with observing a human completing the task and then deriving a corresponding robot behaviour to match the human one. In most of the cases, the learning only occurs once: data is accumulated first and then batch learning is applied to derive a static action policy. 
	    
	The field of \gls{lfd} aims to tackle these two challenges: implementing behaviours too complex to be specified in term of code and empowering end-users with limited technical knowledge to transfer an action policy to a robot. The process is the following: a human demonstrates a correct behaviour through different control means \citep{argall2009survey}, and then offline batch learning is applied to obtain an action policy for the robot. Later, if required, reinforcement learning can complement the demonstrations to reach a successful action policy \citep{billard2008robot}.
	
	%importance of hri - but in teaching, often not object
	In \gls{lfd}, the human-robot interaction is key, however in most of the cases this interaction is only in the learning process and the object of the learning is not social interaction with humans, but manipulation or locomotion tasks. Classical examples are presented in \cite{billard2008robot} are cover manipulations tasks such as grabbing and moving an object, using a racket to hit a ball or throwing tasks.
	
	%examples with hri as target (liu, sequeria and clark) - but examples limited	
	% Except Liu (learning from data) and restricted WoZ
	However, two approaches have applied \gls{lfd} to teach robots a social policy to interact with humans.
	
	The first observes human-human interactions and aims to have a robot replicating the behaviour of one of the humans. \citet{liu2014train} present a data driven approach taking demonstrations from human-human interactions to gather relevant features defining human social behaviours. Authors recorded motion and speech from about 180 interactions in a simulated shopping scenario and then cluster these behaviours into actions. During the interaction, the robot uses a variable-length Markov model predictor to estimate the selection probability of each actions by the human demonstrator and then selects the most probable one. According to the authors, the final performance of the robot was not perfect, but if this approach was scaled using a larger dataset gathered from more human-human interactions in the real world, the performance should improve and become closer to natural human behaviours.
    
    Alternatively, the data can be collected from a \gls{woz} setup. \cite{knox2014learning} coined this approach \emph{Learning from Wizard}: starting for a purely \gls{woz} control study to gather data, and then apply machine learning to derive an action policy. But this paper presents no description of which algorithm could be use or how, and gives no evaluation of the approach, instead it only offers a reflection on the application of this idea. An implementation and evaluation is briefly discussed by the authors in \cite{knox2016learning}, but the lack of implementation details and results reduces the usability of the paper.
    
    This Learning from Wizard is widely used in \gls{hci} and especially dialogue management \citep{rieser2008learning}, and has been implemented by other groups of researchers in robotics. \citet{sequeira2016discovering} extended the idea to a methodology to obtain a fully autonomous robot tutor. This method is composed of multiple steps starting with the observation of a human teacher performing the task; then, the different features used by the teacher to select their actions as well as the actions themselves are encoded and implemented in a robot. The next step is setting up a \gls{woz} experiment where an operator has access to the same features than the robot to make his decisions and controls the robot's action. Finally, a combination manually derived rules and machine learning is applied on the data from the restricted-perception experiment and the robot is tested autonomously. Additional offline refinement steps are possible to fine tune the robot's behaviour if required. 
    
    Both \citet{knox2014learning} and \citet{sequeira2016discovering} stress the importance of using similar features for the Wizard of Oz part than the ones available to the robot during the autonomous part: whilst decreasing the performance in the first interaction, it allows more accurate learning due to the similarity of inputs for the robot and the human controlling it.
        
    \cite{clark2018deep} aimed to bypass these limitations by using a deep Q-network \citep{mnih2015human} to learn an Applied Behaviour Analysis policy for \gls{rat}. They recorded videos, microphone inputs and actions selected in a \gls{woz} interaction with neurotypical participants to train the network with the raw inputs and the actions selected to obtain a controller able to deliver the therapy. However in their study, they had to add a limited human input to inform the state of the therapy and reach a performance of less than 70\% of correct actions which means that the robot would provide inconsistent feedback at some point in the interaction. 
     
    %TODO: see if required, and if yes how to make it better   
    As these methods are based on real interactions either between humans, or between humans and robots controlled by humans, with enough demonstrations the robot should be able to select appropriate actions. However, the efficiency is limited by the type of inputs recorded, the capabilities of the learning algorithm with the inputs space and the quality of the demonstrations which limits the appropriateness of the action policy. After the learning phase, the robot behaviour is  mostly static, without any additional learning provided. As such, the adaptivity is also reduced once the robot is deployed. Sequeira et al. propose that additional offline learning steps could be added, but online learning would allow a smoother transitions and improvement of behaviours.
    %as no intrinsic mechanism is present to prevent the execution of undesired actions which could happen if the robot ends up in an unseen state, the appropriateness of actions cannot be maximal. Additionally, the adaptivity in time is limited, as for most of the techniques the learning happens only once and then the behaviour is fixed. However, the framework proposed by restricted-perception Wizard of Oz should allow asynchronous adaptivity in time using the refinement phase. 
    And finally, all these methods require the presence of humans in a first phase but the robots are fully autonomous later in the interaction, so the autonomy is low in the first phase and then total during the main part of the interaction.neurotypical
    
\subsection{Planning} \label{ssec:planning}
    
    An alternative way to interact in complex environments is to use planning. The robot has access to a set of actions with preconditions and postconditions and a defined goal. To achieve this goal state, it follows the three planning steps: sense, plan and act. The first step, sense, is to acquire information about the current state of the environment. Then, based on the set of actions available and the goal, a plan is created. This plan is a trajectory in the world, a succession of action and states which, according to the defined pre and postconditions, will lead to the goal. Finally, the last step is to execute the plan. The plan can be reevaluated at each time step or only if a state differs from the expected one, in that case the robot updates its plan according to the new state of the environment and continues trying.
    
    %This approach is often used in motion planning, but can integrate some %social aspect as presented in the work of Dragan and colleagues %\citep{dragan2013legibility}, where motion planning is adapted to be more %legible by human. Planning can also be used at a higher level for action %selection.
    
    The efficiency of planning relies on having a precise and accurate set of pre and postconditions for each actions. And as humans are complex and unpredictable, it is a serious challenge, if not impossible, to model them precisely. As such, planning have seen limited use for open social interactions with humans. However, due to the nature of planning, reaching a specific goal in a known environment, it has been applied successfully to \gls{hrc}. Additionally, limiting the interaction to a joint task also simplifies the modelling of the human behaviour as the interaction is more constrained and the human behaviour should limit to a number of expected task related actions. One example of such an application is the Human Aware Task Planner \citep{alili2009task}, one property of this planner is the ability to take into account predefined social rules, such as reducing human idle time, when creating a plan specifying what the human and robot should do. Conforming to these social norm is expected to improve the user experience and ensures a maximised compliance of the human to the plan.
    
    Planning performance depends heavily on the model of the environment the robot has access to. A precise and correct model ensures that the robot will autonomously select the appropriate action whilst an incorrect one would lead to non appropriate behaviours. Similarly, the adaptivity depends on the model the robot has access to and whether it can update it in real time. However, in many cases when interacting with humans, the model is static, only covering the tasks the robot has to complete the different contexts and states it is expected to face and as such presents limited adaptivity to unanticipated situations.
    
    %As long as the model is correct enough, it is ensured to keep the %``astonishment'' low and to maintain a high level of autonomy. The %adaptivity is also highly dependent on the model the robot has access to. %If the planning domain is large enough, the adaptivity can be high, %multiple users can be also predefined to increase the user adaptability, %and if the domain knowledge can be updated dynamically.
    
    %\marginpar{The previous paragraph is a bit unclear to me, but you can leave %it in the RDC2. If you use it anywhere else, it would need to be %rewritten.}
    
    Planning have also been extended with learning, which then allows for adaptive action policies. This type of learning planner have been mostly used in manipulation and navigation to obtain a better trajectory \citep{jain2013learning,beetz2004rpllearn}. In \gls{hri}, \cite{munzer2017efficient} presents a planner adapting its decisions to human preferences in a \gls{hrc} scenario. The robot estimates the risk of each actions and will execute them, propose them (and waiting for approval) or wait for a human decision depending of the risk value. Between repetitions of the task, the robot will update its planner to fit more precisely to the human preferences and improve its action policy for the next iteration of the task. Munzer et al. adopted principles from \gls{lfd} to planning to improve quickly efficiently the performance of the robot. However, while planning is well suited for strictly defined and mostly deterministic task, many social human-robot interactions cannot be totally specified symbolically and with clear actions and outcomes and as such the application of planning to social \gls{hri} is limited.
    % and action selection planning \citep{kirsch2009robot}. 
	
	
	
\subsection{Summary}

	Table \ref{tab:back_controller} presents a summary of the different approaches currently used in social \gls{hri} with their advantages and drawbacks for application in \gls{hri} and the rating on each of the three axes. The two most promising types of control are \gls{lfd} and planning, however, both of them have their drawbacks: \gls{lfd} is applied offline to create a monolithic controller with limited adaptivity after being deployed, and planning's reliance on a model of the world limits its application in open-ended social \gls{hri} in the wild.
	
\afterpage{%
	\clearpage% Flush earlier floats (otherwise order might not be correct)
	\thispagestyle{empty}% empty page style (?)
	\begin{landscape}% Landscape page
		\centering
		\label{tab:back_controller}
		\bgroup
		\def\arraystretch{1.2}
		\begin{tabular}{>{\centering\arraybackslash}m{.1\linewidth}|>{\centering\arraybackslash}m{.2\linewidth}|>{\centering\arraybackslash}m{.2\linewidth}|>{\centering\arraybackslash}m{.15\linewidth}|>{\centering\arraybackslash}m{.11\linewidth}|>{\centering\arraybackslash}m{.07\linewidth}|>{\centering\arraybackslash}m{.07\linewidth}}
			Controller & Advantage & Drawbacks & Application in \gls{hri} & Appropriateness & Adaptivity & Autonomy \\ \hline
			Fixed preprogrammed behaviour & Quick and easy to create, clear specified and repeatable behaviour & Limited to highly constrained interactions & Human-centered studies in highly constrained env. & Low & Null  & Total    \\
			Adaptive preprogrammed behaviour & Relatively simple to program and more robust and efficient than scripted behaviour & Only provide adaptability in limited anticipated context & Human-centered studies in constrained env. & Medium & Low & Total    \\ 
			Wizard of Oz & Use human expertise to select the best action & Require constant high workload from human & Human-centered studies \linebreak Highly critical \gls{hri} & Total  & Total   & Null/Low     \\ 
			Learning from Demonstration & Transfer knowledge from human to robot \linebreak Learning in the real application environment & Lack of learning once deployed & HRI case by case & High & Medium     & High     \\
			Planning & Complex behaviours and adaptable to variations in the environment & Human too complex to have clear set of conditions \linebreak Limited application to social interaction & Complex defined environments \linebreak \acrshort{hrc} & Medium & High       & Total   
		\end{tabular}
		\egroup
		\vspace{-.65\linewidth}
		\captionof{table}{Comparison of robot controllers in \gls{hri}}
	\end{landscape}
	\clearpage% Flush page
}
	
	An ideal controller would learn how to interact by interacting, using demonstrations from an expert to obtain an initial reasonable action policy, but still improving itself after being deployed using reactions from the environment or feedback from a teacher. This type of interaction is similar to \gls{iml}: learning from the interaction and using a human teacher to speed the the learning. Research explored how to learn interactively non-social action policy from interactions with humans \citep{scheutz2017spoken,cakmak2010designing}, but as of April 2018, no controller exists in \gls{hri} applying \gls{iml} to the challenge of learning social interaction with humans. 

	This approach is the one with the most potential as the humans could provide only the required supervision or guidance and let the robot be autonomous most of the time. Learning online an action policy provides potentially open-ended adaptivity and finally, with enough data points and the presence of a human in the action selection loop if required, the appropriateness of actions could also be guaranteed.
	
\section{Interactive Machine Learning} \label{sec:back_iml}
%Mostly for supervised learning 

\gls{ml} is a promising method to provide a robot with an adequate action policy without having to implement in advance all the decisions rules used to select an action. \gls{ml} has two main trends referring to the synchronisation between the learning and the use of algorithm: offline and online learning.

In robotics, offline learning is a technique allowing the robot to change its action policy over time by updating it outside of the interaction (such as Learning from the Wizard in Section \ref{ssec:back_lfd}). Between or before the interactions, a learning algorithm is used to create a new action policy derived from accumulated data.

On the other hand, online learning (such as \gls{rl} or \gls{iml}) has the advantage of benefiting from a high number of updates, constantly refining the agent behaviour, rather than single monolithic definitions or updates of the behaviour. 

\acrfull{iml}, as coined by \cite{fails2003interactive}, differs from \gls{cml} by integrating an expert end-user in the learning process. In classical \gls{sl}, such as deep learning \citep{lecun2015deep}, the learning phase happens offline once to obtain a classifier for later use. On the other hand, \gls{iml} is an iterative online process using a human to correct the errors made by the algorithm as they appear and providing additional useful information to the learner.

\cite{amershi2014power} presents an introduction to \gls{iml} by reviewing the work done and presenting classical approaches and challenges faced when using humans to support machine learning.

%Need to careful not to just rewrite power to people + need to add new material 
%Mention Learning from Demonstration!
\subsection{Goal}

The main goal behind \gls{iml} is to leverage the human knowledge during the learning process to speed it up, to extend the use of classifiers from static algorithms trained only once to evolving agents learning from humans and refining their policies over time. As explained in \cite{fails2003interactive}, classifiers gain to be fast rather than highly inductive and \gls{rl} might gain from using humans to provide rewards \citep{knox2009interactively}. \gls{iml} aims combine the advantages from both \gls{sl} and \gls{rl} and applies this new type of learner to classification or interaction tasks.

Furthermore, by allowing a human user to see the output of an algorithms and provide additional inputs, the learning has the potential to be faster and tailored to this human's desires: by using human expert knowledge and intuition, the system can achieve a better performance faster.

Additionally, a key advantage of \gls{iml} is also being able to empower end-users of robotic or learning systems. These users are often non-technical, but possess valuable knowledge about what the robot should do. \gls{iml} provides an opportunity to allow these users to design the behaviour of their robot, to teach it to behave the way they desire.

These human inputs take three forms: labels for specific datapoints (cf. active learning), feedback over actions (similarly to reward in \gls{rl}) or demonstrations to reproduce (cf. \gls{lfd}).

\subsection{Active learning} \label{ssec:back_active}

Active learning is a form of teaching used in education aiming to increase student achievement by giving them a more active role in the teaching process \citep{johnson1991active}. This approach has been transferred to machine learning, and especially to classifiers by allowing the learner to ask questions, query labels from an oracle for specific datapoints with high uncertainty \citep{settles2012active}. The typical application case is when unlabelled data are plentiful, but labels can be limited in numbers or costly to obtain. As such a trade-off arises between the performance of the classifier and the quantity of queries made by the algorithm. Often this oracle would be a human annotator with the ability to provide a correct label to any datapoint, but their use should be minimised for reasons of cost, time or annoyance for example.

%different challenges for classic active learning and robot teaching (inter)active learning
Using an oracle to provide the label of specific points with high uncertainty should highlight missing features in the current classifier resulting in improvements both in term of accuracy and learning speed. However, this specific relation between the learner and the human teacher poses new questions such as: 
\begin{itemize}
	\item Which points should be selected for the query?
	\item How often the human should be queried?
	\item Who controls the interaction? (i.e. who has the initiative to trigger a query?)
\end{itemize}

Researchers have explored optimal strategies for dealing with this relation between the learner and the oracle. This research has been especially active in \gls{hri} with robots directly asking questions to human participants and exploring how the robot's queries could inform the teacher about the knowledge of the learner \citep{chao2010transparent}. In a follow up study, \cite{cakmak2010designing} showed that most users preferred the robot to be proactive and involved in the learning process but they also wanted to be in control of the interaction, deciding when the robot could ask questions even if it imposed a higher workload on the teacher. However, authors proposed that when teaching a complex task requiring a high workload on the teacher, the robot would probably be expected or should be encouraged to take a more pro-active stance requesting samples to take over some workload from the teacher.
%User are human and want to be considered as such, not oracle (want more control) not willing to be simple oracle - human preferred being in control of the rate and timing of question, rather than being simply a labeller.

Active learning, being able to select a specific sample for labelling, can dramatically improve the performance of the learning algorithm. However, in interaction, the learner is not in control of which sample to submit to an oracle to obtain a label. Datapoints are provided by the interaction and are influenced by the learner's actions and the environment reaction. For agents learning from the interaction, the active learning approach working for classifiers is not applicable, so other methods have been applied such as \gls{rl}, learning from human feedback or \gls{lfd}.
%limit the risks of the exploration and increase the learning speed - demonstrations are not fully consistent so the robot might find states where there is no obvious action to select. 

%LImited application to learn hri as the robot cannot select its samples itself
\subsection{Reinforcement Learning}

The main framework of learning applied to learning from interaction is \gls{rl}, or the problem of finding the best action policy by observing the environment reaction to the agent's action.

\subsubsection{Concept} 
	Young infants and adults learn by interacting with their environment, by producing actions, analysing the environment reactions and measuring progress toward a goal. Similarly, the field of \acrfull{rl} aims to empower agents by making them learn by interacting, using results from trials and errors and potentially delayed rewards to reach an optimal, or at least efficient, action policy \citep{sutton1998reinforcement}. 

	%Discrete time - life as a sequence!
	\gls{rl} considers the time to be discrete, the life to be a sequence of states and actions. The simplest version of \gls{rl} is modelled as a finite \gls{mdp}, a discrete environment defined by the five ensembles $(S, A, P_a(s,s'), R_a(s,s'), \gamma)$, with:
	\begin{itemize}
		\item $S$: a finite set of states defining the agent and environment states
		\item $A$: a finite set of actions available to the agent
		\item $P_a(s,s')$: the probability of transition from state $s$ to $s'$ following action $a$
		\item $R_a(s,s')$: the immediate reward following transition $s$ to $s'$ due to action $a$
		\item $\gamma$: a discount factor applied to future rewards
	\end{itemize}
	
	The goal of the \gls{rl} agent is to find the optimal policy $\pi_*$ (assigning an action from $A$ to each state in $S$) maximising the discounted sum of future rewards. The agent is not aware of all the parameters of the model, but only observes the transitions between states and the rewards provided by the environment and has to update its policy to maximise this cumulated reward. Different algorithms exist to reach this policy, but the main features present in all of them are the concepts of exploration and exploitation.
	
	\textit{Exploration} reflects the idea of trying out new actions to learn more on the environment and potentially gain knowledge improving the policy whilst \textit{exploitation} is the execution of the current best policy to maximise the current gain of rewards. All the algorithms have to balance these two features to reach an optimal action policy. One way to deal with this trade-off is to start with high probably of exploration to collect knowledge on the environment and then decrease this probably to converge toward an efficient policy using this knowledge to make better choice of actions.
	
	The more complex the environment is, the longer the agent has to explore before converging to a good action policy. It is not uncommon to reach numbers such as millions of iterations before reaching an appropriate action policy. And during this exploration phase, the agent's behaviour might seem erratic as the agent tries actions often randomly to observe how the environment is reacting.
	
	%Challenges - tradeoff exploration/itation - uniqueness of data (fleeing nature of time and data) - non statioanrity - sequential delayed consequences ...
	%success: backgammon, helicopter, advertisements, jeopardy, atari -> better perf than any other methods - and "without using human instructions"
	
	%concept of action value function of a policy: value of doing action in state
	%optimal value function = value function of the optimal policy (include delayed consequences)
	%q-learning works for any type of MDP
	%off policy: learning without doing the target policy + behaviour policy
	%policy iteration: evaluate policy - obtain q - greedify to new policy - evalute - obtain new q..
	%in tabular, converge in few iterations, robust
	%bootstrapping: updating an estimate from an estimate: bellman equation
	%Need approximation for generalisation and learning faster - approximate the action value function (linear weighting of features)
	%the function approx subsume much of the problem of hidden state
	%it works often with q-learning, but loses the guaranty of converging! -> still need theoretical work
	%update parameter vector at each time step according to error
	%better with sarsa on policy algo -> often epsilon greedy
	\subsubsection{Application to HRI}
	
	This approach presents many features relevant to \gls{hri}: it possess the autonomy required for meaningful interactions with humans and provides the adaptivity desired for having a large impact. However, as explained in the previous section, traditional \gls{rl} has two main issues: requirement of exploration to gather knowledge about the environment and large number of iteration before reaching an efficient action policy. Generally, \gls{rl} copes with these issues by having the agent interacting in a simulated word. This allows the agent to explore safely in an environment where its actions have limited impact on the real world (only time and energy) and where the speed of the interaction can be highly increased to gather the required datapoints in a reasonable amount of time. However, no simulator of human beings exists today which would be accurate enough to learn an action policy applicable in the real world. Learning to interact with humans by interacting with them would have to take place in the physical world, with real humans, and this implies that these issues of time and random behaviours would have direct impacts. 
	
	To gather informations about the environment, the agent needs to explore, trying out random actions to learn how the humans respond to them and if the agent should repeat them later. When interacting with humans, executing random actions can have dramatic effect on the users, presenting risk of physical harm as robots are often stiff and strong or cause distress as explained before. This reliance on random exploration presents a clear violation of the first principle to interact with humans presented earlier (`Only execute appropriate actions').
	
	Even if random behaviours were acceptable, humans are complex creatures, behaving stochastically, with personal preferences and desires. And as such, learning to interact with them from scratch would require large number of datapoints and as interactions with humans are slow (not many actions are executed per minute) the time required to reach an acceptable policy would be prohibitive. 
	
	Despite this real-world constraints, \gls{rl} has been used in robotics \citep{kober2013reinforcement}, but mostly applied to manipulation, locomotion or navigation tasks. For the reasons stated above, \gls{rl} has never been used to autonomously learn social behaviours for \gls{hri}. 
	
%REWORK	
	\subsubsection{Opportunities}  
	Despite the limitations presented in the previous section, changes can be made to \gls{rl} to increase its applicability to \gls{hri}. Combining \gls{rl} and \gls{iml} ensures that the behaviour is appropriate to interactions with humans even in the learning phase.
	
	\cite{garcia2015comprehensive} insist on \textit{safe} \gls{rl}, ways to ensure that even in the early stages of the interaction, when the agent is still learning about the world, its action policy still achieves a minimal acceptable performance. Authors present two ways to achieve this safety: either use a mechanism to prevent the execution of non-safe actions or provide the agent with enough initial knowledge to ensure that it is staying in a safe interaction zone. These two methods are not limited to \gls{rl} but are also applicable to other machine learning techniques. 
	
	The first method (preventing the agent to execute undesired actions) can be implemented by explicitly preventing the agent to execute specific actions in predefined states. Using this method, the anticipated cases of errors can be prevented, however it seems unlikely that every case could be specified in advance. As such, an efficient way to prevent undesired actions to be executed could be to include a human in the action selection loop in the early learning phase, and giving them the capacity to pre-empt undesired actions before being executed. 
	
	The second method (providing enough initial knowledge) can be achieved by carefully engineering the features used by the algorithm or starting from a initial action policy to build upon. \cite{abbeel2004apprenticeship} propose to use humans demonstrations in a fashion similar to \gls{lfd} but to learn a reward function and an initial working action policy. This method, Inverse Reinforcement Learning, has been applied successfully to teach a flying behaviour to a robotic helicopter. Once the initial policy and a reward function are learnt, \gls{rl} is applied around the provided policy to explore and optimise the policy. That way, only small variation of the policy will happen around the demonstrated one, and these small variations ensure that policies leading to incorrect behaviours are negatively reinforced and avoided before creating issues (such as crashing in the case of the robotic helicopter). 
	
	%Similarly to autonomous online learning, these approaches score high in adaptivity in time and depending of the learning mechanism, sensory inputs and actions, they also do well in being adaptive in the search space and to changing user behaviour. The reliance on human input decreases the autonomy, but increases the appropriateness of actions as this guidance can help to reduce the use of random exploration. However, as the human has often only a partial control, the robot is not ensured to act correctly at every stage of the interaction preventing the appropriateness to be maximal.
	
	Whilst being promising and having been applied for agents in human environments (such as for personalised advertisement - \citealt{theocharous2015personalized})	these approaches have not been used to learn social behaviours or to have robot interacting with humans.

\subsection{Human as a source of feedback on actions}

When an agent is learning in a \gls{rl} fashion and improves its behaviour by receiving rewards from the environment, an intuitive way to steer the agent's behaviour in the desired direction faster is to use human rewards. This approach is an adaptation of `shaping': tuning a animal's behaviour by providing rewards. In \gls{ml}, using rewards from a human to bias and improve the learning presents many advantages: the interface is simple and generalisable to any type of problem, the teacher only needs a way to provide a scalar or a binary evaluation of an action to steer the learning. However, this simplicity of interaction is joined by a limited efficiency and a complexity of interpretation: the issues of how to interpret human rewards and how to combine them with environmental ones if existent are an active research field today.

When used on their own, human rewards enable an agent to learn an action policy even in the absence of any environmental rewards, which is specially interesting to robots as it can be complex to define a clear reward function applicable to \gls{hri} or robotics in general. Early work in that field came from \cite{isbell2006cobot} who designed an agent to interact with a community in the LambdaMOO text based environment. Cobot, the agent had a statistical graph of users and their relations and can execute actions in the environment. Users of LambdaMOO could either reinforce positively or negatively Cobot's action by providing rewards. Isbell et al. presented the first agent to learn social interactions in a complex human online social environment. 

While the goal of Cobot was to create an entity interacting with humans, \cite{knox2009interactively} explored how humans could actively teach an agent an action policy with TAMER (Training an Agent Manually via Evaluative Reinforcement). The agent uses a supervised learner to model the human reward function and then takes the action that would receive the highest reward from the model. Unlike environmental rewards, human rewards are a subjective evaluation of an agent behaviour, as such by knowing humans tendencies and intentions when providing rewards, an agent is able to obtain more information from the reward than when treated similarly to an environmental one. Advice \citep{griffith2013policy} models how confident a learner should be in its teacher to make better use of rewards by decreasing the importance given to rewards from inconsistent or not trustworthy teachers. \cite{loftin2016learning} explore how to infer the strategy used by the teacher in the reward delivery, similar behaviours from different teachers might have different meaning: not rewarding an action might reflect an implicit acknowledgement of the correctness of an action or the active refusal to provide a positive reward (indicating the incorrectness of an action). \cite{macglashan2017interactive} proposed COACH (Convergent Actor-Critic by Humans) to adapt the interpretation of feedback to the current policy, for example, a suboptimal policy could receive positive feedback early on when it compares positively to the average behaviour, while receiving negative feedback later on the teaching when the average agent's performance is better and this dependence of the reward function to the current policy should be taken into account.

Even when environmental rewards are present, human rewards still have opportunities to improve the learning: they can enrich a sparse reward function, guide the robot faster to an optimal policy or correct incomplete or incorrect environmental rewards. \cite{knox2010combining} explore the impact of different ways to combine these two types of rewards and the impacts on the learning of each methods.

%Could read more about this
%Other approaches combine traditional \gls{rl} with critique from humans. A human is requested to evaluate specific behaviours \citep{judah2010reinforcement} or compare to policies to select the most appropriate one \citep{christiano2017deep}. 

%Human don't want to provide only label, they want to explain \cite{stumpf2007toward} + importance on transparency: help to achieve better results and improve user experience

Teachers can also use rewards to communicate other information to the learner. For example, \cite{thomaz2008teachable} aimed to explore how humans would use feedback to teach a robot how to solve a task in a virtual environment. They used \acrfull{irl} as a way to directly combine environmental rewards and human ones. However, during early studies, Thomaz et al. discovered that participants tried to use rewards to convey intentions, informing the robot which part of the environment it should interact with. The next study involved two communication channels, a reward one to provide feedback on the actions and a guidance channel to provide information about which part of the environment the robot should interact with. This guidance has been actively decided to be ambiguous, participants could not explicitly control the robot, but just bias the exploration, and adding this second channel improved the performance of participants. This study presented a first attempt to combine environmental rewards, human ones and human guidance to teach an agent an action policy.

%Using humans to provide rewards to a learner can largely improve the learning by making it faster and safer. However researches show that this evaluation of behaviours  is not enough, participants desire to have more control over the robot actions and this can improve the learning further.

%present the , a method combining feedback from the environment and feedback from a human supervisor to learn a task in a non-social context. Similarly, \citet{knox2009interactively} propose to use RL in an environment where the feedback is not given by the environment itself, but by a human only. In these two cases, human feedback is used to scaffold the learning: it makes it faster and safer and can allow the use of RL in environments without explicit reward. However as the feedback is always given after the execution of the action, there is no guaranty that only expected actions will be executed. This might be a reason why these two approaches have not been used to learn an action policy for social human-robot interactions. But they are nevertheless relevant as they are using the interaction with a human to help an agent to learn an action policy faster and safer than pure RL.

While not applied to robotics but mostly to learn non-social interactions, these implementations of \gls{iml} provide important research on how robots could be taught to interact with humans. These human feedback are especially interesting when the environmental reward function is sparsely defined or non-existent, providing a way to teach robots in any environments. However, humans do not simply evaluate an agent actions, they adopt teaching strategies influencing their way of rewarding and want to provide guidance, hints or commands to help the agent to learn better and faster and they desire to go beyond simply evaluating what the robot is doing and provide it advices about what it should do.

%conclusion of subpart??
%feedback not enough
%When given choice, human will never teach only using rewards  %See if %More recent stuff with deep comparison and things - still requires simulation and so on
%citation makes sense

\subsection{Interactive Learning from Demonstration} \label{ssec:back_ilfd}

As presented in \cite{argall2009survey} and \cite{billard2008robot}, \gls{lfd} is majoritarily used in an offline learning fashion to learn a defined task without extending the action policy once the task is considered mastered. However, tasks such as social interaction are complex even for humans and probably will never be fully mastered for robots; as such and as argued before, robots would highly profit from learning throughout all their life, not only once before being deployed, but learning new tasks and improving their skills as often as required \citep{dautenhahn2004robots}.

With \gls{ilfd}, an agent receives demonstrations not only once, but as often as required after being deployed. \gls{ilfd} is related to Mixed Initiative Control \citep{adams2004mixed} where an agent and a human share control on the agent's actions. The robot acts mostly autonomously, but in some cases (at the initiative of the human or the robot), the human takes over the robot control and make a demonstration that will be used by the robot to refine its action policy for the future.

One approach giving teachers a total initiative on the interaction is Dogged Learning (DL) \citep{grollman2007dogged}. With DL, an agent is autonomously interacting and a teacher has the power to override the agent behaviour at any time by selecting desired actions or outputs. Facing a potential difference between the algorithm's outputs and the teacher's ones, the robot executes the commands with highest confidence (often the human's one) and the learning component aims at reproducing the executed output. If the teacher does not provide any commands, the ones from the algorithm are used. DL does not provide the robot with the opportunity to request a demonstration, but instead, the robot can communicate its uncertainty to the teacher, indirectly asking for demonstrations. 

\cite{chernova2009interactive} propose a method with a more complex interaction between the learner and the teacher. The Confidence Based Algorithm (CBA) is composed of two components: the Confident Execution (CE) and the Corrective Demonstration (CD). The CE enables the agent to act autonomously when its confidence in its action policy is high and on the other hand to actively request a demonstration when the confidence is low. The CD allows the teacher to provide a corrective demonstration when the agent executes an incorrect action, which provide more information to the agent than a classic negative reward. These two components aim to leverage the complementary capabilities of the learner and the teacher. CBA has demonstrated efficient teaching in diverse scenario such as simple driving simulator or other classification tasks, but, the effectiveness of this approach is bounded by the capacity of the learner to estimate this confidence request demonstrations and prevent incorrect behaviour to be executed and the possibility for the teacher to correct undesired actions before they negatively impact world. 

Both methods rely on the teacher being able to anticipate the robot behaviour to provide demonstrations before an incorrect action is executed or before it impacts the agent and environment. As such, the appropriateness of the robot controller is not maximum as the teacher cannot ensure that no incorrect action will be executed during the learning, only that the robot would learn faster from its errors.

\subsection{Importance of control}

Results from active learning, research using human to provide feedback and \gls{lfd} have shown that human teachers take an active stance during the training of an agent and want many ways to influence the learner's behaviour \citep{amershi2014power}. Humans are not oracles, enjoying providing labels and evaluating an agent's actions, they desire to be in control of the learning and provide richer information to the agent. \cite{kaochar2011towards} have shown than when given choice between different teaching methods, humans will never choose to limit themselves to use only feedback, but they want to teach using more modalities.

In addition to improve the teacher's experience in the teaching process, providing the humans with more control improves the learning \citep{thomaz2008teachable,chernova2009interactive}. By allowing the teacher to demonstrate online an action policy and pre-empt or correct undesired actions, the learner interacts mostly in useful states of the environment and with a correct action policy, learning faster and improving its performance highly in early stages of the learning. Another fundamental feature added by this human control over the robot actions is safety: if a domain expert can prevent a robot interacting with humans to make errors and ensures that all its actions are efficient, the quality of the interaction for the humans involved is greatly increased, which further improves the applicability and use of the robot (and would satisfy the two first principles: appropriateness of actions and adaptivity of the robot).

However providing the teacher with this control presents challenges for designing the interaction. Unlike a simple scalar for reward, being able to control the robot requires the teacher to be able to give commands or advices to the robot and to receive additional information about the learner than just observing its behaviour. This additional enriched two-ways communication might be complex to design, especially when the action space is bigger than few actions or the learning mechanism not transparent. In addition to the communication interface, the time scales of the interaction are also key: to give the opportunity to the teacher to pre-empt undesired actions, the learner needs to communicate its intentions in a timely manner to the teacher which complexifies the relation between the learner and the teacher. %This control also impacts the algorithm itself

%\cite{amershi2014power} emphasise throughout their paper the desire of user to have more control over the learning progress (through timing, suggestions, corrections, decisions...).

%\subsection{Challenges}

%Humans are bad teachers, not consistent, making errors...

\section{Summary}

This chapter presented first an overview of fields where robots interact socially with humans. From these cases of application, we defined three principles a robot controller should follow. To interact efficiently with humans, the robot should:
\begin{enumerate}
   	\item Only execute appropriate actions.
   	\item Have a high level of adaptivity and learn.
   	\item Have a high level of autonomy.
\end{enumerate}

Secondly, a review of current controller for robots in \gls{hri} reported that no approach applied today in the field validates these principles. The review was extended to more general methods in \gls{ml} with potential to satisfy these principles. \gls{iml} shows promises for enabling a robot to learn online how to interact with humans, especially when the teacher is given control over the robot behaviour and can demonstrate a correct action policy. However while humans have been used to teach robot behaviours or concepts, teaching them to interact with human in an interactive, online fashion has not been demonstrated in the field so far and would satisfy all these requirements.