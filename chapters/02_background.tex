%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Background} \label{chap:background}

This chapter describes the application of robot interacting with humans and related research in agent and robot control. In the first section the different fields of application of social \gls{hri} and then draws from them requirements for controlling a robot interacting with humans.  The second section provides the current state of the art in robot control for \gls{hri} and analyse it through the constraints presented in subsection \ref{ssec:back_const}. And finally the third section describe the state of the field of \acrlong{iml} and how it has been used to \gls{hri}.

\section{Social Human-Robot Interaction}

\subsection{Fields of application}

\acrlong{hri} covers the full spectrum of interactions between humans and robots. However, as this thesis is focused on teaching robots to interact with humans, we used the following criteria on the interactions to select the subfields of \gls{hri} relevant to our research:
\begin{itemize}
\item interaction between a robot and a human: both the human and the robot are influencing each other's behaviour
\item Socially Interactive Robots as defined in \citep{Fong2003}: the social
    side of the interaction is key (unlike physical human-robot interaction, such as an exoskeleton, physical rehabilitation or pure teleoperation as in robotic assisted surgery).
\end{itemize}

\subsubsection{Socially Assistive Robotics}

	\gls{sar} is a term coined in \cite{feil2005defining} and refers to robot providing assistance to human user through social interaction and has been defined by \cite{tapus2007socially} as one of the grand challenges of robotics.
	
	\paragraph{One of the principal application of \gls{sar} is care for the elderlies}
	Ageing of population is a known challenge for today and the near future: the \cite{united2017world} reports than \emph{population aged 60 or over is growing faster than all younger age groups}. This will decrease the support ratio (number of worker per retiree) forcing society to find ways to provide care to an increasing number of persons using a decreasing workforce. Robots represent a unique opportunity to provide for this lacking workforce and support elderlies potentially allowing them to stay at home rather than joining elderly care centres \citep{di2014web} or supporting the nursing home staff \citep{wada2004effects}.
	
	%For this reason, robots for the elderlies is one of the major robotic market today and for the next decades as demonstrated by the number of projects all around the globe targeting this issue: Robot-Era \citep{bevilacqua2012robot}, Accompany project \citep{amirabdollahian2013accompany}, CompanionAble \citep{badii2009companionable}  and the Robear project in Japan    \footnote{\url{http://www.riken.jp/en/pr/press/2015/20150223_2/}} to cite     only a small subset of them.	
	
    However, the acceptance of these robots in elderly care facilities or at
    their homes, is still a complex task. Multiple studies report a good
    acceptance of robot and positive effects on stress in home cares both for
    the elderly and the nursing staff \citep{wada2004effects}, but as mentioned
    in \citep{broadbent2009acceptance}, this acceptance could be increased by
    matching more closely the behaviour of the robots to the actual needs of the
    patients.	
		
	\paragraph{The second main application of SAR is Robot Assisted Therapy}
    In addition of providing social support, robot can also be used in the process of therapies, following a patient during their rehabilitation to improve their health, their acceptance in society or recover from accident.
    In therapies, robots have first been used as physical platform to help
    patient to recover physically from strokes or cerebral palsy for example
    \citep{sivan2011systematic}. They were primally used as mechatronic tools
    helping humans to accomplish repetitive task. But in the late 90s,
    robots have started to be used for their social capabilities. For example the
    AuRoRA Project \citep{dautenhahn1999robots}
    started in 1998 to explore the use of robot as therapeutic tools for
    children with \gls{asd}.
    Since AuRoRA, many projects have started all around the world to use robot
    to help patient with ASD, as shown by the review presented in
    \citep{diehl2012clinical} or more recently the \gls{dream} \citep{esteban2017build}. \gls{rat} is not limited to \gls{asd} only, the use of robots
    is also explored in hospitals, for example to support children with diabetes
    \citep{belpaeme2012multimodal}, to support elderly with dementia
    \citep{wada2005psychological} or to provide encouragement and monitor cardiac rehabilitation \citep{lara2017human}.	
	
\subsubsection{Education} 
	Social robots are also being used in education, supporting teachers to provide learning content to children. As presented in \cite{mubin2013review}, the robot can take multiple roles (the \emph{tool} role has bee excluded for this overview due to its non-social interaction).
	
	\paragraph{Robotic teachers providing class lessons.} 
	One of the most obvious role robots could have in a classroom is replacing the teacher to provide the content to the children in an lesson as presented in \cite{verner2016science}. However this approach is seldom used in robots for education. The aim is not to replace teachers but to support them with new tools to improve the teaching process both for the teachers and the children, the review by \cite{mubin2013review} do not even mention robot teachers as a possible role for robots in educations. 
	
	\paragraph{Robotics tutors aim to provide tailored teaching content to the child(ren) they are interacting with.} 
	Studies reported that individualised   feedbacks can increase the performance of students \citep{bloom19842}, but    due to the large number of students supervised by a single teacher in classes today, tutoring is complex to apply. Robotic tutors could provide this powerful one to one tailored interaction not available in current classroom without increasing the number of teachers. Studies have also shown that adding robot in a classroom can improve the children learning outcomes     \citep{kanda2004interactive}. Additionally, robots can be used at home to     elicit advantages over web or paper based instructions     \citep{han2005educational}. However as pointed by \citet{kennedy2015robot}, social behaviours have to be carefully managed as a robot too social could    decrease the learning for the children compared to a less social robot. 
	
	\paragraph{Peer robots learning alongside the children.}
	A peer robot will not mentor the child to learn certain concepts, but will learn with or from the child. Unlike other roles of robots in education, peer robots can fit new roles still vacant in education today. For example, in Co-Writer \citep{hood2015children}, the child has to teach a robot how to write, and as the child demonstrate correct handwriting, they improves their own. Peer robots can leverage the ``prot\'eg\'e effect'': learning by teaching an agent. The robot can take the role of a less knowledgeable agent with endless patience and encourage the student to perform repetitive tasks such as handwriting and improve.
	
\subsubsection{Search and rescue, and military} 
	
    Robots are already used during search and rescue missions. After a natural or artificial catastrophe, robots can be sent to analyse the damage area and report or rescue the surviving victims of the incident \citep{murphy2008search}. These robots have to interact socially with two kinds of humans partners: the survivors and the rescue team. In both cases the social component of the interaction is key: the survivor can be in a shocked state and the robot could be the only link they received with the external world after the accident. In this case, a social response is expected from the robot and it has to be carefully controlled. On the other side, the rescue team monitoring the robots is under pressure to act quickly and can be stressed too. Even if the robot does not display social behaviour, rescuers interacting with it migh develop some feeling toward the robots they are using during these tense moments and this has to be taken into account when designing the robot and its behaviour \citep{fincannon2004evidence}.
	
    Similar human behaviours (emotional bonding with a robot) have also been observed in the army. Soldiers developed feelings toward the robot they use in a daily basis: taking pictures with it, introducing it to their friends and so on. This relation can go as far as soldier risking their life to try to save the robot used by their squad \citep{singer2009wired}. In that case, the social side of the robot have to be carefully managed to prevent it to have an opposite effect that the one desired: preventing the waste of human lives.
		
\subsubsection{Hospitality and Entertainment} 
	
	Robots are also interacting with humans in hotels around the world. The Relay robot (Savioke\footnote{\url{http://www.savioke.com/}}) deliver commands from the guests of hotels from the reception to their room. Whilst the social interaction is still minimal today, these robots interact everyday with humans and provoke social reactions from them. On the research side, scientists have explored robots as receptionist in a hall at Carnegie Melon University \citep{gockley2005designing} and robots as museums and exhibitions guide since the late 90s \citep{thrun1999minerva,burgard1999experiences} and since explore how to improve the social interaction for guide robots.
	
	Similarly, robots guide and advices humans in shops and shopping mall. Long term studies explored how humans perceive and interact with robots in this environment \cite{kanda2009affective} and how robots should behave with clients \cite{kanda2008will}.

    Robots have also entered homes and family circles. Twenty years ago, Sony created Aibo, a robotic dog to be used as pet in Japanese families. An analysis of online discussion of owners published 6 year after their introduction gives interesting insights on the relationship that owners created with their robots \citep{friedman2003hardware}. For example 42\% of the members assigned intentionality to the robot, like preferences, emotions or even feelings. Similar behaviours can also be observed when the robot is not presented as a pet, but just a tool. In \cite{fink2013living}, authors reported that a participant was worried that their Roomba would feel lonely when they would be in holidays. More recently, the Pepper robot has been sold to family in Japan, however, as of March 2018 no english study has reported results of the interaction with family members or long term use.

\subsubsection{Collaborative Robots in industry}
	%TODO: check citations
	Robots in industry used to be locked behind cage to prevent humans to interact with them and getting hurt. However recently, social robots such as Baxter \citep{guizzo2012rethink} have been designed to collaborate with humans and share the same workspace interacting physically and socially with factory workers. These robots need to be usable and reprogrammable by non-robotic experts and need to convey and understand intentions from motions and eye gaze \citep{bauer2008human}. The topic of motion legibility: how to convey intention using only motion trajectory is being covered extensively by the literature: in \citet{dragan2013legibility} and successive work, authors present ways to improve clarity of intentions and so improve the collaboration between humans and robots.

	%TODO: rework
    In addition to legibility, an other important topic in human-robot collaboration is task assignment: if a goal has to be achieved by a human-robot team, the repartition of tasks should be carefully managed to  optimise the end result in term of task performance, but also to ensure comfort for the human. Multiple sets of implicit rules have to be taken into account in a human-robot collaboration context, and so the task repartition system should be aware of them and follow them as proposed in \cite{montreuil2007planning}.
	
	%TODO: expand by using 3rd hand project and other

\subsection{Constraints} \label{ssec:back_constraints}

    The previous section demonstrated that robots are already interacting with sensitive populations: young children, elderlies, persons with handicap or in a stressful situations (victims of catastrophes, soldier or persons requiring healthcare for example). As pointed previously in the case of robots for the elderlies, this number is likely to rise for the other categories too. In this context, failure to meet expectations or lack of social norms or awareness can physically hurt humans around it or lead to offence, anger, frustration or boredom. As such, the behaviour of robots interacting with humans need to be carefully managed to meet expectations and behave socially.
    
    These undesired behaviours can have many different origins: lack of sensory capabilities to identify necessary environmental features, lack of knowledge to interpret human behaviours appropriately, failure to convey intentions, impossibility to execute the required action or incorrect action policy. Due to the wide range of origin, this research focuses only on the last point: obtaining a correct action policy. The other issues are either orthogonal and would lead to failure even with an ``optimal'' action policy as external factors prevent the robot to solve a problem or can be handled by having an appropriate action policy selecting suboptimal actions when the optimal ones cannot be used, or reacting correctly to human behaviours without having to interpret them on a higher level. 
    
    Appropriate actions are highly dependent on the interaction context: they could aim to match users' expectations of a robot behaviour, or complete a specific task. Nonetheless, the intention behind being \textit{appropriate} here is that actions executed should be guaranteed not to present risk for the humans involved in the interaction; for example, preventing physical harm or mental distress while achieving the robot assigned objectives.

    Additionally, interactions with humans ``in the wild'' \cite{belpaeme2012multimodal} do not happen in well defined environments or rigid laboratory setups: in the real world, robots will have to interact in diverse environments, with a large number of different persons, during extended periods of time or with initial incomplete or incorrect knowledge: as such the action policy needs to be adaptable to the context, to the users and over time. Robots need to be adaptive, both to react to changing environments, and to improve its action policy over time.

    Lastly, in many cases today, interactive robots are not autonomous but partially controlled by a human operator. We argue that to have a real Human-Robot Interaction, the robot needs to be as autonomous as possible. As pointed out in \citet{baxter2016characterising}, by relying too much on a human to control a robot, we are shifting from a human-robot to a human-human interaction using a robot as proxy. 
    
    Section \ref{sec:back_behaviour} analyses the current state of the art in action selection for social robots interacting with humans following these three axes introduced in the previous paragraphs:

    \begin{itemize}
        \item Appropriateness of actions
        \item Adaptivity
        \item Autonomy
    \end{itemize}

    HRI being a large field, other research axes are equally important such as  the complexity of the interaction, the depth of the interaction, the constraints put on the environment, the ability of the robot to set its own goals or the dependence on social rules. However, these axes are more influenced by the goal and the context of the specific human-robot interaction taking place rather than the action policy itself, so for this literature review, the focus is be on the three axis mentioned previously.

    We argue that to be able to sustain meaningful HRI, robots should score highly in the three axes presented before, following these three principles:

    \begin{enumerate}
        \item Only execute appropriate actions.
        \item Have a high level of adaptivity.
        \item Have a high level of autonomy.
    \end{enumerate}

\subsubsection{Appropriateness of Actions} \label{ssec:appropriateness}
%TODO: take back from  here        
    As argued previously, much of social human-robot interaction takes place in  stressful or sensitive environments where humans have particular expectations about the robot's behaviour. Additionally, even in less critical situations, human-human interaction are subject to a large set of social norms and conventions resulting from precise expectations of the interacting partners \citep{sherif1936psychology}. Some of these expectations are also in place when interacting with robots \citep{bartneck2004design}.
	
    Failing to produce appropriate actions, for example by not matching the users' expectations, can have a negative impact on the interaction, potentially compromising future interactions. Similarly failing to behave appropriately can harm the persons interacting with the robots as shown in the example of the elderly user and the medication, or the therapeutic use of robots for children with ASD or for persons with dementia.  We argue that a robot requires a way to ensure that all the actions it is executing should not present risk to the human involved in the interaction.
	
    %Some scholars will argue that surprise is an important element in social
    %HRI, as it can revitalise the interaction and increase the engagement of
    %the users \citep{lemaignan2014dynamics}. With this principle of Least
    %Astonishment, we do not argue the robot needs to constantly produce a
    %non-surprising behaviour, but rather that it has to be in control of the
    %surprise its actions can produce and be able to select an action matching
    %the users' expectations. 

    Being able to behave appropriately is a real challenge: real interactions involve a large sensory space, with the interactants often being unpredictable. In addition, social interaction is grounded in a large number of often implicit norms, with expectations being highly dependent on the context of interaction.  It seems unlikely that an action policy covering every possibility can be provided to the robot before the start of the interaction. For this reason, social robots needs to have an action policy able to generate the appropriate reactions for the anticipated states, but also they have to be able to manage uncertainty: being able to select a correct action even when facing a sensory state with no explicit predefined action to do. 

    In this literature review, the appropriateness of actions axis is a continuous spectrum characterising how much the system controlling the robotis in control of the interaction and can act in a safe way for the users at any moment of the interaction. For example, a robot selecting its action randomly would have a low appropriateness as no mechanism prevent the execution of unexpected or undesired action. On the other hand, a robot continuously selecting the action that an expert would have selected would have a high value as domain experts should have the knowledge of which action is the correct one in this interaction domain.


\subsubsection{Adaptivity}	\label{ssec:adap}

    For reasons similar to the ones stated in subsection \ref{ssec:appropriateness} and as pointed by other research groups \citep{argall2009survey, hoffman2016openwoz}, an optimal behaviour is unlikely to be programmable by hand. Additionally, the end user can be from a different population intended by the designers of the robot controller, the environment where the robot will be used might not be perfectly defined or the desired behaviour might need to be customisable by or for the end user. For these reasons, the  robot needs to be able to update its action policy to improve its behaviour. We use the term \emph{adaptivity} to represent this ability to change the action policy. This adaptivity has three components: the adaptivity in space (being able to change an action policy according to the environment), the adaptivity in users (being able to change an action policy according to the user) and the adaptivity in time (being able to change an action policy over time). 

    The same robot might be expected to interact in different environments. For example a robot used as an assistant for elderly people will have to interact in the home of the owner, but can also have to follow the owner in the street or in a supermarket. In these different environments, different behaviour will be expected from the robot, so to be able to behave accordingly, the robot has to be adaptive in space.

    Additionally, in most of the application field presented earlier, robots have to interact with a large number of users, and often, these interaction partners are not know in advance: in education the name and specificities of every child can hardly be specified in advance. In entertainment or search and rescue, none of the user is known beforehand. Adaptivity can be a way to discriminate the different users and adopt the action policy the more suited to the current user and update it according to this user's behaviour.

    Similarly, seeing the different fields where robots have to be social, there is an expectation to interact over an extensive period with the same user, e.g. companion robots for the elderly, military robots for a squad or robots used in therapeutic settings. With these long interactions, adaptivity in time allows the robot to tailor its behaviour to the user it is interacting with and makes it able to track the changes of preferences that could occur over long period of interaction. Adaptivity in time can also allow the robot to learn from its errors and be able to manage uncertainty better as it is interacting.

    For this review, adaptivity will be a continuous scale ranging from no adaptivity at all (the robot has a single action policy that it will use in all the interactions), to high adaptivity (the robot is able to change its action policy during an interaction and adapt it to the persons and context of the interaction). As this adaptivity is over three axes, some robots can  have a high adaptivity in users (by adapting their behaviour to the actions of their interactants), but not in time (if the same inputs always trigger the same output), and not in space (if only one specific context of interaction is taken into account). In that case, the action selection mechanism will receive a relatively low adaptivity rating.

\subsubsection{Autonomy}
	
    We will show in subsection \ref{subsec:WoZ}, that as of today, many
    experiments are conducted using a robot tele-operated by a human. Whilst
    having a human controlling the robot presents many advantages, e.g. the
    human can provide the knowledge and the adaptivity required or has sensing
    and reasoning capabilities not yet implemented on the robot, multiple
    reasons push us away from this type of interaction as stated by
    \citet{Thill2013}. It is not a scalable method to interact for a long time
    or on large scale, the human-robot interaction tends to become a human-human
    interaction \citep{baxter2016characterising} and it might introduce multiple
    biases in the robot behaviour \citep{howley2014effects}. For these reasons
    among many, we argue that a robot used in social HRI should be as autonomous
    as possible.
	
    The third axis we will use in this literature review is the autonomy. As
    stated by 	 \citet{beer2014toward}, autonomy is organised following a
    spectrum from no autonomy at all: a human is totally controlling the robot
    (doing sensory perception, analysis and action selection) to a full
    autonomy: the robot is capable to sense and act in it's environment without
    relying at all on a human. Some systems use a hybrid combination of human
    control and autonomy. In these shared autonomy system, the boundary between
    the autonomous control and the human can happen on several levels: from
    labelling of sensory inputs \citep{depalma2016nimbus} to assign reward to
    action to teach the robot an action policy \citep{thomaz2008teachable}.
    Similarly, this help can be triggered by the robot or the human, at specific
    stages of the interaction or at any time or can be a simple guidance or a
    command. 

\section{Current robot behaviours in HRI} \label{sec:back_behaviour}

    This section will present diverse approaches currently used in HRI to allow
    a robot to select an action. As the number of individual techniques is too
    large for an exhaustive review, we organised the literature into broader
    categories. For each category, we will present the corresponding approach,
    indicate leading works done in this direction and qualitatively rate it on
    the three axes defined in the previous section.

\subsection{Wizard of Oz} \label{subsec:WoZ}

    Wizard of Oz is a specific case of tele-operation where the robot is not
    autonomous but at least partially controlled by an external operator to
    create the illusion of autonomy in an interaction with a user. It outsources
    the difficulty of action selection or sensory interpretation to a human
    controller. This technique has emerged from the Human Computer Interaction
    field in 1983 \citep{kelley1983empirical} and is today common practice in
    HRI \citep{riek2012wizard}. It is even so widely used that researchers are
    promoting the use of a single framework shared by the community
    \citep{hoffman2016openwoz}.
	
    Wizard of Oz in itself is a large field and there exist two variations of
    this method as shown in \citep{baxter2016characterising} where the authors
    discriminate Wizard of Oz into two categories: perceptual Wizard of Oz and
    cognitive Wizard of Oz depending of the level of involvement of the operator
    in the action selection process. As shown later is this review, this method
    is also important because other approaches use it to provide data to make
    the robot more autonomous. 

    Some systems can also combine human control and predefined autonomous
    behaviour,  \citet{shiomi2008semi} propose a semi-autonomous communication
    robot. This robot is mainly autonomous, but has the ability to make explicit
    request to a human supervisor in predefined cases where the sensory input is
    not clear enough to make an action.

    With Wizard of Oz, the adaptivity and the appropriateness are provided
    almost exclusively by the human, so these characteristics are dependent of
    the human expertise and are generally optimal. However, due to the reliance
    on human supervision to control the robot, the autonomy is low. For
    semi-autonomous robots, the picture is more blurry, as the takeover from the
    human could be triggered by the robot or by the human and the information
    shared and the quantity of human control on the robot will have an important
    impact on the three axes. For example, in the system proposed in
    \citep{shiomi2008semi} the robot explicit makes request to the human, but
    the human cannot take the initiative to step in the interaction limiting the
    adaptivity (no learning mechanism is added) and as no mechanism prevents the
    robot to make undesired decision, it can still create more surprised on the
    users compared to Wizard of Oz.
	
\subsection{Fixed preprogrammed behaviour}

    One of the simplest ways to have a robot interacting with a human is to have
    an explicit fixed behaviour. The robot is fully autonomous and follows a
    script or a finite state machine for action selection. This approach is
    dependent on having a well defined and predictable environment to have the
    interaction running smoothly. If the interaction modalities are limited and
    the interaction's goal precise enough or if a mediator is used for the
    interaction and limits the possible actions from the human and the robot, a
    optimal behaviour for the robot can be predefined for all (sensible) human
    actions.

    This is the approach followed in a large number of study in HRI, from the
    use of robot in schools for robotic tutors teaching a second language to
    children \citep{kennedy2016social} to studies investigating psychological
    traits in HRI such as trust for example \citep{kahn2015will}.

    In essence, this type of controller has a no adaptivity over time, as all
    the possible behaviour and conditions are predefined in advance and the
    robot has a single action policy. However this method is well suited for
    many human-centred studies where the adaptivity in time of the robot is not
    required. Additionally, a robot could be programmed with different
    behaviours, and then select the one corresponding to the current interaction
    following rules given prior the experiment. For example, in
    \citet{leyzberg2014personalizing} the robot can deliver some predefined
    content according to the current performance of the participant, presenting
    personalised behaviour as long as the participants is behaving as expected.
    In that case, the system would have a low adaptivity: the different types of
    users are already predefined and there is no adaptivity in space (a single
    interaction context is preprogrammed) and no adaptivity in time.

    This method scores highly on autonomy as no external human is required to
    control the robot. The robot behaves appropriately, as the environment is
    generally constrained enough to limit the robot's and the human's actions.
    However, as everything is specified in advance, the adaptivity is low.

\subsection{Reactive behaviour}

Reactive controllers are systems reacting directly to input data without
trying to reach specific goals. %For example, the subsumption architecture
presented by Brooks in \citep{brooks1986robust}. The main idea is not to
have an explicit behaviour to react to each situation, but to have a set of
possible behaviours competing for the control of the robot.  For example,
robotic control have been inspired by homeostasis, the tendency to keep
multiple elements at equilibrium. This property is observed in many
physiological systems, and approaches following a similar direction and have
been utilised in social HRI. For example, \citet{breazeal1998motivational}
use a set of drives (social, stimulation, security and fatigue) which are
represented by a variable each and have to be kept within a predefined
range. If these values are outside the desired homeostatic range, the robot
is either over or under-stimulated and this will affect its emotion status
and it will display an emotion accordingly. Homeostasis approaches have also
been extended in diverse directions. \citet{cao2014robee} present a system
based on a homeostasis subsystem to generate drives, which are equivalent to
temporary goals, which then use planning techniques (cf. sub-section
\ref{subsec:planning}) already predefined to satiate this particular drive.  

Due to the implicit description of behaviours these methods are more robust
in unconstrained environments than a purely scripted controller, while
remaining totally autonomous. However the action policy is not adaptive and
as the behaviours are not totally defined and controlled, there is no
guarantee against the robot acting in inconsistent way in some specific
cases limiting the appropriateness of actions.

Efforts have been made to extend the homeostasis approaches beyond purely
reactive systems with the use of hormone models  for example
\citep{Lones2014}. This method allows the previous experiences of the robot
to impact the behaviour expressed providing limited adaptivity in time to
the robot. However this approach was only applied to a robot interacting in
a non-social environment.

\subsection{Learning from Demonstration}
	% HRI is in the teaching, not in the application - often manipulation
	% Except Liu (learning from data) and restricted WoZ

    Machine  learning is a promising method to provide a robot with an adequate
    action policy without having to implement in advance all the features used
    by the action selection mechanism. Offline learning is a technique allowing
    the robot to change its action policy over time by updating the action
    policy outside of the interaction. Between interactions, a learning
    algorithm is used to create a new action policy derived from the previous
    experiences.

    As when interacting it might be complicated and time consuming to acquire
    data points for learning, offline learning methods are mainly inspired from
    the Learning from Demonstration framework (LfD) \citep{argall2009survey}.
    With LfD, the idea is to take inspiration from human demonstrations to
    accelerate the learning for an agent or to teach tasks that could not be
    preprogrammed manually \citep{billard2013robot}. The classical approach
    starts with observing a human completing the task and then deriving a
    corresponding robot behaviour to match the human one. In most of the cases,
    the learning only occurs once: data is accumulated first and then batch
    learning is applied to derive a static action policy. 

    For example, \citet{liu2014train} presents a data driven approach taking
    demonstration from human-human interactions to gather the relevant features
    defining human social behaviour. Motion and speech are recorded from about
    180 interactions in a simulated shopping scenario and behaviours are
    clustered into \emph{behaviour elements}. Finally, during the interaction,
    the robot uses a variable-length Markov model predictor to estimate the
    selection probability of each actions by the human demonstrator, and then
    selects the one with the highest. According to the authors, the current
    performance of the robot is not perfect, but if this approach was scaled
    using a larger dataset gathered from normal human-human interactions in the
    real world, the performance is expected to improve.

    Alternatively, Knox et al. propose the \emph{Learning from Wizard} approach
    in \citep{knox2014learning}. The first phase also consists on data
    collection, the robot is first tele-operated by an expert in a Wizard of Oz
    setting. Once enough data has been gathered, a learning algorithm is applied
    to derive an action policy. However this paper presents no description of
    which algorithm could be use or how, and gives no evaluation of the
    approach, but instead only offers a reflection on the application of this
    idea.

    \citet{sequeira2016discovering} presents a complete approach to obtain a
    fully autonomous robot tutor. This method is composed of multiple steps
    starting with the observation of a human teacher performing the task. Then,
    the different features used by the human demonstrator to select his actions
    as well as the actions themselves are encoded and implemented in a robot.
    The next step is setting up a Wizard of Oz experiment where the operator has
    access to the same features than the robot to make his decisions and
    controls the robot's action. Then, a combination manually derived rules and
    machine learning is applied on the data from the restricted-perception
    experiment and finally the robot is tested autonomously. Additional offline
    refinement steps are possible if the behaviour is not exactly the one
    desired. 

    Both \citet{knox2014learning} and \citet{sequeira2016discovering} stress the
    importance of using similar features for the Wizard of Oz part than the ones
    available to the robot during the autonomous part: whilst decreasing the
    performance in the first interaction, it allows more accurate learning due
    to the similarity of inputs for the robot and the human controlling it.

    As these methods are based on real interactions either between humans, or
    between humans and robots controlled by humans, with enough demonstrations
    the robot should be able to select the appropriate actions. However, as no
    intrinsic mechanism is present to prevent the execution of undesired actions
    which could happen if the robot ends up in an unseen state, the
    appropriateness of actions cannot be maximal. Additionally, the adaptivity
    in time is limited, as for most of the techniques the learning happens only
    once and then the behaviour is fixed and the learning is only used in a
    specific context. However, the framework proposed by restricted-perception
    Wizard of Oz should allow asynchronous adaptivity in time using the
    refinement phase. And finally, all these methods require the presence of
    humans in a first phase but the robots are fully autonomous later in the
    interaction, so the autonomy is high during the main part of the
    interaction.

\subsection{Planning} \label{subsec:planning}

    An alternative way to interact in more complex environment is to use
    planning. The robot has access to a set of actions with preconditions and
    postconditions and has to achieve a defined goal. To do so, it follows the
    three planning steps: sense, plan and act. The first step, sense, is to
    acquire information about the current state of the environment. Then,  based
    on the set of actions available and the goal, a plan (i.e. a succession of
    actions) is created which should result in reaching the goal, while
    respcting pre- and post conditions when selecting actions. Finally, the last
    step is to execute the plan. If the resulting state of the new environment
    is not the one desired, the robot replans to find a solution suitable for
    the new state.

    %This approach is often used in motion planning, but can integrate some
    %social aspect as presented in the work of Dragan and colleagues
    %\citep{dragan2013legibility}, where motion planning is adapted to be more
    %legible by human. Planning can also be used at a higher level for action
    %selection.

    In the literature, planning has not been used to generate an action policy
    to interact social with a human, however it has been used to assign tasks to
    both members of a human-robot team to achieve a defined task. One example of
    this is the Human Aware Task Planner \citep{alili2009task}. One property of
    this planner is the ability to take into account predefined social rules,
    such as reducing human idle time, when creating a plan specifying what the
    human and robot should do.

    Planning performance depends heavily on the model of the environment the
    robot has access to. A detailed model can ensure that the robot will select
    the appropriate action whilst being totally autonomous. Similarly, the
    adaptivity depends on the model the robot has access to and whether it can
    update it in real time. However, in many cases when interacting with humans,
    the model is static, only covering a subset of the different tasks that the
    robot can be required to achieve and the different contexts it can face.

    %As long as the model is correct enough, it is ensured to keep the
    %``astonishment'' low and to maintain a high level of autonomy. The
    %adaptivity is also highly dependent on the model the robot has access to.
    %If the planning domain is large enough, the adaptivity can be high,
    %multiple users can be also predefined to increase the user adaptability,
    %and if the domain knowledge can be updated dynamically.

    %\marginpar{The previous paragraph is a bit unclear to me, but you can leave
    %it in the RDC2. If you use it anywhere else, it would need to be
    %rewritten.}

    Planning can also be extended with learning, which then allows for adaptive
    action policies. This has been done in motion planning, to obtain a better
    trajectory \citep{jain2013learning,beetz2004rpllearn} and action selection
    planning \citep{kirsch2009robot}. But to our knowledge, no planner used in
    social HRI includes a module allowing it to change its model by increasing
    the number of actions, adding new rules or changing the pre- and
    postcondition of actions at runtime.

\subsection{Learning from Interaction}


Diarc

That's where we sit, but not often applied for HRI - next parts describe the options

\section{Reinforcement Learning} %Not
 applied to learn to interact

\subsection{Concept}

\subsection{Limitations}

\subsection{Opportunities}

\section{Interactive Machine Learning}

\acrfull{iml}, as coined by \cite{fails2003interactive}, differs from \acrfull{cml} by integrating an expert end-user in the learning process. In classical supervised learning, such as deep leanrning \cite{lecun2015deep} , the learning phase happens offline once to obtain a classifier for later use. On the other hand, \acrshort{iml} is an iterative online process using a human to correct the errors made by the algorithm as they appear.

\subsection{Goal}

The main goal behind \gls{iml} is to leverage the human knowledge during the learning process to speed it up. As explained in \cite{fails2003interactive}, algorithms gain to be fast rather than highly inductive. \cite{amershi2014power} present a review %Read and continue user modify behaviour based on learner output
%amershi provide an introduction to IML
simultaneous development and usage

\subsection{Active learning}
%More supervised - learner driven - reduce workload on user

User are human and want to be considered as such, not oracle (want more control) not willing to be simple oracle \cite{cakmak2010designing} %Read paper

\subsection{Human as a source of feedback on actions}
%More reinforcement, loftin, Crayon... (cf AI-HRI)
\cite{isbell2006cobot}

When given choice, human will never teach only using rewards \cite{kaochar2011towards} %See if citation makes sense
%Human don't want to provide only label, they want to explain \cite{stumpf2007toward} + importance on transparency: help to achieve better results and improve user experience

%Need to careful not to just rewrite power to people + need to add new material - with deep learning and stuff
\subsection{Importance of control}

\cite{amershi2014power} emphasise throughout their paper the desire of user to have more control over the learning progress (through timing, suggestions, corrections, decisions...).

\subsection{Challenges}

DIARC?

    As explained before, due to the large number of interactions needed and due
    to the risk presented by blind exploration, pure RL has not been used in
    HRI. However, variants have been proposed to use human expertise to
    bootstrap the learning of RL based algorithm to learn non social tasks
    \citep{kober2013reinforcement}. \citet{thomaz2008teachable} present the
    Interactive Reinforcement Learning, a method combining feedback from the
    environment and feedback from a human supervisor to learn a task in a
    non-social context. Similarly, \citet{knox2009interactively} propose to use
    RL in an environment where the feedback is not given by the environment
    itself, but by a human only. In these two cases, human feedback is used to
    scaffold the learning: it makes it faster and safer and can allow the use of
    RL in environments without explicit reward. However as the feedback is
    always given after the execution of the action, there is no guaranty that
    only expected actions will be executed. This might be a reason why these two
    approaches have not been used to learn an action policy for social
    human-robot interactions. But they are nevertheless relevant as they are
    using the interaction with a human to help an agent to learn an action
    policy faster and safer than pure RL.

    As stated in \citet{garcia2015comprehensive}, two ways exist to make the RL
    safer: either a mechanism is present to prevent the execution of non-safe
    actions or enough initial knowledge is provided to ensure that the robot is
    staying in a safe zone. These two methods can also be used with other
    machine learning techniques. 

    An example of the first method is presented in \citet{chernova2009}, where
    the authors used a confidence-based autonomy approach to control a robot. It
    combines learning from demonstration and active learning
    \citep{johnson1991active} to limit the risks of the exploration and increase
    the learning speed. Initially the robot is provided with demonstrations of
    the task by a human and then has to try on its own. However, as it is
    expected from a human, the demonstrations are not fully consistent so the
    robot might find states where there is no obvious action to select. A human
    expert is also present and the robot can ask a demonstration for the points
    where the uncertainty is high. 

    Using human guidance in high uncertainty states can help the robot to select
    appropriate actions even when some knowledge is missing at the robot's side.
    However this approach, similarly to \citet{shiomi2008semi}, is limited to
    the cases where the robot can estimate the confidence in its sensors and its
    action selection mechanism. One of the differences between the approach
    presented in \citet{shiomi2008semi} and in \citet{chernova2009} is that in
    the latter this information is used for learning to improve the action
    policy. One of the key points of these method is the uncertainty estimation,
    if the estimation is not precise enough, the robot can fail to detect
    ambiguous states and act in an undesired manner.

    The second way to have a safer learning (exploring only around a known safe
    action policy) is followed by \citet{Abbeel2004}. In this paper, the authors
    used inverse reinforcement learning to teach a flying behaviour to a robotic
    helicopter. In this case, the robot is provided with demonstration of a safe
    policy, and then derives from these examples the reward function supposed to
    defined the expected optimal behaviour. Finally classical RL is applied
    around the provided policy to explore and to optimise the policy according
    to the estimated reward function.

    Similarly to autonomous online learning, these approaches score high in
    adaptivity in time and depending of the learning mechanism, sensory inputs
    and actions, they also do well in being adaptive in the search space and to
    changing user behaviour. The reliance on human input decreases the autonomy,
    but increases the appropriateness of actions as this guidance can help to
    reduce the use of random exploration. However, as the human has often only a
    partial control, the robot is not ensured to act correctly at every stage of
    the interaction preventing the appropriateness to be maximal. 

    It should be noted that these approaches have not been used to learn
    behaviours for social human-robot interaction, probably due again to the
    complexity of the interaction and due to the probability to execute
    non-desired actions for the current online learning methods.
	

%Add litt reviews from AIHRI and R4L


\section{Summary}
