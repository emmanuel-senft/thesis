%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Background} \label{chap:background}
\glsresetall

This chapter describes social \gls{hri} and presents related research in agent and robot control. The first section introduces the different fields of application of social \gls{hri} and draws from them requirements for controlling a robot interacting with humans (the robot should: only execute appropriate actions and have a high level of adaptivity and autonomy). The second section provides the current state of the art in robot control for \gls{hri} and analyses it through the requirements presented in the previous section. And finally, the third section presents \gls{iml}, an alternative method holding promises to teach agents how to interact and how it could be applied to \gls{hri}.

\section{Social Human-Robot Interaction}

\subsection{Fields of application} \label{ssec:back_hri}

\gls{hri} covers the full spectrum of interactions between humans and robots ranging from physical assistance and teleoperation to social companions. However, this thesis being focused on teaching robots to interact socially with humans, the following criteria on the interactions were used to select the subfields of \gls{hri} relevant to this research:
\begin{itemize}
\item Presence of an interaction between a robot and a human: both the human and the robot are influencing each other's behaviour.
\item The robot is ``socially interactive'' \citep{Fong2003}. The \emph{social} element of the interaction between the robot and the human is  key. As such, it rules out purely physical human-robot interaction, such as exoskeleton, physical rehabilitation or pure teleoperation as in robotic assisted surgery.
\end{itemize}

Applying these constraints on \gls{hri} resulted on five subfields involving social interactions between robots and humans: 
\begin{itemize}
	\item \acrfull{sar}
	\item Education
	\item Search and Rescue, and Military
	\item Hospitality and Entertainment
	\item Collaborative Robots in Industry
\end{itemize}

\subsubsection{Socially Assistive Robotics}

	\gls{sar} is a term coined by \cite{feil2005defining} and refers to a robot providing assistance to human users through social interaction. This field has been defined by \cite{tapus2007socially} as one of the grand challenges of robotics.
	
	One of the main applications of \gls{sar} is care for the elderly. In the near future, the ageing of population will have large impacts on the world, and societies will have to find solutions to tackle this challenge. The \cite{united2017world} reports that the ``population aged 60 or over is growing faster than all younger age groups''. This unbalance of growth will decrease the support ratio (number of worker per retiree) forcing societies to find ways to provide care to an increasing number of people using a decreasing staff. Robots represent a unique opportunity to compensate for this lacking workforce potentially allowing elderly to stay at home rather than joining elderly care centres \citep{di2014web} or simply to support the nursing home staff \citep{wada2004effects}.
	
	%For this reason, robots for the elderlies is one of the major robotic market today and for the next decades as demonstrated by the number of projects all around the globe targeting this issue: Robot-Era \citep{bevilacqua2012robot}, Accompany project \citep{amirabdollahian2013accompany}, CompanionAble \citep{badii2009companionable}  and the Robear project in Japan    \footnote{\url{http://www.riken.jp/en/pr/press/2015/20150223_2/}} to cite     only a small subset of them.	
	
	%Might want to remove
    %However, the acceptance of these robots in elderly care facilities or at their homes may still be problematic. Multiple studies report a good acceptance of robot and positive effects on stress in home cares both for the elderly and the nursing staff \citep{wada2004effects,tamura2004entertainment}. But as mentioned by \citet{broadbent2009acceptance}, this acceptance could be increased by matching more closely the behaviours of the robots to the actual patients' needs and expectations.
		
	The second main application of \gls{sar} is \gls{rat}. Robots might might be used to provide therapies or to follow and support a patient during their rehabilitation to improve their health, their acceptance in society or recover better from an accident. In therapies, robots were first used as physical platforms to help patient in rehabilitation therapy during the 80s \citep{harwin1988robot}. During this period, robots were primally used as mechatronic tools helping humans to accomplish repetitive task. But in the late 90s, robots started to be used for their social capabilities. For example the AuRoRA Project \citep{dautenhahn1999robots} started in 1998 to explore the use of robot as therapeutic tools for children with \gls{asd}. Since AuRoRA, many other projects, such as the DREAM project\footnote{\url{https://www.dream2020.eu}}, have started all around the world to use robots to help patient with ASD \citep{diehl2012clinical,esteban2017build}. 
	
	\gls{rat} is not limited to \gls{asd} only, the use of robots is also explored in hospitals, for example to support children with diabetes \citep{belpaeme2012multimodal}, to support elderly with dementia \citep{wada2005psychological} and stroke recovering patients \citep{mataric2007socially} or to monitor and provide encouragement in cardiac rehabilitation therapies \citep{lara2017human}.	
	
	These examples demonstrate that already today, robots are interacting with sensible populations (the elderly or patients in therapy). This implies that robots' social behaviours need to be constantly correct to ensure that no harm will be caused to these populations. And due to the shortage of workforce, these robots need to be as autonomous as possible.
	
\subsubsection{Education} 
	Social robots are being used in education, supporting teachers to provide learning content to children, transforming the teaching process from passive learning to active learning \citep{linder2001facilitating}. In education, robots can take multiple roles such as peer, tutor or tool \citep{mubin2013review}. It should be noted that Mubin et al. stress that the intentions of the robotic community is not to replace teachers by robots, but provide them with a new form of technological teaching-aid. Nevertheless, \cite{verner2016science} presented positive results from an early stage study using a tall humanoid robot to deliver a science lesson to children. The remaining of the section will describe more in details the roles of tutor and peer robots (the `tool' role has been excluded due to the lack of social interaction between the robot and the students).
	
	\paragraph{Robotics tutors providing tailored teaching content in 1 to 1 interaction.} 
	Individualised feedback has been shown to increase the performance of students \citep{cohen1982educational,bloom19842}. However, tutoring requires a larger trained staff and as such is more costly than large classes supervised by a single teacher. For this reason, tutoring is seldom used in public education today. Tutoring is however often available through private teacher at additional cost for the parents, potentially increasing social inequalities \citep{bray2009confronting}. Robotic tutors could provide this powerful one to one tailored interaction to every student during school time, leading to higher learning gain for the children without dramatically increasing the workload on teachers  \citep{kanda2004interactive,leyzberg2012physical,kennedy2016social,gordon2016affective}. In addition to classroom uses, robots could also support children at home as they have been shown to elicit advantages over web or paper based instructions \citep{han2005educational}. 
	
	\paragraph{Peer robots learning alongside the children.}
	Unlike other types of agents in education, peer robots have the opportunity to fit a special role seldom present in education today: the role of care-receiver rather than care-giver \citep{tanaka2012children}. Peer learning has demonstrated benefits both for the helper and those helped in \gls{hhi} \citep{topping2005trends}. In \gls{hri}, a peer robot does not mentor a child to teach them new concepts, but learns alongside or from them, supporting them during the process and encouraging these children to produce behaviours improving their own learning. For example, in the Co-Writer project \citep{hood2015children}, a child has to teach a robot how to write, and as the child demonstrates correct handwriting to the robot, they improve their own skills at drawing letters. Peer robots are able to leverage the concept of learning by teaching \citep{frager1970learning} and peer learning \citep{topping2005trends} in a way hardly matched by humans. The robot can take the role of a less knowledgeable agent with endless patience and encourage the student to perform repetitive tasks such as handwriting and improve. In a similar position, an adult would not be a believable agent requiring learning and younger students might not have the compliance and the patience to learn from another child.
	
	To provide efficient tutoring or peer support, robots need to be able to personalise their behaviour to the children they are interacting with in order to maximise the children's learning gain. Additionally, as pointed by \citet{kennedy2015robot}, a robot too social could decrease the learning for the children compared to a less social robot, consequently the robot's social behaviours have to be carefully managed to ensure its effectiveness. 
	
\subsubsection{Search and Rescue, and Military} 
	
    Robots are already deployed in the real world, outside of labs and used during search and rescue missions \citep{murphy2008search}. For instance, after a natural or artificial catastrophe, robots have been sent to analyse the damaged area and report or rescue the surviving victims of the incident. During these missions, robots have to interact socially with two kinds of human partners: the survivors and the rescue team. In both cases the social component of the interaction is key: the survivor is probably in a shocked state and the searching robot could be the first link they have with the external world after the accident \citep{murphy2008search}. In this case, a social response is expected from the robot and it has to be carefully controlled. On the other side, the rescue team monitoring the robots is under high pressure to act quickly and faces traumatic events too. Even if the robot does not display a social behaviour, rescuers interacting with it might develop some feeling toward the robots they are using during these tense moments \citep{fincannon2004evidence}.
	
    Similar human behaviours (i.e. emotional bonding with a robot) have also been observed in the army \citep{singer2009wired}. Robots have been deployed as teleoperated drones and ground units alongside soldiers to complete scouting task or cleaning minefields. By interacting with a robot for extensive periods, some soldiers developed feelings toward this robot they used in a daily basis: taking pictures with it and introducing it to their friends. These relationships have gone as far as soldier risking their own life to in order to save the robot used by their squad \citep{singer2009wired}. 
    
    These examples in these two fields demonstrate that the sociability of the robot has to be taken into account when interacting in such stressful environments. Overlooking the importance of social relationships human will form can lead to dramatic consequences (e.g. soldiers taking risks for the robot). As such, during the interaction, the robot's behaviour needs to be constantly appropriate not to create misleading expectations and to ensure that the goal of the interaction will be met.
		
\subsubsection{Hospitality and Entertainment} 
	
	Robots are also interacting with humans in hotels around the world; for example, the Relay robot (Savioke\footnote{\url{http://www.savioke.com/}}) delivers amenities directly to the guests' rooms in more than 70 hotels in the US, Europe and Japan\footnote{\url{https://www.spectrum.ieee.org/view-from-the-valley/robotics/industrial-robots/ces-2018-delivery-robots-are-fulltime-employees-at-a-las-vegas-hotel}}. Whilst the social interaction is still minimal today, these robots interact everyday with humans and have been seen evoking social reactions from them\footnote{\url{https://www.fastcodesign.com/3057075/how-savioke-labs-built-a-robot-personality-in-5-days}}. 
	
	On the research side, scientists have designed and tested a robot as a receptionist in a hall at Carnegie Melon University \citep{gockley2005designing}. Since the late 90s, researchers also explored how robots could guide visitors in museums and exhibitions \citep{thrun1999minerva,burgard1999experiences}. These researches continue today to explore how to improve the social interaction between tourists and these guide robots \citep{bennewitz2005towards}. Similarly, robots also inform and advise humans in shops and shopping malls. Long term studies have explored how humans perceive and interact with robots in this environment \citep{kanda2009affective} and how robots should behave with clients \citep{kanda2008will}.

    Robots have also entered homes and family circles: from vacuum cleaners (e.g Roomba) to companion robot (e.g Pepper) passing by pet robots (e.g Aibo). A notable example is Aibo: twenty years ago, Sony created Aibo, a robotic dog to be used as pet in Japanese families and a new version was released in early 2018. An analysis of online discussions of owners published 6 years after Aibo's first introduction gives insights on the relationship that owners created with their robots \citep{friedman2003hardware}. For example 42\% of the community members assigned intentionality to the robot, such as preferences, emotions or even feelings. Similar behaviours have also been observed when the robot is not presented as a pet, but even just a a tool. For instance, \cite{fink2013living} report that one of their participants was worried that their Roomba would feel lonely when they would be away on holiday. More recently, the Pepper robot has been sold to families in Japan\footnote{\url{https://www.softbankrobotics.com/emea/en/robots/pepper}}. However, as of early 2018, no study in English has reported results of the interactions with families or long term use and acceptance.
    
    Similarly to previous fields, robots' behaviour (social or not social) will impact their users in ways hardly predictable in advance. This reinforces the necessity for their behaviour to remain appropriate in any step of the interaction. Additionally, when interacting with unknown users, the robot will have to face a wide range of users' expectations, and react to different unanticipated behaviours. This forces the robot to adapt to these different users and react according to their needs and desires. Finally, robots will also be deployed to interact with the same people over long periods. To sustain engagement over such time scales, these robots need to change their behaviour over time to overcome possible boredom due to the vanishement of the novelty effect in repeated interactions \citep{salter2004robots}, for instance by referring to previous experiences and enrich their behaviour \citep{leite2013social}.

\subsubsection{Collaborative Robots in Industry}

	In industry, robots used to be locked behind cages to prevent humans to interact with them and getting hurt. However, recently social robots, such as Baxter \citep{guizzo2012rethink}, have been designed to collaborate with humans; they share the same workspace and interact physically and socially with factory workers. As these robots need to be safe to human interacting with them, they are often softly actuated, using active or passive compliance to avoid the risk related to collision between a stiff robot and a human. Additionally, when collaborating with people, the legibility of motion is fundamental. In other words, to interact efficiently and safely with humans, robots need to make their intentions clear to humans surrounding them \citep{dragan2013legibility} and reciprocally, they also need to interpret human social cue to infer their goals and intentions.
	
	Beyond safety and legibility, another key challenge in \gls{hrc} is task assignment: if a goal has to be achieved by a human-robot team, the partitioning of tasks should be carefully managed to  optimise the end result in term of task performance, but also to ensure comfort for the human. Explicit and implicit rules and personal preferences describe the role and behaviours of each participant and have to be taken into account in \gls{hrc}. As such, the task repartition system and other planners used in \gls{hrc} should be aware of these social norms and follow them \citep{montreuil2007planning}. For example, recent work done in the 3\textsuperscript{rd} Hand project\footnote{\url{http://3rdhandrobot.eu/}} explored how a robot should support a human in a collaborative assembly task by adapting its behaviour to this human's personal preferences and how this adaptation improve the team's efficiency \citep{munzer2017efficient}.
	
	As demonstrated in \cite{munzer2017efficient}, intelligent systems involved in \gls{hrc} should adapt their behaviour to the interaction partners, be aware of preferences and rules to follow to ensure that the robot's behaviour is always appropriate, efficient and safe for the humans involved in the interaction. An additional challenge faced by robots with the recent advances in \gls{ml}, and especially with the omnipresence of neural networks and deep learning, is \gls{xai} \citep{wachter2017transparent}. As agents learning to interact will make mistakes and behave unexpectedly from time to time, they have to be able to provide explanations for these errors in a way understandable by humans. This challenge is especially visible in \gls{hrc} where both humans and robots aim to collaborate to complete a task together. \cite{hayes2017improving} propose to achieve transparency through policy explanation, by allowing robots to answer questions and explain their behaviour by self observation and logic deduction. This transparency aims at increasing trust between the agents involved in the interaction and improve the robot's efficiency.

\subsection{Requirements on Robots Interacting with Humans} \label{ssec:back_constraints}

    The review in Section \ref{ssec:back_hri} demonstrated that robots are already interacting with vulnerable populations: young children, the elderly, people requiring healthcare or in a stressful situations (victims of catastrophes or soldiers for example). Additionally, people tend to a create emotional bonding with these robots even if they are not interacting socially with their users. As such, the behaviour of robots interacting with humans need to be carefully controlled to manage humans' expectations and ensure their safety. In other words, robots need to constantly behave in socially acceptable manners, avoiding any confusing, inappropriate or dangerous behaviour. Failure to do so might prevent the interaction to fit its goal or even lead to physical injuries or potentially elicit offence, anger, frustration, distress or boredom. 
    
    These undesired behaviours may come from different origins: lack of sensory capabilities to identify necessary environmental features, lack of knowledge to interpret human behaviours appropriately, failure to convey intentions, impossibility to execute the required action or incorrect action policies. Due to the wide range of origins of these potential social faux-pas, this research focuses only on the last point, obtaining an appropriate action policy: assuming a set of inputs, finding a way to have the robot select an appropriate action. The other issues are either orthogonal and would lead to failure even with an `optimal' action policy as external factors prevent the robot from solving the problem or could be handled by having a better action policy (e.g. a policy generalising more efficiently or selecting suboptimal actions when the optimal ones are not available).%, or reacting correctly to human behaviours without having to interpret them on a higher level. 
    
    Appropriate actions are highly dependent on the interaction context: they could aim to match or reduce users' expectations of a robot's behaviour or complete a specific task. Additionally, actions correct in a certain context might be inappropriate in another one. Nonetheless, the intention behind being \textit{appropriate} here is that actions executed should be guaranteed not to present risks for the humans involved in the interaction (for example, preventing physical harm or mental distress), while helping the robot to move toward its goal and achieving its objectives.

    Additionally, interactions with humans `in the wild' \citep{belpaeme2012multimodal} do not happen in well defined environments or rigid laboratory setups. In the real world, robots have to interact in diverse environments, with a large number of different people, for extended periods of time or with initial incomplete or incorrect knowledge. As such, the action policy also needs to be adaptable to the context and users as well as evolve over time. In summary, robots need to be adaptive, to react to changing environments, cover a larger field of application and improve their action policy over time.

    Lastly, in many cases today, interactive robots are not autonomous but partially controlled by a human operator \citep{riek2012wizard}. We argue that to have a real and useful \gls{hri}, the robot needs to be as autonomous as possible. %As pointed out in \citet{baxter2016characterising}, by relying too much on a human to control a robot, we are shifting from a human-robot to a human-human interaction using a robot as proxy. 
    Robot are expected to be used in area where the workforce is already in shortage (e.g. healthcare) and requiring humans to control these robots decrease widely their applicability. As a community, \gls{hri} should strive toward more autonomy for robots interacting with humans.
    
    Based on these considerations, we define three axes to analyse robot controllers and evaluate how suited these controller are to interact with humans. Each axis is associated to a principle the robot has to follow to sustain meaningful social interactions:
    \begin{enumerate}
    	\item Appropriateness of actions - The robot should only execute appropriate actions.
    	\item Adaptivity - The robot should be adaptivity to its environment and in time.
    	\item Autonomy - The robot should be as autonomous as possible.
    \end{enumerate}
    We will use these axes to analyse current robotic controller types in Section \ref{sec:back_behaviour}.    
    
    As \gls{hri} is a large field, other research axes are equally important, such as the complexity or the depth of the interaction, the constraints put on the environment, the ability of the robot to set its own goals, the dependence and knowledge of social rules or the range of application of a robot to cite only a few. However, we ignored these axes in the current work as they are more influenced by the goal and the context of the specific human-robot interaction taking place than the action policy itself. Additionally, an appropriate, adaptive, and autonomous robot should be able to safely and autonomously learn to interact in deeper and more complex interactions and learn to extend its abilities beyond the ones it initially started with to increase its range of applications.

\subsubsection{Appropriateness of Actions} \label{ssec:appropriateness} %Similar to safety
    As argued previously, much of social human-robot interaction takes place in  stressful or sensitive environments, where humans have particular expectations about a robot's behaviour. Additionally, even in less critical situations, human-human interactions are subject to a large set of social norms and conventions resulting from precise expectations of the interacting partners \citep{sherif1936psychology}. And some of these expectations are also transferred to interactions with robots \citep{bartneck2004design}.
    
    We define appropriate actions, as actions taking into account the social side of the interaction, and producing a correct robot behaviour at the right time. This behaviours needs to be safe for surrounding humans and help the robot to reach its goal. Failing to produce these appropriate actions, for example by not matching the users' expectations, may have a negative impact on the interaction, potentially compromising future interactions if the human feel disrespected, confused or annoyed. Furthermore, failing to behave appropriately can even harm the people interacting with the robot: not reminding an elderly to take their medication, not taking into account the state of mind of survivors after a disaster or behaving inconsistently with children with \gls{asd} might lead to dramatic consequences. Robots require a way to ensure that all the actions they execute do not present risks to the humans involved in the interaction while moving the robot closer to its goal.
	
	%TODO: check if worth mentioning Asimov
    %Surprise is an important element in social interaction, as it can revitalise the interaction and increase the engagement of users \citep{lemaignan2014dynamics}. As such, a robot does not need to be consistently fully predictable, but in any contexts of interaction the robot should not present danger for humans it is interacting with. This requirement is similar to the first law of Asimov \citep{asimov1942runaround}, but not as a law that the robot should follow (as it would be impossible to implement it that way, and that the point of Asimov was that these laws are not sufficient), but as a design principle for the humans developing a robot behaviour or as defined by \cite{murphy2009beyond}: ``A human may not deploy a robot without the human-robot work system meeting the highest legal and professional standards of safety and ethics.''
    
    However, being able to behave appropriately is a tremendous challenge: real interactions involve a large sensory space, with the people participating often being unpredictable or at least highly stochastic. In addition, social interactions are grounded by a large number of (often implicit) norms, with expectations being highly dependent on the interaction context \citep{sherif1936psychology}. As such, it seems unlikely that every possible case of interaction and reactions from the humans interacting with the robot could be anticipated beforehand \citep{dautenhahn2004robots}. But regardless of the complexity of the interaction, social robots need an action policy able to generate the appropriate reactions for the expected states and human behaviours. This action policy also needs to be able to manage uncertainty, selecting a correct action even when facing a sensory state not explicitly anticipated by the designers.
    
    For the review in Section \ref{sec:back_behaviour}, the appropriateness of actions axis is a continuous spectrum characterising how much the system controlling the robot ensures that the robot acts in a safe and useful way for the users at any moment of the interaction. For example, a robot selecting its actions randomly has a low appropriateness as no mechanism prevents the execution of unexpected or undesired actions. On the other hand, a robot continuously selecting the action a human expert would select has a high value as domain experts know of which action is the correct one in this application domain.


\subsubsection{Adaptivity}	\label{ssec:adap}
	%TODO: Probably better, but not quite yet
    As stated before, humans are complex, indeterministic and unpredictable agents, as such an optimal robot behaviour is not likely to be known in advance and programmable by hand \citep{dautenhahn2004robots,argall2009survey}. Specifically, end users will express behaviours not anticipated by the designers, the interaction environment is often not perfectly definable and the desired behaviour might also need to be customisable by or for the end user or evolve over time. While many studies in \gls{hri} use robots following a static script, to interact meaningfully outside of lab settings or scientific studies, robots need this flexibility to extend their range of application and improve their interactions with users. In other words, robots interacting with people need to be able to adapt their action policy to the environment and improve their behaviour over time. We use the term \emph{adaptivity} to represent this ability to express a behaviour suited to different conditions and refine it over time. 
    
    We propose three components for this adaptivity. The basic one is the adaptivity to the environment, i.e. the generalisation of the behaviour (reacting accordingly to unseen inputs). The second one is personalisation and adaptation: being able to adapt a behaviour to the current user or context. Finally, the last component is the adaptivity in time which is in essence learning (the possibility to enrich and refine the action policy over time). 
       
    \paragraph{Generalisation:} Robots are interacting in human centred environments which are complex and highly stochastic. These environments are often under specified and robot designers cannot explicitly anticipate every single possible human reactions or occurring events. Furthermore, the state representations often use large vectors with multiple possibilities for each values. As such, predefining a specific robot reaction for each state possibility or each possible human behaviour is not feasible. Consequently, robots should have an action policy able to generalise to unseen and unexpected situations and react appropriately to different environments.
    
    \paragraph{Personalisation and adaptation: } As robot are interacting with humans, they will encounter different type of environments, contexts of interactions and persons with different roles. For example a robot used as an assistant for elderly people will have to interact in the home of the owner, but also follow them in the street or in a supermarket. In these different interaction contexts, distinct behaviour will be expected from the robot. Similarly, different humans being might have distinct roles and the robot needs to adapt its action policy to the type of person it is interacting with. For instance, in education, an autonomous would have to interact both with the students and the teacher, and its behaviour needs to take into account the role of the people it is interacting with. Additionally, the robot needs to personalise its behaviour to the person it interacts with: in entertainment or search and rescue, none of the users are known beforehand but providing a personalised behaviour adapted to the context may significantly impact the outcomes of the interaction. In summary, robots need to be able to adapt their actions policy to the environment and context they interact in and personalise their behaviour to the different users and their status.

	\paragraph{Learning:} When deployed in the wild, robots will be expected to interact over extensive periods of time with the same user, e.g. companion robots for the elderly, military robots for a squad or robots used in \gls{rat} \citep{leite2013social}. With these long-term social interactions, a key aspect in the engagement and efficiency is the co-adaptation between the user and the robot. Learning allows the robot to tailor its behaviour to the current user and track the changes of preferences and desires occurring over long-term interaction. Additionally, providing a robot with learning enables it to be used by non-experts in robotics, granting them a way to design their own human-robot interactions, and making use of their expertise and knowledge to have the robot interacting as they desire. This is crucial as many application of social robotics, such as \gls{rat}, happen in environment where non-technical people possess the domain expertise required to ensure that the robot is efficient. And, as stated by \cite{amershi2014power}, this reduces the requirements on expensive and time consuming rounds-trips between domain-experts and engineers and additionally decreases the risks of confusion between these different communities. Finally, this adaptivity in time allow the robot to learn from its errors and improve its action policy over time. Furthermore, this learning can enrich the robot's action policy to allow it to tackle new task beyond its initial role, increasing its application and use.
	
    In summary, for this review, adaptivity is a continuous scale ranging from no adaptivity at all: the robot has a linear script that it follows in all the interactions, to high adaptivity: the robot can generalise to unforeseen situations, dynamically changes its action policy according to the context of interaction and its partners, learns new actions and tasks and improves its action policy over time. 

\subsubsection{Autonomy}
	%TODO Reread
	Today, many experiments in \gls{hri} are conducted using a robot tele-operated by a human \citep{riek2012wizard,baxter2016characterising}, and whilst having a human controlling the robot presents many advantages (e.g. the human provides the knowledge and the adaptivity required and has sensing and reasoning capabilities not yet implemented on the robot), multiple reasons push us away from this type of interaction \citep{Thill2013}. First, relying solely on tele-operation is not suited for deploying robots in the real world. Human control does not scale to interact for long periods of time or in many places: robots are expected to interact in fields already lacking workforce (e.g. healcare), so if robots needs to be continuously controlled, the advantage of automation is highly reduced. Second, the human-robot interaction tends to become ``a human-human interaction mediated by a `mechanical puppet' '' \citep{baxter2016characterising}, which decrease the relevance of the robot as an agent and as such limits the knowledge gain for interactions with future fully autonomous robots. And finally, human control of a robot's actions introduces multiple biases in the robot's behaviour \citep{howley2014effects}, and these biases from human operators will affect the robots' behaviour, decreasing the replicability of behaviours. For these reasons, among many, we argue that a robot used in social \gls{hri} should be as autonomous as possible. A limited human supervision could support the robot and be used to improve its behaviour, but the robot should not rely on humans for its action selection during the main parts of the interaction. 
		
    To analyse the different robot controller's autonomy, we take inspiration from \citet{beer2014toward}. Beer et al. define three components of autonomy: sensory perception, analysis and action selection. And autonomy is organised following a spectrum of different levels from no autonomy at all: a human is totally controlling the robot (doing sensory perception, analysis and action selection) to a full autonomy: the robot senses and acts on its environment without relying on human inputs. Levels exist between these extremes where a human and a robot share perception, decision and/or action: for example the robot can request information from a supervisor or the supervisor can override the action or goal being executed \citep{sheridan1978human}.
    
%    Some systems use a hybrid combination of human control and autonomy. In these shared autonomy system, the boundary between the autonomous control and the human can happen on several levels: from labelling of sensory inputs \citep{depalma2016nimbus} to assigning reward to action to teach the robot an action policy \citep{thomaz2008teachable}. Similarly, this help can be triggered by the robot or the human, at specific stages of the interaction or at any time or can be a simple guidance or a command. 

\subsubsection{Interdependence of factors}

	These three axes used for the review: appropriateness of action, adaptivity and autonomy are not independent. Especially, as a robot able to learn might be also able to improve its appropriateness of action and its autonomy as it refines its action policy. This impact of adaptivity on the two other axes is fundamental to increase the robot's fields of application, performance and usability. However, while a learning robot could eventually reach an optimal, perfect, and autonomous action policy, the behaviour expressed by the robot in early stages of the learning, while the action policy is not appropriate yet, is critical. Even during this learning phase, the robot's behaviour needs to be safe and useful for humans interacting with it. As such, adaptivity is a key element for a robot controller to improve and reach a correct and autonomous policy, but a mechanism must ensure that at every step of the interaction the robot's behaviour is appropriate regardless of the level of progress of the learning.

\section{Current robot controllers in HRI} \label{sec:back_behaviour}

    The previous section presented three axes to evaluate a robot controller: the appropriateness of actions, the range of adaptivity and the level of autonomy. Based on these three axes, this section presents and analyses high level categories of robot control currently used in \gls{hri} to define a robot behaviour. 
    %As the number of individual techniques is too large for an exhaustive review, we organised the literature into broader categories. 
    For each category, we will present the corresponding approach, indicate representative works done in this direction and qualitatively rate it on the three axes.
	
\subsection{Scripted behaviour}

    One of the simplest ways to have a robot interacting with a human is probably to have explicit fixed behaviours. In this case, the robot is fully autonomous and follows a script for action selection. Success in using this approach is dependent on having a well defined and predictable environment to have the interaction running smoothly. However, if the interaction modalities (possible range of behaviours and goals) are limited enough, a constant action policy can be sufficient to handle all (sensible) human actions. 
    
    This approach is followed in a large number of research in \gls{hri}: as many studies are human-centred, the focus is not in the complexity of the robot's behaviour but on how different humans would interact with and react to a robot displaying behaviour with defined and controlled specificities. This also allows researchers to compare conditions with controlled differences and analyse the impact of small variation of behaviour. Whilst this has advantages when exploring people's reactions to robots, this method can hardly be used to deploy robots to interact with humans on a daily basis. Real world applications take place in undefined and open environments where human potential behaviours are almost infinite. Additionally, a fixed robot behaviour might also create boredom in users once the novelty effect vanishes \citep{salter2004robots}.
  
    By essence, this type of controller has a no adaptivity as the robot is following a preprogrammed script, but is fully autonomous as no external human is required to control the robot; and when the application domain is highly specified, the behaviour can be mostly appropriate.

\subsection{Adaptive preprogrammed behaviour}
	
	To go beyond a script, robot can also be programmed to react in predefined ways to expected human actions. By adaptive preprogrammed behaviour, we denote a behaviour programmed before the interaction, but explicitly (or implicitly) including ways to adjust the action policy in reaction to anticipated human behaviours. This preprogrammed adaptation takes two forms: either having a fixed number of variables impacted by the actions of the partner and guiding the action policy (for instance using homeostasis), or explicitly planning for specific behaviours to be produced if predefined conditions are met (for example using a finite state machine).
	
	%Robots have been programmed with different behaviours, selecting the one corresponding to the current interaction following a set of rules given prior the interaction. 
	
	Homeostasis, the tendency to keep multiple elements at equilibrium, is constantly used by living systems to survive and is also a good example of the first case of preprogrammed adaptation used in social \gls{hri}. For instance, \citet{breazeal1998motivational} used a set of drives (social, stimulation, security and fatigue) which are represented by a variable each and have to be kept within a predefined range to represent a `healthy' situation. If these variables reach values outside the desired homeostatic range, the robot is either over or under-stimulated, this will affect the robot's emotional status and it will display an emotion accordingly. Homeostasis approaches have also been extended to robotic pets \citep{arkin2003ethological} or \gls{rat} \citep{cao2017collaborative}.
	
	On the other hand, a case of planned adaptation is clearly presented in \citet{leyzberg2014personalizing}. Participants have to play a cognitive game,  and a robot delivers predefined advises on strategies depending on the performance and the current lack of knowledge of the participant. With these anticipated human behaviours, the robot can provide personalised support as long as the participants behave within expectations. 

	%Could talk  about other adaptations: tutoring (bielefeld) or chess or other / aibo?
	
	Similarly to other behaviour-based methods used in robotic control (such as the subsumption architecture \citealt{brooks1986robust}), due to the indirect description of behaviours, homeostasis-based methods are more robust in unconstrained environments than a purely scripted controller, while remaining totally autonomous. However the action policy is not adaptive in time and as the fixed rules of actions limit the adaptability to unexpected event. Furthermore, with this indirect description of actions, there is no guarantee against the robot acting in inconsistent way in some specific cases which limits the appropriateness of actions. Similarly, planned adaptation provides adaptivity to the environment but only in highly limited cases expected by the designers. This limit the adaptability of such an approach as the robot does not learn and may face situations not expected by the designers, also reducing the maximum appropriateness of actions.

	%Efforts have been made to extend the homeostasis approaches beyond purely reactive systems with the use of hormone models \citep{Lones2014}. This method allows the previous experiences of the robot to impact the behaviour expressed providing limited adaptivity in time to the robot. However this approach was only applied to a robot interacting in a non-social environment and the robot behaviour was biased rather than fully adaptive.
	
	Both predefined adaptation and homeostasis-based methods score highly in autonomy and can have a moderate to high level of appropriateness, but the adaptivity is low as they can only adapt to the environment within predefined, anticipated and limited boundaries and the robot does not learn.

\subsection{Wizard of Oz} \label{subsec:WoZ}

	\acrfull{woz} is a specific case of tele-operation where the robot is not autonomous but at least partially controlled by an external human operator to create the illusion of autonomy in an interaction with a user. It outsources the difficulty of action selection and/or sensory interpretation to a human operator. This technique has emerged from the \gls{hci} field \citep{kelley1983empirical} and is today common practice in \gls{hri} \citep{riek2012wizard}. Similar to scripted behaviours, \gls{woz} is mostly used in human-centred studies to explore how humans react to robot and not as a realistic way to control robots in the wild. A second use of \gls{woz} is to safely gather data to develop a robot controller from human demonstrations (cf. Section \ref{ssec:back_lfd}).
	
	Even in \gls{woz}, part of the robot's behaviour is autonomous, and combining this robot autonomy and human control can be done in multiple ways. \cite{baxter2016characterising} define two levels of \gls{woz} related to the levels of autonomy presented by \cite{beer2014toward} that correspond to the level of human involvement in the action selection process. Cognitive \gls{woz} aims to provide a robot with human-like cognition or deliberative capabilities; while in perceptual \gls{woz}, the human only replaces sensory system and feeds information to the robot controller. Typically, perceptual \gls{woz} replaces challenging features of the controller required for a study, but not relevant to the research question. One of such typical challenge is \gls{nlp}. Despite all the progress made in speech recognition, \gls{nlp} is still a challenge in \gls{hri}, especially when interacting with children \citep{kennedy2017child}. And as some studies require a limited speech recognition element to test an hypothesis, using a human for that part of the interaction allows to run study without having to solve complex technical challenges (for instance, see \citealt{cakmak2010designing}).

	This level of human control impacts the autonomy of a system: a robot relying on human only to do perception has a higher autonomy than a robot fully controlled by an operator. Similar to the different levels of autonomy presented earlier, systems can also combine human control and predefined autonomous behaviour in mixed systems. For example, \citet{shiomi2008semi} propose a semi-autonomous informative robot being mainly autonomous, but with the ability to make explicit request to a human supervisor in predefined cases where the sensory input is not clear enough to make a decision. %The human can also provide additional information on the state of the world to inform the robot of events or do natural language recognition, to inform an autonomous system about the sentences said by the users. %MIght need to add other ref
	
	With \gls{woz}, the adaptivity and the appropriateness of actions are provided almost exclusively by the human, so these characteristics are dependent of the human expertise but are generally high. However, due to the reliance on human supervision to control the robot, the autonomy is low. For semi-autonomous robots, the picture is more complex: as explained by \cite{beer2014toward}, the initiative, the human's role and the quantity of information and control shared influence the level of autonomy. For example, in \citet{shiomi2008semi} the robot explicitly makes requests to the human, but the human cannot take the initiative to step in the interaction limiting the adaptivity (especially as the robot policy is fixed). And as the human only has limited control over the robot's behaviour, no mechanism prevents the robot to make undesired decision, leading to a higher autonomous, but a lower appropriateness of actions and adaptivity compared to classical \gls{woz}.

\subsection{Learning from Demonstration} \label{ssec:back_lfd}
%mention that its applicable to end users and allow designers to have to implement the behaviour and all the specific cases
	%motivation for lfd
	%behaviour too complex to be coded or requirement of end user knowledge
	%MIght require ref
	As stated by numerous researchers, explicitly defining a behaviour and manually implementing it on a robot can take a prohibitive amount of time or could not be possible at all \citep{argall2009survey,billard2008robot,dautenhahn2004robots}. This statement applies equally well to manipulation tasks and social interaction. In both cases, humans have some knowledge or expertise that should be transferred to the robot. However in social robotics, experts of the field often do not have the technical knowledge to implement efficient behaviours on a robot, which results in numerous design iterations between the users and engineers to reach a consensus. 
	
	%As when interacting it might be complicated and time consuming to acquire data points for learning, offline learning methods are mainly inspired from the Learning from Demonstration framework (LfD) \citep{argall2009survey}. With LfD, the idea is to take inspiration from human demonstrations to accelerate the learning for an agent or to teach tasks that could not be preprogrammed manually \citep{billard2013robot}. The classical approach starts with observing a human completing the task and then deriving a corresponding robot behaviour to match the human one. In most of the cases, the learning only occurs once: data is accumulated first and then batch learning is applied to derive a static action policy. 
	    
	The field of \gls{lfd} aims to tackle these two challenges: implementing behaviours too complex to be specified in term of code and empowering end-users with limited technical knowledge to transfer an action policy to a robot. The learning process starts with a human demonstrations a correct behaviour \citep{argall2009survey}, and then offline batch learning is applied to obtain an action policy for the robot. Later, if required, reinforcement learning can complement the demonstrations to reach a successful action policy \citep{billard2008robot}.
	%importance of hri - but in teaching, often not object
	In \gls{lfd}, the human-robot interaction is key, however in most of the cases this interaction is only in the learning process and the object of the learning is not social interaction with humans, but manipulation or locomotion tasks such as grasping and moving an object, using a racket to hit a ball or throwing tasks \cite{billard2008robot}.
	
	%examples with hri as target (liu, sequeria and clark) - but examples limited	
	% Except Liu (learning from data) and restricted WoZ
	However, two approaches have applied \gls{lfd} to teach robots a social policy to interact with humans.	The first one aims at learning directly from human-human interactions and analyse the human behaviours to replicate them on a robot. For example, \citet{liu2014train} present a data driven approach taking demonstrations from human-human interactions to gather relevant features defining human social behaviours. Liu et al. recorded motion and speech from about 180 interactions in a simulated shopping scenario and then clustered these behaviours into high-level actions. During the interaction, the robot uses a variable-length Markov model predictor to estimate probability of the human demonstrator to execute each actions and finally winner-take-all is applied to select the most probable action. According to the authors, the final performance of the robot was not perfect, but if this approach was scaled using a larger dataset gathered from more human-human interactions in the real world, the performance should improve and become closer to natural human behaviours.
    
    In the second approach, the data is collected through a \gls{woz} setup and aims to learn to replicate the wizard action policy to reach an autonomous social behaviour. \cite{knox2014learning} coined this approach ``\gls{lfw}''. The method starts with a purely \gls{woz} control study to gather data, and then, an action policy is derived by applying machine learning on the collected data. However, this original paper presents no description of which algorithm could be used or how, and gives no evaluation of the approach, instead it only offers a reflection on the application of this idea. An implementation and evaluation is briefly discussed by the authors in \cite{knox2016learning}, but the lack of implementation details and results reduces the usability of the paper.
    
    \gls{lfw} is widely used in \gls{hci} (especially dialogue management; \citealt{rieser2008learning}) and has also been implemented by other groups of researchers in robotics. For example, \citet{sequeira2016discovering} extended and tested this idea to a create a fully autonomous robot tutor. This method is composed of a series of steps: 
    \begin{enumerate}
    	\item Collect observations of a human teacher performing the task.
    	\item Define the different actions used by the teacher and the features used for the action selection.
    	\item Implement these actions and features on a robotic system.
    	\item Set up a restricted-perception \gls{woz} experiment where an operator uses only the identified features to select actions for the robot.
    	\item Combine machine learning applied on the data and hand-coded rules to create an autonomous robot controller.
    	\item Deploy the autonomous robot.
    	\item[(7.] If required, add offline refinement steps to fine tune the robot's behaviour.)
    \end{enumerate}

    Both \citet{knox2014learning} and \citet{sequeira2016discovering} stress the importance of using similar features for the Wizard of Oz control to the ones available to the robot during the autonomous part. Although this decreases the performance in the first interaction, it allows more accurate learning overall due to the similarity of inputs for the robot and the human controlling it.
        
    \cite{clark2018deep} aimed to bypass these limitations by using a deep Q-network \citep{mnih2015human} to learn an Applied Behaviour Analysis policy for \gls{rat}. They recorded videos, microphone inputs and actions selected in a \gls{woz} interaction with neurotypical participants to train the network with the raw inputs and the actions selected to obtain a controller able to deliver the therapeutic intervention. However, in their study, the autonomous robot required limited human input to inform the algorithm of the state of the therapy and only reached a behavioural intervention with an accuracy inferior to 70\%. This means that even with additional human input the robot would provide inconsistent feedback at some points in the interaction. However, this study only used a limited amount of data, and using more training examples should lead to better results.
     
    %TODO: see if required, and if yes how to make it better   
    \gls{lfd} methods are based on real interactions either between humans, or between humans and robots controlled by humans; and, with enough demonstrations the robot should be able to select appropriate actions. However, efficiency is limited by the type of inputs recorded, the capabilities of the learning algorithm with the inputs space and the quality of the demonstrations which limit the appropriateness of the action policy (as seen in \citealt{clark2018deep}). Furthermore, after the learning phase, the robot's behaviour is  mostly static, without any additional learning provided. As such, while possessing a good generalisation capability, \gls{lfd} do not possess the adaptivity in time once the robot is deployed. \cite{sequeira2016discovering} propose to add offline learning steps could be added, but online learning would allow for smoother transitions and improvement of the behaviours.
    %as no intrinsic mechanism is present to prevent the execution of undesired actions which could happen if the robot ends up in an unseen state, the appropriateness of actions cannot be maximal. Additionally, the adaptivity in time is limited, as for most of the techniques the learning happens only once and then the behaviour is fixed. However, the framework proposed by restricted-perception Wizard of Oz should allow asynchronous adaptivity in time using the refinement phase. 
    Finally, all these methods require the presence of humans in a first phase but the robots are fully autonomous later in the interaction, so the autonomy is low in the first phase and then total during the main part of the interaction.
    
\subsection{Planning} \label{ssec:planning}
    
    An alternative way to interact in complex environments is to use planning \citep{asada1986robot}. The robot has access to a set of actions with preconditions and postconditions and a defined goal. To achieve this goal state, it follows the three planning steps: sense, plan and act. The first step, \emph{sense}, consist on acquiring information about the current state of the environment. Then, based on the set of actions available and the goal, a \emph{plan} is created. This plan is a trajectory in the world, a succession of action and states which, according to the defined pre and postconditions, will lead to the goal. Finally, the last step is to \emph{act}, to execute the plan. The plan can be reevaluated at each time step or only if an encountered state differs from the expected one, in that case the robot updates its plan according to the new state of the environment and continues trying.
    
    %This approach is often used in motion planning, but can integrate some %social aspect as presented in the work of Dragan and colleagues %\citep{dragan2013legibility}, where motion planning is adapted to be more %legible by human. Planning can also be used at a higher level for action %selection.
    
    The efficiency of planning relies on having a precise and accurate set of pre and postconditions for each action. And as humans are complex and unpredictable, it is a serious challenge, if not impossible, to model them precisely. As such, planning have seen limited use for open social interactions with humans. However, due to the nature of planning, reaching a specific goal in a known environment, it has been applied successfully to \gls{hrc}. Additionally, limiting the interaction to a joint task also simplifies the modelling of the human: the interaction is more constrained and the human behaviour should limit to a number of expected task-related actions. The Human Aware Task Planner \citep{alili2009task} is an example of planning used to assign task between a robot and human in a \gls{hrc} scenario. One specificity of this planner is the ability to take into account predefined social rules (such as reducing human idle time) when creating a plan to allocate tasks to the human-robot team. Including these social norms in the plan construction is expected to improve the user experience and ensures maximised human compliance to the plan.
    
    Planning performance depends heavily on the model of the environment the robot has access to. A precise and correct model ensures that the robot will autonomously select the appropriate action whilst an incorrect one would lead to non appropriate behaviours. Similarly, the adaptivity depends on the model the robot has access to and whether it can update it in real time. However, in many cases when interacting with humans, the model is static, only covering the tasks the robot has to complete the different contexts and states it is expected to face and as such presents limited generalisation capabilities to unanticipated situations or or non task-related human actions.
    
    %As long as the model is correct enough, it is ensured to keep the %``astonishment'' low and to maintain a high level of autonomy. The %adaptivity is also highly dependent on the model the robot has access to. %If the planning domain is large enough, the adaptivity can be high, %multiple users can be also predefined to increase the user adaptability, %and if the domain knowledge can be updated dynamically.
    
    %\marginpar{The previous paragraph is a bit unclear to me, but you can leave %it in the RDC2. If you use it anywhere else, it would need to be %rewritten.}
    
    Planning has also been extended with learning, which then allows for more adaptive and personalised action policies. This type of learning planner has been mostly used in manipulation and navigation to obtain better trajectories \citep{jain2013learning,beetz2004rpllearn}. In \gls{hri}, \cite{munzer2017efficient} presented a planner adapting its decisions to human preferences in a \gls{hrc} scenario. With this approach, the robot estimates the risk of each actions and depending of the risk value will execute them, propose them (and waits for approval before executing an action), or wait for a human decision. Between repetitions of the task, the robot will update its planner to fit more precisely to the human preferences and improve its action policy for the next iteration of the task. Munzer et al. adopted principles from \gls{lfd} to planning to improve quickly and efficiently the performance of the robot. However, while planning is well suited for strictly defined and mostly deterministic tasks, many social human-robot interactions cannot be totally specified symbolically and with clear actions and outcomes and as such the application of planning to social \gls{hri} is limited.
    % and action selection planning \citep{kirsch2009robot}. 
	
	
\subsection{Summary}

	Table \ref{tab:back_controller} presents a summary of the different approaches currently used in social \gls{hri} with their advantages and drawbacks for application in \gls{hri} and the evaluation on each of the three axes. The two most promising types of control are \gls{lfd} and planning, however, both of them have their drawbacks: \gls{lfd} is applied offline to create a monolithic controller with limited adaptivity after being deployed, and planning's reliance on a model of the world limits its application in open-ended social \gls{hri} in the wild.
	
\afterpage{%
	\clearpage% Flush earlier floats (otherwise order might not be correct)
	\thispagestyle{empty}% empty page style (?)
	\begin{landscape}% Landscape page
		\centering
		\bgroup
		\ra{1.2}
		\begin{tabular}{@{}>{\raggedright}m{.115\linewidth}>{\raggedright}>{\raggedright}m{.2\linewidth}>{\raggedright}m{.2\linewidth}>{\raggedright}m{.15\linewidth}>{\raggedright}m{.11\linewidth}>{\raggedright}m{.07\linewidth}>{\arraybackslash}m{.07\linewidth}@{}} \toprule
			Controller & Advantage & Drawbacks & Application in \gls{hri} & Appropriateness & Adaptivity & Autonomy \\ \midrule
			Fixed \linebreak preprogrammed\linebreak behaviour & Quick and easy to create \linebreak Clear specified and repeatable behaviour & Limited to highly constrained interactions & Human-centred studies in highly\linebreak constrained env. & Low & Null  & Maximal \\[.5cm]
			Adaptive \linebreak preprogrammed\linebreak behaviour & Relatively simple to program \linebreak More robust and efficient than scripted behaviour & Only provide adaptability in\linebreak limited anticipated context & Human-centred\linebreak studies in constrained environments & Medium & Low & Maximal \\ 
			Wizard of Oz & Use human expertise to select the best action & Require constant high\linebreak workload from human & Human-centred\linebreak studies \linebreak Highly critical \gls{hri} & Maximal  & Maximal & Null/Low     \\ 
			Learning from Demonstration & Transfer knowledge from\linebreak human to robot in the real\linebreak application environment & Lack of learning once deployed & HRI case by case & High & Medium     & High     \\
			Planning & Complex behaviours and adaptable to variations in the environment & Human are too complex to have clear set of conditions \linebreak Limited application to social interaction & Complex defined environments \linebreak \acrshort{hrc} & Medium & High       & Maximal \\
			\bottomrule
		\end{tabular}
		\egroup
		\vspace{-.61\linewidth}
		\captionof{table}{Comparison of robot controllers in \gls{hri}}
		\label{tab:back_controller}
	\end{landscape}
	\clearpage% Flush page
}
	
	An ideal controller would learn how to interact by interacting, using demonstrations from an expert to obtain an initial reasonable action policy, but still improving itself after being deployed using reactions from the environment or feedback from a teacher. This type of interaction is similar to \gls{iml}: learning from the interaction and using a human teacher to speed up the learning. Researchers have explored how to learn interactively non-social action policy from interactions with humans \citep{scheutz2017spoken,cakmak2010designing}, but as of April 2018, no controller exists in \gls{hri} applying \gls{iml} to the challenge of learning social interaction with humans. 

	A supervised learning from interaction would be the approach with the most potential as this type of learning can validate the three requirements: appropriateness of actions, adaptivity and autonomy. By essence, this continuous online learning aims at providing open-ended adaptivity to the robot. Including a human with control over the robot's actions can ensure that actions are appropriate. And finally, as the robot learns, accumulates datapoints and demonstrations from the teacher it improves its action policy, reducing the reliance and workload on the human to reach high levels of autonomy while conserving the constant appropriateness of actions and the adaptivity.

\section{Interactive Machine Learning} \label{sec:back_iml}
%Mostly for supervised learning 

As seen with example in \gls{lfd}, \acrfull{ml} is a promising method to provide a robot with an adequate action policy without having to implement in advance all the decisions rules used to select an action. \gls{ml} has two main trends referring to the synchronisation between the learning and the use of algorithm: offline and online learning.

In robotics, offline learning is a technique allowing the robot to change its action policy over time by updating it outside of the interaction (such as Learning from the Wizard in Section \ref{ssec:back_lfd}). Between or before the interactions, a learning algorithm is used on a dataset previously accumulated to create a new action policy.

On the other hand, online learning (such as \gls{rl}; \citealt{sutton1998reinforcement}) learns during the interaction. Rather than single monolithic definitions or updates of the behaviour, this constant refinement of the agent behaviour benefits from a high number of updates, allowing the robot to learn even during the first interaction and never stop improving its behaviour.

\acrfull{iml}, as coined by \cite{fails2003interactive}, is a type of online learning with two specificities:
\begin{itemize}
	\item Use of an end-user expert in the learning process.
	\item Learn by multitude of consecutive small updates of behaviour.
\end{itemize}
These two characteristics differ greatly from classical offline learning, such as deep learning \citep{lecun2015deep} which uses costly monolithic learning steps without human influence to define a static behaviour. On the other hand, \gls{iml} is an iterative process where the behaviour is improved at each small steps, and where the end-user can provide feedback on the learner's performance during all these iterations. \gls{iml} aims to learn faster, by continuously using the human expert to correct the errors made by the algorithm as they appear, provide additional useful information to the learner and improve the knowledge gained at each learning step.

\cite{amershi2014power} presents an introduction to \gls{iml} by reviewing the work done and presenting classical approaches and challenges faced when using humans to support machine learning.

%Need to careful not to just rewrite power to people + need to add new material 
%Mention Learning from Demonstration!
\subsection{Goal}

The main goal behind \gls{iml} is to leverage the human knowledge during the learning process to speed it up, to extend the use of classifiers from static algorithms trained only once to evolving agents learning from humans and refining their policies over time. As explained in \cite{fails2003interactive}, classifiers gain to use human knowledge to iterate quickly to reach a good solution and agents learning from the interaction would gain from using additional feedback from humans \citep{thomaz2008teachable,knox2009interactively}. \gls{iml} aims to combine the advantages from both \gls{sl} and online learning and applies this new type of learner to classification or interaction tasks.

Furthermore, by allowing a human user to see the output of an algorithms and provide additional inputs, the learning has the potential to be faster and tailored to this human's desires. By using human expert knowledge and intuition, the system can achieve a better performance faster (\citep{thomaz2008teachable}). Additionally, a key advantage of \gls{iml} is also being able to empower end-users of robotic or learning systems. These users are often non-technical, but possess valuable knowledge about what the robot should do. \gls{iml} provides an opportunity to allow these users to design the behaviour of their robot, to teach it to behave the way they desire.

These human inputs take three forms: labels for specific datapoints (cf. active learning - Section \ref{ssec:back_active}), feedback over actions (similarly to reward in \gls{rl}; cf. Section \ref{ssec:back_rl}) or demonstrations to reproduce (cf. \gls{lfd} - \ref{ssec:back_lfd}).

\subsection{Active learning} \label{ssec:back_active}

Active learning is a form of teaching used in education aiming to increase student achievement by giving them a more active role in the learning process \citep{johnson1991active}. This approach has been transferred to machine learning, especially to classifiers, by allowing the learner to ask questions and query labels from an oracle for specific datapoints with high uncertainty \citep{settles2012active}. The typical application case is when unlabelled data are plentiful, but labels can be limited in numbers or costly to obtain. As such, a trade-off arises between the performance of the classifier and the quantity of queries made by the algorithm. Often, the oracle would be a human annotator with the ability to provide a correct label to any datapoint, but their use should be minimised for reasons of cost, time or annoyance.

%different challenges for classic active learning and robot teaching (inter)active learning
Using an oracle to provide the label of specific points with high uncertainty should highlight missing features in the current classifier resulting in improvements both in term of accuracy and learning speed. However, this specific relation between the learner and the human teacher raises new questions such as: 
\begin{itemize}
	\item Which points should be selected for the query?
	\item How often the human should be queried?
	\item Who controls the interaction? (i.e. who has the initiative to trigger a query?)
\end{itemize}

Researchers have explored optimal strategies for dealing with this relation between the learner and the oracle. This research has been especially active in \gls{hri} with robots directly asking questions to human participants and exploring how the robot's queries could inform the teacher about the knowledge of the learner \citep{chao2010transparent}. In a follow up study, \cite{cakmak2010designing} showed that most users preferred the robot to be proactive and involved in the learning process. On the other hand, they also wanted to be in control of the interaction, deciding when the robot could ask questions even if it imposed a higher workload on the teacher. However, authors proposed that when teaching a complex task requiring a high workload on the teacher, the robot would probably be expected or should be encouraged to take a more proactive stance requesting samples to take over some workload from the teacher.
%User are human and want to be considered as such, not oracle (want more control) not willing to be simple oracle - human preferred being in control of the rate and timing of question, rather than being simply a labeller.

Active learning, being able to select a specific sample for labelling, can dramatically improve the performance of the learning algorithm \citep{settles2012active}. However, when interacting in the world, the learner is not in control of which sample can be submitted to an oracle to obtain a label. Datapoints are provided by the interaction and are influenced by the learner's actions and the environment reaction. For agents learning during the interaction, the active learning approach working for classifiers is not applicable, so other methods have been applied such as \gls{rl}, learning from human feedback or \gls{lfd}.
%limit the risks of the exploration and increase the learning speed - demonstrations are not fully consistent so the robot might find states where there is no obvious action to select. 

%LImited application to learn hri as the robot cannot select its samples itself
\subsection{Reinforcement Learning} \label{ssec:back_rl}

The main framework of learning applied to learning from interaction is \acrfull{rl}. \gls{rl} aims to solve the problem of finding the best action policy by observing the environment reaction to the agent's action.

\subsubsection{Concept} 
	Young infants and adults learn by interacting with their environment, by producing actions and receiving a direct sensory motor feedback from their environment. By learning the impact of their actions, humans can learn how to achieve their goals. Similarly, the field of \gls{rl} aims to empower agents by making them learn by interacting, using results from trials and errors and potentially delayed rewards to reach an optimal, or at least efficient, action policy \citep{sutton1998reinforcement}. 

	%Discrete time - life as a sequence!
	\gls{rl} agents interact in a discretised version of the time, considering life as a sequence of states and actions. The simplest version of \gls{rl} is modelled as a finite \gls{mdp}, a discrete environment defined by the five ensembles $(S, A, P_a(s,s'), R_a(s,s'), \gamma)$ \citep{howard1960dynamic}, with:
	\begin{itemize}
		\item $S$: a finite set of states defining the agent and environment states.
		\item $A$: a finite set of actions available to the agent.
		\item $P_a(s,s')$: the probability of transition from state $s$ to $s'$ following action $a$.
		\item $R_a(s,s')$: the immediate reward following transition $s$ to $s'$ due to action $a$.
		\item $\gamma$: a discount factor applied to future rewards.
	\end{itemize}
	
	The goal of the \gls{rl} agent is to find the optimal policy $\pi_*$ (assigning an action from $A$ to each state in $S$) maximising the discounted sum of future rewards. The agent is not aware of all the parameters of the model, but only observes the transitions between states and the rewards provided by the environment and has to update its policy to maximise this cumulated reward. Different algorithms exist to reach this policy, but the main features present in all of them are the concepts of exploration and exploitation \citep{sutton1998reinforcement}.
	
	\emph{Exploration} reflects the idea of trying out new actions to learn more on the environment and potentially gain knowledge improving the policy; whilst \emph{exploitation} is the execution of the current best policy to maximise the current gain of rewards. All the algorithms have to balance these two features to reach an optimal action policy. One way to deal with this trade-off is to start with high probably of exploration, to rapidly collect knowledge on the environment and then decrease this probably to converge toward an efficient policy, using this knowledge to make better choice of actions.
	
	The more complex the environment is, the longer the agent has to explore before converging to a good action policy. It is not uncommon to reach numbers such as millions of iterations before reaching an appropriate action policy \citep{sutton1998reinforcement}. And during this exploration phase, the agent's behaviour might seem erratic as the agent tries actions often randomly to observe how the environment is reacting.
	
	%Challenges - tradeoff exploration/itation - uniqueness of data (fleeing nature of time and data) - non statioanrity - sequential delayed consequences ...
	%success: backgammon, helicopter, advertisements, jeopardy, atari -> better perf than any other methods - and "without using human instructions"
	
	%concept of action value function of a policy: value of doing action in state
	%optimal value function = value function of the optimal policy (include delayed consequences)
	%q-learning works for any type of MDP
	%off policy: learning without doing the target policy + behaviour policy
	%policy iteration: evaluate policy - obtain q - greedify to new policy - evalute - obtain new q..
	%in tabular, converge in few iterations, robust
	%bootstrapping: updating an estimate from an estimate: bellman equation
	%Need approximation for generalisation and learning faster - approximate the action value function (linear weighting of features)
	%the function approx subsume much of the problem of hidden state
	%it works often with q-learning, but loses the guaranty of converging! -> still need theoretical work
	%update parameter vector at each time step according to error
	%better with sarsa on policy algo -> often epsilon greedy
	\subsubsection{Application to HRI}
	
	This approach presents many features relevant to \gls{hri}: it possesses the autonomy required for meaningful interactions with humans and provides the adaptivity desired for having a large impact. However, as explained in the previous section, traditional \gls{rl} has two main issues: requirement of exploration to gather knowledge about the environment and large number of iterations before reaching an efficient action policy. Generally, \gls{rl} copes with these issues by having the agent interacting in a simulated world. This allows the agent to explore safely in an environment where its actions have limited impact on the real world (only time and energy) and where the speed of the interaction can be highly increased to gather the required datapoints in a reasonable amount of time. However, no simulator of human beings exists today which would be accurate enough to learn an action policy applicable in the real world. Learning to interact with humans by interacting with them would have to take place in the physical world, with real humans, and this implies that these issues of time and random behaviours would have direct impact. 
	
	To gather informations about the environment, the agent needs to explore, trying out random actions to learn how the humans respond to them and if the agent should repeat them later. When interacting with humans, executing random actions can have dramatic effect on the users, presenting risk of physical harm as robots are often stiff and strong or cause distress. This reliance on random exploration presents a clear violation of the first principle to interact with humans presented earlier (`Only execute appropriate actions').
	
	Even if random behaviours were acceptable, humans are complex creatures, behaving stochastically, with personal preferences and desires. And as such, learning to interact with them from scratch would require large number of datapoints and as interactions with humans are slow (not many actions are executed per minute) the time required to reach an acceptable policy would be prohibitive. 
	
	Despite this real-world constraints, \gls{rl} has been used in robotics \citep{kober2013reinforcement}, but mostly applied to manipulation, locomotion or navigation tasks. For the reasons stated above, as of early 2018, \gls{rl} has never been used to autonomously learn social behaviours for \gls{hri}. 
	
	\subsubsection{Opportunities}  
	Despite the limitations presented in the previous section, changes can be made to \gls{rl} to increase its applicability to \gls{hri}. Combining \gls{rl} and \gls{iml} ensures that the behaviour is appropriate to interactions with humans even in the learning phase.
	
	\cite{garcia2015comprehensive} insist on \textit{safe} \gls{rl}, ways to ensure that even in the early stages of the interaction, when the agent is still learning about the world, its action policy still achieves a minimal acceptable performance. The authors present two ways to achieve this safety: either use a mechanism to prevent the execution of non-safe actions or provide the agent with enough initial knowledge to ensure that it is staying in a safe interaction zone. These two methods are not limited to \gls{rl} but are also applicable to other machine learning techniques to make them safer (for instance by using \gls{lfd} \citealt{billard2008robot}). 
	
	The first method (preventing the agent to execute undesired actions) can be implemented by explicitly preventing the agent to execute specific actions in predefined states or by having a list of safe actions \citep{alshiekh2017safe}. Using this method, the anticipated cases of errors can be prevented. However it seems unlikely that every case could be specified in advance. As such, an efficient way to prevent the execution of undesired actions could be to include a human expert in the action selection loop in the early learning phase, and giving them the capacity to preempt undesired actions before their execution. 
	
	The second method (providing enough initial knowledge) can be achieved by carefully engineering the features used by the algorithm or starting from a initial action policy to build upon. For example, \cite{abbeel2004apprenticeship} proposed to use humans demonstrations in a fashion similar to \gls{lfd} but to learn a reward function and an initial working action policy. This method, Inverse Reinforcement Learning, has been applied successfully to teach a flying behaviour to a robotic helicopter. Once the initial policy and a reward function are learned, \gls{rl} is applied around the provided policy to explore and optimise the policy. That way, only small variation of the policy will happen around the demonstrated one, and these small variations ensure that policies leading to incorrect behaviours are negatively reinforced and avoided before creating issues (such as crashing in the case of the robotic helicopter). 
	
	%Similarly to autonomous online learning, these approaches score high in adaptivity in time and depending of the learning mechanism, sensory inputs and actions, they also do well in being adaptive in the search space and to changing user behaviour. The reliance on human input decreases the autonomy, but increases the appropriateness of actions as this guidance can help to reduce the use of random exploration. However, as the human has often only a partial control, the robot is not ensured to act correctly at every stage of the interaction preventing the appropriateness to be maximal.
	
	Whilst being promising and having been applied for agents in human environments (such as for personalised advertisement - \citealt{theocharous2015personalized})	these approaches have not been used to learn social behaviours or to have robot interacting with humans.

\subsection{Human as a source of feedback on actions}

When an agent is learning in a \gls{rl} fashion and improves its behaviour by receiving rewards from the environment, an intuitive way to steer the agent's behaviour in the desired direction faster is to use human rewards. This approach is an adaptation of `shaping': tuning a animal's behaviour by providing rewards \citep{bouton2007learning}. In \gls{ml}, using rewards from a human to bias and improve the learning presents advantages: the interface is simple and generalisable to any type of problem, and the teacher only needs a way to provide a scalar or a binary evaluation of an action to steer the learning. However, this simplicity of interaction is associated with a limited efficiency and a complexity of interpretation: the issues of how to interpret human rewards and how to combine them with environmental ones if existent are an active research field today \citep{knox2010combining}.

When used on their own, human rewards enable an agent to learn an action policy even in the absence of any environmental rewards, which is specially interesting robotics as a clear reward function applicable to \gls{hri} or robotics in general can be complex to define. Early work in that field came from \cite{isbell2006cobot} who designed an agent to interact with a community in the LambdaMOO text based environment. Cobot, the agent, had a statistical graph of users and their relations and executed actions in the environment. Users of LambdaMOO could either reinforce positively or negatively Cobot's action by providing rewards. While the interaction between the agent and the users was limited, Isbell et al. presented the first agent to learn social interactions with humans in an online complex and social environment. 

While the goal of Cobot was to create an entity interacting with humans, \cite{knox2009interactively} explored how humans could actively teach an agent an action policy in the absence of environmental rewards using TAMER (Training an Agent Manually via Evaluative Reinforcement). With this approach, the agent uses a supervised learner to model the human reward function and then takes the action that would receive the highest reward from the model. 

However, unlike environmental rewards, human rewards are subjective evaluations of an agent's behaviour. As such by knowing humans tendencies and intentions when providing rewards, an agent is able to obtain more information from these human rewards than by treating them the same way as environmental ones. Many researchers explored how to obtain more information from human reward. For instance, Advice \citep{griffith2013policy} models how trustworthy the teacher is and as such how much importance the learner should give to their rewards. For example, rewards from inconsistent teachers will be reduced as the agent should only provide limited attention to them. Alternatively, \cite{loftin2016learning} explored how to infer the strategy used by the teacher in the reward delivery. Similar behaviours from different teachers might have different meaning: not rewarding an action might reflect an implicit acknowledgement of the correctness of an action or the active refusal to provide a positive reward (indicating the incorrectness of an action). By modelling this intention, the real meaning of rewards can be inferred and used to further improve the learning. A last relevant feature explored by this community is the dependence in time of the human reward policy. While reward functions are generally constant in time with \gls{rl}, with humans they might vary according to the current performance of the agent. For example, a suboptimal policy could receive positive feedback early on, when it compares positively to the average behaviour; while receiving negative feedback later on, when the average agent's performance is better. \cite{macglashan2017interactive} proposed COACH (Convergent Actor-Critic by Humans) to model how humans adapt their rewarding scheme in function of the agent's performance and deal with this non-stationary reward function. Similarly to other factors biasing human rewarding strategies, this dependence of the reward function to the current agent's policy should be taken into account to maximise the knowledge gained from human rewards.

Even when environmental rewards are present, human rewards still have opportunities to improve the learning: they can enrich a sparse reward function, guide the robot faster to an optimal policy or correct incomplete or incorrect environmental rewards. \cite{knox2010combining} explore the impact of nine different ways to combine these two types of rewards and the impacts on the learning of each methods. And, from these analysis, they explain how to select an approach according to the specificities of the environment and the reward function.

%Could read more about this
%Other approaches combine traditional \gls{rl} with critique from humans. A human is requested to evaluate specific behaviours \citep{judah2010reinforcement} or compare to policies to select the most appropriate one \citep{christiano2017deep}. 

%Human don't want to provide only label, they want to explain \cite{stumpf2007toward} + importance on transparency: help to achieve better results and improve user experience

Teachers can also use rewards to communicate other information to the learner. For example, \cite{thomaz2008teachable} aimed to explore how humans would use feedback to teach a robot how to solve a task in a virtual environment. They used \acrfull{irl} as a way to directly combine environmental rewards and human ones. However, during early studies, Thomaz and Breazeal discovered that participants tried to use rewards to convey intentions, informing the robot which part of the environment it should interact with. The next study involved two communication channels, a reward one to provide feedback on the actions and a guidance channel to provide information about the action the robot should execute. This guidance has been actively decided to be ambiguous; participants could not explicitly control the robot, but just bias the exploration, and adding this second channel improved the performance of participants. This study presented a first attempt to combine environmental rewards, human ones and human guidance to teach an agent an action policy and demonstrated the importance of giving additional ways for the teacher to impact the robot's behaviour.

%Using humans to provide rewards to a learner can largely improve the learning by making it faster and safer. However researches show that this evaluation of behaviours  is not enough, participants desire to have more control over the robot's actions and this can improve the learning further.

%present the , a method combining feedback from the environment and feedback from a human supervisor to learn a task in a non-social context. Similarly, \citet{knox2009interactively} propose to use RL in an environment where the feedback is not given by the environment itself, but by a human only. In these two cases, human feedback is used to scaffold the learning: it makes it faster and safer and can allow the use of RL in environments without explicit reward. However as the feedback is always given after the execution of the action, there is no guaranty that only expected actions will be executed. This might be a reason why these two approaches have not been used to learn an action policy for social human-robot interactions. But they are nevertheless relevant as they are using the interaction with a human to help an agent to learn an action policy faster and safer than pure RL.

While not being applied to robotics but mostly to learning non-social interactions, these implementations of \gls{iml} provide important research describing how robots could be taught to interact with humans. These human rewards are especially interesting when the environmental reward function is sparsely defined or non-existent, providing a way to teach robots in any environments. However, humans do not simply evaluate an agent's actions, they adopt teaching strategies influencing their way of rewarding and want to provide guidance, hints or commands to help the agent to learn better and faster. In summary, human teachers desire to go beyond simply evaluating what the robot is doing by providing advices or commands about how it should behave.

%conclusion of subpart??
%feedback not enough
%When given choice, human will never teach only using rewards  %See if %More recent stuff with deep comparison and things - still requires simulation and so on
%citation makes sense

\subsection{Interactive Learning from Demonstration} \label{ssec:back_ilfd}

As presented in \cite{argall2009survey} and \cite{billard2008robot}, \gls{lfd} is majoritarily used in an offline learning fashion to learn a defined task without extending the action policy once the task is considered mastered. However, tasks such as social interaction are complex even for humans and probably will never be fully mastered for robots; as such (and as argued before), robots would highly profit from learning throughout all their life, not only once before being deployed, but learning new tasks and improving their skills as often as required \citep{dautenhahn2004robots}.

With \gls{ilfd}, an agent receives demonstrations not only once, but as often as required after being deployed. \gls{ilfd} is related to Mixed Initiative Control \citep{adams2004mixed} where an agent and a human share control on the agent's actions. The robot acts mostly autonomously, but in some cases (at the initiative of the human or the robot), the human takes over the robot control and make a demonstration that will be used by the robot to refine its action policy for the future.

One approach giving teachers a total initiative on the interaction is Dogged Learning (DL) \citep{grollman2007dogged}. With DL, an agent is autonomously interacting and a teacher has the power to override the agent behaviour at any time by selecting desired actions or outputs. Facing a potential difference between the algorithm's outputs and the teacher's ones, the robot executes the commands with highest confidence (often the human's one) and the learning component aims at reproducing the executed output. If the teacher does not provide any commands, the ones from the algorithm are used. DL does not provide the robot with the opportunity to request a demonstration, but instead, the robot can communicate its uncertainty to the teacher, indirectly asking for demonstrations. 

\cite{chernova2009interactive} propose a method with a more complex interaction between the learner and the teacher. The Confidence Based Algorithm (CBA) is composed of two components: the Confident Execution (CE) and the Corrective Demonstration (CD). The CE enables the agent to act autonomously when its confidence in its action policy is high and on the other hand to actively request a demonstration when the confidence is low. The CD allows the teacher to provide a corrective demonstration when the agent executes an incorrect action, which provide more information to the agent than a classic negative reward. These two components aim to leverage the complementary capabilities of the learner and the teacher. CBA has demonstrated efficient teaching in diverse scenario such as simple driving simulators or other classification tasks. But the effectiveness of this approach is bounded by the capacity of the learner to estimate this confidence to be able to request demonstrations and prevent incorrect behaviour to be executed. Another limit of such an approach is the impossibility for the teacher to correct undesired actions before they negatively impact world. 

Both methods rely on the teacher being able to anticipate the robot's behaviour to provide demonstrations before an incorrect action is executed or before it impacts the agent and its environment. As such, the appropriateness of the robot controller is not at maximum as the teacher cannot ensure that no incorrect action will be executed during the learning, only that the robot would learn faster from its errors.

\subsection{Importance of control}

Results from active learning, research using human to provide feedback and \gls{lfd} have shown that human teachers take an active stance during the training of an agent and want multiple ways to influence the learner's behaviour \citep{amershi2014power}. Humans are not oracles, enjoying providing labels and evaluating an agent's actions, they desire to be in control of the learning and provide richer information to the agent. \cite{kaochar2011towards} have shown than when given choice between different teaching methods, humans will never choose to limit themselves to use only feedback, but they want to teach using more modalities.

In addition to improving the teacher's experience in the teaching process, providing the humans with more control improves the learning \citep{thomaz2008teachable,chernova2009interactive}. By allowing the teacher to demonstrate an action policy online, bias the action selection and preempt or correct undesired actions, the learner interacts mostly in useful states of the environment and with a correct action policy. This lead to faster learning and would improving the robot's performance highly in early stages of the learning. Another fundamental feature added by this human control over the robot's actions is safety. If a domain expert can prevent a robot interacting with humans to make errors and can ensure that all its actions are efficient, the quality of the interaction for the humans involved is greatly increased. This will further improve the applicability and use of the robot and would satisfy the two first principles: appropriateness of actions and adaptivity of the robot.

However providing the teacher with this control presents challenges for designing the interaction. Unlike a simple scalar reward, being able to control the robot requires the teacher to be able to give commands or advice to the robot and to receive additional information about the learner beyond its observable behaviour. This enriched two-way communication might be complex to design, especially when the action space is bigger than a few actions or the learning mechanism not transparent. In addition to the communication interface, the time scales of the interaction are also key: to give the opportunity to the teacher to preempt undesired actions, the learner needs to communicate its intentions in a timely manner to the teacher which complexifies the relation between the learner and the teacher. %This control also impacts the algorithm itself

%\cite{amershi2014power} emphasise throughout their paper the desire of user to have more control over the learning progress (through timing, suggestions, corrections, decisions...).

%\subsection{Challenges}

%Humans are bad teachers, not consistent, making errors...

\section{Summary}

This chapter presented first an overview of fields of \gls{hri} where robots interact socially with humans. From these cases of application, we defined three principles a robot controller should follow. To interact efficiently with humans, the robot should:
\begin{enumerate}
   	\item Only execute appropriate actions.
   	\item Have a high level of adaptivity and learn.
   	\item Have a high level of autonomy.
\end{enumerate}

Secondly, a review of current controllers for robots in \gls{hri} reported that no approach applied today in the field validates these principles. The review was extended to more general methods in \gls{ml} with potential to satisfy these principles. \gls{iml} shows promises for enabling a robot to learn online how to interact with humans, especially when the teacher is given control over the robot's behaviour and can demonstrate a correct action policy. However while humans have been used to teach robots behaviours or concepts, teaching them to interact with human in an interactive, online fashion has not been demonstrated in the field so far and would satisfy all these requirements.