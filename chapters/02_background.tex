%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Background} \label{chap:background}

This chapter describes the application of robot interacting with humans and related research in agent and robot control. In the first section the different fields of application of social \gls{hri} and then draws from them requirements for controlling a robot interacting with humans.  The second section provides the current state of the art in robot control for \gls{hri} and analyse it through the constraints presented in Section \ref{ssec:back_constraints}. And finally the third section describe the state of the field of \acrlong{iml} and how it has been used to \gls{hri}.

\section{Social Human-Robot Interaction}

\subsection{Fields of application}

\acrlong{hri} covers the full spectrum of interactions between humans and robots. However, as this thesis is focused on teaching robots to interact with humans, we used the following criteria on the interactions to select the subfields of \gls{hri} relevant to our research:
\begin{itemize}
\item interaction between a robot and a human: both the human and the robot are influencing each other's behaviour
\item Socially Interactive Robots as defined in \citep{Fong2003}: the social
    side of the interaction is key (unlike physical human-robot interaction, such as an exoskeleton, physical rehabilitation or pure teleoperation as in robotic assisted surgery).
\end{itemize}

\subsubsection{Socially Assistive Robotics}

	\gls{sar} is a term coined in \cite{feil2005defining} and refers to robot providing assistance to human user through social interaction and has been defined by \cite{tapus2007socially} as one of the grand challenges of robotics.
	
	\paragraph{One of the principal application of \gls{sar} is care for the elderlies}
	Ageing of population is a known challenge for today and the near future: the \cite{united2017world} reports than \emph{population aged 60 or over is growing faster than all younger age groups}. This will decrease the support ratio (number of worker per retiree) forcing society to find ways to provide care to an increasing number of persons using a decreasing workforce. Robots represent a unique opportunity to provide for this lacking workforce and support elderlies potentially allowing them to stay at home rather than joining elderly care centres \citep{di2014web} or supporting the nursing home staff \citep{wada2004effects}.
	
	%For this reason, robots for the elderlies is one of the major robotic market today and for the next decades as demonstrated by the number of projects all around the globe targeting this issue: Robot-Era \citep{bevilacqua2012robot}, Accompany project \citep{amirabdollahian2013accompany}, CompanionAble \citep{badii2009companionable}  and the Robear project in Japan    \footnote{\url{http://www.riken.jp/en/pr/press/2015/20150223_2/}} to cite     only a small subset of them.	
	
    However, the acceptance of these robots in elderly care facilities or at
    their homes, is still a complex task. Multiple studies report a good
    acceptance of robot and positive effects on stress in home cares both for
    the elderly and the nursing staff \citep{wada2004effects}, but as mentioned
    in \citep{broadbent2009acceptance}, this acceptance could be increased by
    matching more closely the behaviour of the robots to the actual needs of the
    patients.	
		
	\paragraph{The second main application of SAR is Robot Assisted Therapy}
    In addition of providing social support, robot can also be used in the process of therapies, following a patient during their rehabilitation to improve their health, their acceptance in society or recover from accident.
    In therapies, robots have first been used as physical platform to help
    patient to recover physically from strokes or cerebral palsy for example
    \citep{sivan2011systematic}. They were primally used as mechatronic tools
    helping humans to accomplish repetitive task. But in the late 90s,
    robots have started to be used for their social capabilities. For example the
    AuRoRA Project \citep{dautenhahn1999robots}
    started in 1998 to explore the use of robot as therapeutic tools for
    children with \gls{asd}.
    Since AuRoRA, many projects have started all around the world to use robot
    to help patient with ASD, as shown by the review presented in
    \citep{diehl2012clinical} or more recently the \gls{dream} \citep{esteban2017build}. \gls{rat} is not limited to \gls{asd} only, the use of robots
    is also explored in hospitals, for example to support children with diabetes
    \citep{belpaeme2012multimodal}, to support elderly with dementia
    \citep{wada2005psychological} or to provide encouragement and monitor cardiac rehabilitation \citep{lara2017human}.	
	
\subsubsection{Education} 
	Social robots are also being used in education, supporting teachers to provide learning content to children. As presented in \cite{mubin2013review}, the robot can take multiple roles (the \emph{tool} role has bee excluded for this overview due to its non-social interaction).
	
	\paragraph{Robotic teachers providing class lessons.} 
	One of the most obvious role robots could have in a classroom is replacing the teacher to provide the content to the children in an lesson as presented in \cite{verner2016science}. However this approach is seldom used in robots for education. The aim is not to replace teachers but to support them with new tools to improve the teaching process both for the teachers and the children, the review by \cite{mubin2013review} do not even mention robot teachers as a possible role for robots in educations. 
	
	\paragraph{Robotics tutors aim to provide tailored teaching content to the child(ren) they are interacting with.} 
	Studies reported that individualised   feedbacks can increase the performance of students \citep{bloom19842}, but    due to the large number of students supervised by a single teacher in classes today, tutoring is complex to apply. Robotic tutors could provide this powerful one to one tailored interaction not available in current classroom without increasing the number of teachers. Studies have also shown that adding robot in a classroom can improve the children learning outcomes     \citep{kanda2004interactive}. Additionally, robots can be used at home to     elicit advantages over web or paper based instructions     \citep{han2005educational}. However as pointed by \citet{kennedy2015robot}, social behaviours have to be carefully managed as a robot too social could    decrease the learning for the children compared to a less social robot. 
	
	\paragraph{Peer robots learning alongside the children.}
	A peer robot will not mentor the child to learn certain concepts, but will learn with or from the child. Unlike other roles of robots in education, peer robots can fit new roles still vacant in education today. For example, in Co-Writer \citep{hood2015children}, the child has to teach a robot how to write, and as the child demonstrate correct handwriting, they improves their own. Peer robots can leverage the ``prot\'eg\'e effect'': learning by teaching an agent. The robot can take the role of a less knowledgeable agent with endless patience and encourage the student to perform repetitive tasks such as handwriting and improve.
	
\subsubsection{Search and rescue, and military} 
	
    Robots are already used during search and rescue missions. After a natural or artificial catastrophe, robots can be sent to analyse the damage area and report or rescue the surviving victims of the incident \citep{murphy2008search}. These robots have to interact socially with two kinds of humans partners: the survivors and the rescue team. In both cases the social component of the interaction is key: the survivor can be in a shocked state and the robot could be the only link they received with the external world after the accident. In this case, a social response is expected from the robot and it has to be carefully controlled. On the other side, the rescue team monitoring the robots is under pressure to act quickly and can be stressed too. Even if the robot does not display social behaviour, rescuers interacting with it migh develop some feeling toward the robots they are using during these tense moments and this has to be taken into account when designing the robot and its behaviour \citep{fincannon2004evidence}.
	
    Similar human behaviours (emotional bonding with a robot) have also been observed in the army. Soldiers developed feelings toward the robot they use in a daily basis: taking pictures with it, introducing it to their friends and so on. This relation can go as far as soldier risking their life to try to save the robot used by their squad \citep{singer2009wired}. In that case, the social side of the robot have to be carefully managed to prevent it to have an opposite effect that the one desired: preventing the waste of human lives.
		
\subsubsection{Hospitality and Entertainment} 
	
	Robots are also interacting with humans in hotels around the world. The Relay robot (Savioke\footnote{\url{http://www.savioke.com/}}) deliver commands from the guests of hotels from the reception to their room. Whilst the social interaction is still minimal today, these robots interact everyday with humans and provoke social reactions from them. On the research side, scientists have explored robots as receptionist in a hall at Carnegie Melon University \citep{gockley2005designing} and robots as museums and exhibitions guide since the late 90s \citep{thrun1999minerva,burgard1999experiences} and since explore how to improve the social interaction for guide robots.
	
	Similarly, robots guide and advices humans in shops and shopping mall. Long term studies explored how humans perceive and interact with robots in this environment \cite{kanda2009affective} and how robots should behave with clients \cite{kanda2008will}.

    Robots have also entered homes and family circles. Twenty years ago, Sony created Aibo, a robotic dog to be used as pet in Japanese families. An analysis of online discussion of owners published 6 year after their introduction gives interesting insights on the relationship that owners created with their robots \citep{friedman2003hardware}. For example 42\% of the members assigned intentionality to the robot, like preferences, emotions or even feelings. Similar behaviours can also be observed when the robot is not presented as a pet, but just a tool. In \cite{fink2013living}, authors reported that a participant was worried that their Roomba would feel lonely when they would be in holidays. More recently, the Pepper robot has been sold to family in Japan, however, as of March 2018 no english study has reported results of the interaction with family members or long term use.

\subsubsection{Collaborative Robots in industry}
	%TODO: check citations
	Robots in industry used to be locked behind cage to prevent humans to interact with them and getting hurt. However recently, social robots such as Baxter \citep{guizzo2012rethink} have been designed to collaborate with humans and share the same workspace interacting physically and socially with factory workers. These robots need to be usable and reprogrammable by non-robotic experts and need to convey and understand intentions from motions and eye gaze \citep{bauer2008human}. The topic of motion legibility: how to convey intention using only motion trajectory is being covered extensively by the literature: in \citet{dragan2013legibility} and successive work, authors present ways to improve clarity of intentions and so improve the collaboration between humans and robots.

	%TODO: rework
    In addition to legibility, an other important topic in human-robot collaboration is task assignment: if a goal has to be achieved by a human-robot team, the repartition of tasks should be carefully managed to  optimise the end result in term of task performance, but also to ensure comfort for the human. Multiple sets of implicit rules have to be taken into account in a human-robot collaboration context, and so the task repartition system should be aware of them and follow them as proposed in \cite{montreuil2007planning}.
	
	%TODO: expand by using 3rd hand project and other

\subsection{Constraints} \label{ssec:back_constraints}

    The previous section demonstrated that robots are already interacting with sensitive populations: young children, elderlies, persons with handicap or in a stressful situations (victims of catastrophes, soldier or persons requiring healthcare for example). As pointed previously in the case of robots for the elderlies, this number is likely to rise for the other categories too. In this context, failure to meet expectations or lack of social norms or awareness can physically hurt humans around it or lead to offence, anger, frustration or boredom. As such, the behaviour of robots interacting with humans need to be carefully managed to meet expectations and behave socially.
    
    These undesired behaviours can have many different origins: lack of sensory capabilities to identify necessary environmental features, lack of knowledge to interpret human behaviours appropriately, failure to convey intentions, impossibility to execute the required action or incorrect action policy. Due to the wide range of origin, this research focuses only on the last point: obtaining a correct action policy. The other issues are either orthogonal and would lead to failure even with an ``optimal'' action policy as external factors prevent the robot to solve a problem or can be handled by having an appropriate action policy selecting suboptimal actions when the optimal ones cannot be used, or reacting correctly to human behaviours without having to interpret them on a higher level. 
    
    Appropriate actions are highly dependent on the interaction context: they could aim to match users' expectations of a robot behaviour, or complete a specific task. Nonetheless, the intention behind being \textit{appropriate} here is that actions executed should be guaranteed not to present risk for the humans involved in the interaction; for example, preventing physical harm or mental distress while achieving the robot assigned objectives.

    Additionally, interactions with humans ``in the wild'' \cite{belpaeme2012multimodal} do not happen in well defined environments or rigid laboratory setups: in the real world, robots will have to interact in diverse environments, with a large number of different persons, during extended periods of time or with initial incomplete or incorrect knowledge: as such the action policy needs to be adaptable to the context, to the users and over time. Robots need to be adaptive, both to react to changing environments, and to improve its action policy over time.

    Lastly, in many cases today, interactive robots are not autonomous but partially controlled by a human operator. We argue that to have a real Human-Robot Interaction, the robot needs to be as autonomous as possible. As pointed out in \citet{baxter2016characterising}, by relying too much on a human to control a robot, we are shifting from a human-robot to a human-human interaction using a robot as proxy. 
    
    Section \ref{sec:back_behaviour} analyses the current state of the art in action selection for social robots interacting with humans following these three axes introduced in the previous paragraphs:

    \begin{itemize}
        \item Appropriateness of actions
        \item Adaptivity
        \item Autonomy
    \end{itemize}

    HRI being a large field, other research axes are equally important such as  the complexity of the interaction, the depth of the interaction, the constraints put on the environment, the ability of the robot to set its own goals or the dependence on social rules. However, these axes are more influenced by the goal and the context of the specific human-robot interaction taking place rather than the action policy itself, so for this literature review, the focus is be on the three axis mentioned previously.

    We argue that to be able to sustain meaningful HRI, robots should score highly in the three axes presented before, following these three principles:

    \begin{enumerate}
        \item Only execute appropriate actions.
        \item Have a high level of adaptivity.
        \item Have a high level of autonomy.
    \end{enumerate}

\subsubsection{Appropriateness of Actions} \label{ssec:appropriateness}
    As argued previously, much of social human-robot interaction takes place in  stressful or at least sensitive environments where humans have particular expectations about the robot's behaviour. Additionally, even in less critical situations, human-human interactions are subject to a large set of social norms and conventions resulting from precise expectations of the interacting partners \citep{sherif1936psychology}. Some of these expectations are also transferred to interactions with robots \citep{bartneck2004design}.
	
    Failing to produce appropriate actions, for example by not matching the users' expectations, can have a negative impact on the interaction, potentially compromising future interactions if the human feel disrespected, confused or annoyed. Similarly failing to behave appropriately can harm the persons interacting with the robots: not reminding an elderly to take their medication, not taking into account the state of mind of survivors after a disaster or not behaving consistently with children with \gls{asd} can lead to dramatic consequences.  We argue that robots requires a way to ensure that all the actions they are executing should not present risk to the human involved in the interaction.
	
	%TODO: check if worth mentioning Asimov
    Some scholars argue that surprise is an important element in social HRI, as it can revitalise the interaction and increase the engagement of users \citep{lemaignan2014dynamics}. We do not argue that a robot needs to be consistently fully predictable, but that in any contexts of interaction the robot should not present danger similarly to the first law of Asimov \citep{asimov1942runaround}, but not as a law that the robot should follow, but as a design principle for the humans developing a robot behaviour or as defined by \cite{murphy2009beyond}: “A human may not deploy a robot without the human–robot work system meeting the highest legal and professional standards of safety and ethics.”
    
    Being able to behave appropriately is a real challenge: real interactions involve a large sensory space, with the interactants often being unpredictable or at least highly stochastic. In addition, social interaction is grounded by a large number of often implicit norms, with expectations being highly dependent on the context of interaction.  It seems unlikely that an action policy covering every possibility can be provided to the robot before the start of the interaction. For this reason, social robots need an action policy able to generate the appropriate reactions for the anticipated states, but they also have to manage uncertainty: being able to select a correct action even when facing a sensory state with no explicit predefined action to do. 

    In this literature review, the appropriateness of actions axis is a continuous spectrum characterising how much the system controlling the robot is in control of the interaction and can act in a safe way for the users at any moment of the interaction. For example, a robot selecting its action randomly would have a low appropriateness as no mechanism prevent the execution of unexpected or undesired action. On the other hand, a robot continuously selecting the action that a human expert would have selected would have a high value as domain experts have the knowledge of which action is the correct one in this interaction domain.


\subsubsection{Adaptivity}	\label{ssec:adap}
	%TODO: maybe change ref and see if I want to change a bit the content: not sure the axe of adaption in space, user and time is relevant...
    For reasons similar to the ones stated in Section \ref{ssec:appropriateness} and as pointed by other research groups \citep{argall2009survey, hoffman2016openwoz}, an optimal behaviour is unlikely to be programmable by hand. End users can express behaviours not anticipated by the designers, the environment is probably not perfectly defined or the desired behaviour might need to be customisable by or for the end user. For these reasons, robots interacting with humans need to be able to update their action policy to improve their behaviour. We use the term \emph{adaptivity} to represent this ability to change the action policy at runtime. This adaptivity has three components: the adaptivity in space (being able to change an action policy according to the environment), the adaptivity in users (being able to change an action policy according to the user) and the adaptivity in time (being able to change an action policy over time). 

    The same robot might be expected to interact in different environments. For example a robot used as an assistant for elderly people will have to interact in the home of the owner, but can also have to follow the owner in the street or in a supermarket. In these different environments, different behaviour will be expected from the robot, as such to be able to behave accordingly, the robot has to be adaptive in space.

    Additionally, in most of the application fields presented earlier, robots have to interact with a large number of users, and often, these interaction partners are not know in advance and can have different roles: in education the name and particularities of each child can hardly be specified in advance. In entertainment or search and rescue, none of the user is known beforehand. Adaptivity can be a way to identify the different users and adopt the action policy to suit the current user more specifically and update it according to this user's behaviour, preferences or capabilities.

    Similarly, in these fields where the interaction is social, robots can be expected to interact over an extensive period with the same user, e.g. companion robots for the elderly, military robots for a squad or robots used in \gls{rat}. With these long-term interactions, adaptivity in time allows the robot to tailor its behaviour to the current user and track the changes of preferences that could occur over long periods of interaction. Adaptivity in time can also allow the robot to learn from its errors and manage uncertainty better with more interactions.

    For this review, adaptivity will be a continuous scale ranging from no adaptivity at all (the robot has a single action policy that it will use in all the interactions), to high adaptivity (the robot dynamically change its action policy during an interaction and adapt it to the persons and context of the interaction). As this adaptivity is over three axes, some robots can  have a high adaptivity in users (by adapting their behaviour to the actions of their interactants), but not in time (if the same inputs always trigger the same output), and not in space (if only one specific context of interaction is taken into account). In that case, the action selection mechanism will receive a relatively low adaptivity rating.

\subsubsection{Autonomy}
	%TODO Reread
    We will show in Section \ref{subsec:WoZ}, that as of today, many experiments are conducted using a robot tele-operated by a human. Whilst having a human controlling the robot presents many advantages, e.g. the human can provide the knowledge and the adaptivity required and has sensing and reasoning capabilities not yet implemented on the robot, multiple reasons push us away from this type of interaction \citet{Thill2013}. It is not a scalable method to interact for a long period of time or on large scale, the human-robot interaction tends to become a human-human interaction \citep{baxter2016characterising} and it might introduce multiple biases in the robot behaviour \citep{howley2014effects}. For these reasons among many, we argue that a robot used in social HRI should be as autonomous as possible.
	
    The third axis of this literature review is the autonomy. As stated by \citet{beer2014toward}, autonomy is organised following a spectrum from no autonomy at all: a human is totally controlling the robot (doing sensory perception, analysis and action selection) to a full autonomy: the robot senses and acts on its environment without relying on human inputs. Levels exist between these extremes where a human and a robot share perception, decision or action: for example the robot can request information from a supervisor or the supervisor can overide the action or goal being executed
    
%    Some systems use a hybrid combination of human control and autonomy. In these shared autonomy system, the boundary between the autonomous control and the human can happen on several levels: from labelling of sensory inputs \citep{depalma2016nimbus} to assign reward to action to teach the robot an action policy \citep{thomaz2008teachable}. Similarly, this help can be triggered by the robot or the human, at specific stages of the interaction or at any time or can be a simple guidance or a command. 

\section{Current robot behaviours in HRI} \label{sec:back_behaviour}

    This section presents diverse approaches currently used in HRI to control a robot. As the number of individual techniques is too large for an exhaustive review, we organised the literature into broader categories. For each category, we will present the corresponding approach, indicate leading works done in this direction and qualitatively rate it on the three axes defined in the previous section.

\subsection{Wizard of Oz} \label{subsec:WoZ}

	\acrfull{woz} is a specific case of tele-operation where the robot is not autonomous but at least partially controlled by an external human operator to create the illusion of autonomy in an interaction with a user. It outsources the difficulty of action selection or sensory interpretation to a human operator. This technique has emerged from the \gls{hci} field in 1983 \citep{kelley1983empirical} and is today common practice in \gls{hri} \citep{riek2012wizard}. \gls{woz} can assume different levels corresponding to the involvement of the human in the action selection process. \cite{baxter2016characterising} differentiate two main levels: perceptual \gls{woz} (the human only replace sensory system and feed information to the robot controller) and cognitive \gls{woz} (the human makes decisions about what the robot should do next). This method can also be used to gather data to develop a robot controller from human demonstration (cf. Section: \ref{ssec:back_lfd}).
 
	Similarly to the different levels of autonomy presented earlier, systems can combine human control and predefined autonomous behaviour. \citet{shiomi2008semi} proposes a semi-autonomous communication robot. This robot is mainly autonomous, but has the ability to make explicit request to a human supervisor in predefined cases where the sensory input is not clear enough to make an action.
 
	With \gls{woz}, the adaptivity and the appropriateness are provided almost exclusively by the human, so these characteristics are dependent of the human expertise and are generally optimal. However, due to the reliance on human supervision to control the robot, the autonomy is low. For semi-autonomous robots, the picture is more complex: as explained by \cite{beer2014toward}, the initiative, the human's role and the quantity of information and control shared will influence the level of autonomy. For example, in \citep{shiomi2008semi} the robot explicitly makes requests to the human, but the human cannot take the initiative to step in the interaction limiting the adaptivity (especially as the robot policy is fixed) and as no mechanism prevents the robot to make undesired decision, the appropriateness of actions is average compared to Wizard of Oz.
	
\subsection{Fixed preprogrammed behaviour}

    One of the simplest ways to have a robot interacting with a human is to have an explicit fixed behaviour. The robot is fully autonomous and follows a script or a finite state machine for action selection. This approach is dependent on having a well defined and predictable environment to have the interaction running smoothly. If the interaction modalities (possible range of behaviour and goal) are limited enough, an optimal behaviour for the robot can be predefined for all (sensible) human actions. This approach is followed in a large number of research in \gls{hri}. Many studies being human-centred, the focus is not in the complexity of the robot's behaviour but on how different humans would interact with and react to a robot displaying a fixed behaviour (for comparison purposes). While being useful to explore human's reactions to robots, due to the low range of application outside of well defined environments, this method can hardly be used to deploy robots to interact with humans on a daily basis.

    By essence, this type of controller has a no adaptivity as the robot is following a preprogrammed script, but scores highly on autonomy as no external human is required to control the robot and the behaviour is mostly appropriate. 

\subsection{Adaptive preprogrammed behaviour}
	
	To go beyond a script, the robot can also react to the human behaviour. The robot could be programmed with different behaviours, and then select the one corresponding to the current interaction following rules given prior the experiment. For example, in \citet{leyzberg2014personalizing} the robot can deliver some predefined content according to the current performance of the participant, presenting personalised behaviour as long as the participants is behaving within expectations. 

	Reactive controllers mapping directly input data to behaviour without trying to reach specific high level goals can also be used successfully in \gls{hri}. For example, homeostasis, the tendency to keep multiple elements at equilibrium, is constantly used by living systems to survive and have been used to control robot in social interaction. \citet{breazeal1998motivational} uses a set of drives (social, stimulation, security and fatigue) which are represented by a variable each and have to be kept within a predefined range. If these values are outside the desired homeostatic range, the robot is either over or under-stimulated and this will affect its emotion status and it will display an emotion accordingly. Homeostasis approaches have also been extended to robotic pets \citep{arkin2003ethological} or \gls{rat} \citet{cao2017collaborative}.

	Due to the implicit description of behaviours, homeostasis-based methods are more robust in unconstrained environments than a purely scripted controller, while remaining totally autonomous. However the action policy is not adaptive and as the behaviours are not totally defined and controlled, there is no guarantee against the robot acting in inconsistent way in some specific cases limiting the appropriateness of actions.

	Efforts have been made to extend the homeostasis approaches beyond purely reactive systems with the use of hormone models for example \citep{Lones2014}. This method allows the previous experiences of the robot to impact the behaviour expressed providing limited adaptivity in time to the robot. However this approach was only applied to a robot interacting in a non-social environment.
	
	Both predefined adaptation and homeostasis-based methods score highly in autonomy and relatively highly appropriateness, but fairly low in adaptation as they can only adapt within predefined and anticipated boundaries.

\subsection{Learning from Demonstration} \label{ssec:back_lfd}
%mention that its applicable to end users and allow designers to have to implement the behaviour and all the specific cases
	%motivation for lfd
	%behaviour too complex to be coded or requirement of end user knowledge
	%MIght require ref
	As stated by numerous researchers, explicitly defining a robot behaviour and manually implementing it on a robot can take a prohibitive amount of time or not be possible at all. This statement applies both to manipulation tasks and social interaction. However in both cases, humans have some knowledge or expertise that should be transferred to the robot. And in many case for social robots experts of the field do not have the technical knowledge to implement this behaviour on a robot. 
	
	%As when interacting it might be complicated and time consuming to acquire data points for learning, offline learning methods are mainly inspired from the Learning from Demonstration framework (LfD) \citep{argall2009survey}. With LfD, the idea is to take inspiration from human demonstrations to accelerate the learning for an agent or to teach tasks that could not be preprogrammed manually \citep{billard2013robot}. The classical approach starts with observing a human completing the task and then deriving a corresponding robot behaviour to match the human one. In most of the cases, the learning only occurs once: data is accumulated first and then batch learning is applied to derive a static action policy. 
	    
	The field of \acrfull{lfd} aims to tackle these two challenges: implementing behaviours too complex to be specified in term of code and empowering end-users with limited technical knowledge to transfer an action policy to a robot. Humans demonstrate a correct behaviour through different control means \citep{argall2009survey}, and then offline batch learning is applied to obtain an action policy for the robot and if requirement reinforcement learning can complement the demonstrations to reach a successful action policy \citep{billard2008robot}.
	
	%importance of hri - but in teaching, often not object
	In \gls{lfd} the interaction between the robot and the human teacher is key, however in most of the cases the object of the learning is not social interaction with humans, but manipulation or locomotion tasks. For example, \cite{Abbeel2004} used telecontroled helicopters experts to demonstrate flying acrobatics, and extracted from these demonstrations a reward function used to train a controller to reproduce these acrobatics at a super-human level through Inverse Reinforcement Learning.
	
	%examples with hri as target (liu, sequeria and clark) - but examples limited	
	% Except Liu (learning from data) and restricted WoZ
	Two approaches have explored using \gls{lfd} to teach robots a social policy to interact with humans.
	
	The first one is using human-human interactions to collect data about human behaviours and transfer them to a robot. \citet{liu2014train} present a data driven approach taking demonstration from human-human interactions to gather relevant features defining human social behaviour. Authors recorded motion and speech from about 180 interactions in a simulated shopping scenario, then behaviours have been clustered into \emph{behaviour elements}. Finally, during the interaction, the robot uses a variable-length Markov model predictor to estimate the selection probability of each actions by the human demonstrator, and then selects the one with the highest probability. According to the authors, the fianl performance of the robot was not perfect, but if this approach was scaled using a larger dataset gathered from normal human-human interactions in the real world, the performance should improve and become closer to natural human behaviours.
    
    Alternatively, the data can be collected from a \acrlong{woz} setup. \citep{knox2014learning} coined this approach \emph{Learning from Wizard}: starting for a purely \gls{woz} control to gather data, and then apply machine learning to derive an action policy. But this paper presents no description of which algorithm could be use or how, and gives no evaluation of the approach, but instead only offers a reflection on the application of this idea.
    
    This Learning from Wizard have been implement by two groups of researchers. \citet{sequeira2016discovering} extended the idea to a full methodology to obtain a fully autonomous robot tutor. This method is composed of multiple steps starting with the observation of a human teacher performing the task. Then, the different features used by the teacher to select their actions as well as the actions themselves are encoded and implemented in a robot. The next step is setting up a \gls{woz} experiment where the operator has access to the same features than the robot to make his decisions and controls the robot's action. Then, a combination manually derived rules and machine learning is applied on the data from the restricted-perception experiment and finally the robot is tested autonomously. Additional offline refinement steps are possible if the behaviour is not exactly the one desired. 
    
    Both \citet{knox2014learning} and \citet{sequeira2016discovering} stress the importance of using similar features for the Wizard of Oz part than the ones available to the robot during the autonomous part: whilst decreasing the performance in the first interaction, it allows more accurate learning due to the similarity of inputs for the robot and the human controlling it.
        
    \cite{clark2018deep} decided to bypass these limitations by using a deep Q network \citep{mnih2015human} to learn a ABA behaviour for \gsl{rat}. They recorded videos, microphone inputs and actions selected in a \gls{woz} interaction with participants. Then, the network learns from the raw inputs and the actions selected an action policy to control the robot and deliver the therapy. However in their study, the performance of the system was low (less than 70\%) which means that the robot would provided inconsistent feedback at some point and they required limited human inputs to reach that level. 
     
    %TODO: see if required, and if yes how to make it better   
    As these methods are based on real interactions either between humans, or between humans and robots controlled by humans, with enough demonstrations the robot should be able to select the appropriate actions. However, as no intrinsic mechanism is present to prevent the execution of undesired actions which could happen if the robot ends up in an unseen state, the appropriateness of actions cannot be maximal. Additionally, the adaptivity in time is limited, as for most of the techniques the learning happens only once and then the behaviour is fixed and the learning is only used in a specific context. However, the framework proposed by restricted-perception Wizard of Oz should allow asynchronous adaptivity in time using the refinement phase. And finally, all these methods require the presence of humans in a first phase but the robots are fully autonomous later in the interaction, so the autonomy is high during the main part of the interaction.
    
\subsection{Planning} \label{ssec:planning}
    
    An alternative way to interact in more complex environment is to use planning. The robot has access to a set of actions with preconditions and postconditions and has to achieve a defined goal. To do so, it follows the three planning steps: sense, plan and act. The first step, sense, is to acquire information about the current state of the environment. Then, based on the set of actions available and the goal, a plan (i.e. a succession of actions) is created which should result in reaching the goal, while respcting pre- and post conditions when selecting actions. Finally, the last step is to execute the plan. If the resulting state of the new environment is not the one desired, the robot replans to find a solution suitable for the new state.
    
    %This approach is often used in motion planning, but can integrate some %social aspect as presented in the work of Dragan and colleagues %\citep{dragan2013legibility}, where motion planning is adapted to be more %legible by human. Planning can also be used at a higher level for action %selection.
    
    In the literature, planning has not been used to generate an action policy to interact social with a human, however it has been used to assign tasks to both members of a human-robot team to achieve a defined task. One example of this is the Human Aware Task Planner \citep{alili2009task}. One property of this planner is the ability to take into account predefined social rules, such as reducing human idle time, when creating a plan specifying what the human and robot should do.
    
    Planning performance depends heavily on the model of the environment the robot has access to. A detailed model can ensure that the robot will select the appropriate action whilst being totally autonomous. Similarly, the adaptivity depends on the model the robot has access to and whether it can update it in real time. However, in many cases when interacting with humans, the model is static, only covering a subset of the different tasks that the robot can be required to achieve and the different contexts it can face.
    
    %As long as the model is correct enough, it is ensured to keep the %``astonishment'' low and to maintain a high level of autonomy. The %adaptivity is also highly dependent on the model the robot has access to. %If the planning domain is large enough, the adaptivity can be high, %multiple users can be also predefined to increase the user adaptability, %and if the domain knowledge can be updated dynamically.
    
    %\marginpar{The previous paragraph is a bit unclear to me, but you can leave %it in the RDC2. If you use it anywhere else, it would need to be %rewritten.}
    
    Planning can also be extended with learning, which then allows for adaptive action policies. This has been done in motion planning, to obtain a better trajectory \citep{jain2013learning,beetz2004rpllearn} and action selection planning \citep{kirsch2009robot}. But to our knowledge, no planner used in social HRI includes a module allowing it to change its model by increasing the number of actions, adding new rules or changing the pre- and postcondition of actions at runtime.

\subsection{Learning from the Interaction}
	%Mention reinforcement learning here or in a special section??

	A last time of controller seldom used for creating action policies to interact with humans is learning from the interaction. The robot can create an action policy by exploring and learning from interacting with the world. This method assumes that the agent can update its action policy online, or regularly, and refine its knowledge of the world or at least of how to interact within it. 
	
	However, due to the complexity to create such a system and to the challenges of social interactions with humans this approach is almost never used. One example where a robot learns a non social policy online by receiving spoken human commands and explaination is using the DIARC cognitive architecture \cite{scheutz2017spoken}. In this paper, authors describe how a robot can learn concepts and mapping actions to verbal commands by receiving human information.

	This approach is the one with the most potential as the humans could provide only the required supervision or guidance and let the robot be autonomous most of the time, the learning provide potentially open-ended adaptivity and finally, the with enough data points and the presence of a human in the action selection loop if required, the appropriateness of actions could be guaranteed.

	%Relatively common in HRI, but the hri is the teaching process, not the object of the teaching
	
\subsection{Summary}

	Table X presents a summary of the different approaches currently used in \gls{hri} with their rating on each of the categories and how widely their are applied. As shown in the paper, the most promising type of control is \textit{Learning from the interaction}, but it is seldom used to learn how to interact with humans. However, other fields (\acrlong{rl} and \gls{hci}) have explored ways of controlling agents included in this larger approach and we will present them in the next sections.

\section{Reinforcement Learning} 	
\subsection{Concept}
	%TODO: maybe highlight why it would be intersting for HRI: life long learning and improving, learning from errors -> interesting to have a RL component in robots deployed - explain that normally the robot starts without initial knowledge
	Young infants and adults learn by interacting with their environment, by producing actions, analysing how the environment reacts and measure progress toward a goal. Similarly, the field of \acrfull{rl} aims to empower agents by making them learn by interacting, using results from trials and errors and potentially delayed rewards to reach an optimal, or at least efficient, action policy \cite{sutton1998reinforcement}.

	%Discrete time - life as a sequence!
	The simplest version of \glossary{rl} is modelled as a finite \acrfull{mdp}, a 5-tuples $(S, A, P_a(s,s'), R_a(s,s'), \gamma)$, with:
	\begin{itemize}
		\item $S$: a finite set of states defining the agent and environment states
		\item $A$: a finite set of actions available to the agent
		\item $P_a(s,s')$: the probability of transition from state $s$ to $s'$ following action $a$
		\item $R_a(s,s')$: the immediate reward following transition $s$ to $s'$ due to action $a$
		\item $\gamma$: a discount factor applied to future rewards
	\end{itemize}

	The goal of the \gls{rl} agent is to find the optimal policy $\pi_*$ maximising the discounted sum of future rewards. The agent is not aware of all the parameters of the model, and only observes the transitions between states and the rewards provided by the environment and has to update its policy to maximising this cumulated reward. Different algorithms exist to reach this policy, but the main features present in all of them is the concepts of exploration and exploitation.
	
	Exploration reflects the idea to try out new actions to learn more on the environment and potential gain knowledge improving the policy whilst exploitation is the execution of the current best policy to maximise the current gain of rewards. All the algorithms have to balance these two features to reach an optimal action policy. One way to deal with this trade off is to start with high probably of exploration to collect knowledge on the environment and then decrease this probably to converge toward a policy using this knowledge to make better choice of actions.
	
	The more complex the environment is, the longer the agent has to explore before converging to a good action policy, it is not uncommon to reach numbers such as millions of iterations before reaching an appropriate action policy. And during this exploration phase, the agent's behaviour might seem erratic as the agent tries actions often randomly to observe how the environment is reacting.
	
	%Challenges - tradeoff exploration/itation - uniqueness of data (fleeing nature of time and data) - non statioanrity - sequential delayed consequences ...
	%success: backgammon, helicopter, advertisements, jeopardy, atari -> better perf than any other methods - and "without using human instructions"
	
	%concept of action value function of a policy: value of doing action in state
	%optimal value function = value function of the optimal policy (include delayed consequences)
	%q-learning works for any type of MDP
	%off policy: learning without doing the target policy + behaviour policy
	%policy iteration: evaluate policy - obtain q - greedify to new policy - evalute - obtain new q..
	%in tabular, converge in few iterations, robust
	%bootstrapping: updating an estimate from an estimate: bellman equation
	%Need approximation for generalisation and learning faster - approximate the action value function (linear weighting of features)
	%the function approx subsume much of the problem of hidden state
	%it works often with q-learning, but loses the guaranty of converging! -> still need theoretical work
	%update parameter vector at each time step according to error
	%better with sarsa on policy algo -> often epsilon greedy
\subsection{Limitations}

	As explained in the previous section, traditional \gls{rl} has two main issues: requirement of exploration to gather knowledge about the environment and large number of iteration before converging. Generally, \gls{rl} copes with these issues by using a simulator to test the agent, which allows the agent to explore safely in an environment where its actions have no impact on the real world and where the speed of the interaction can be highly increase to gather the required datapoints in a more reasonable amount of time. However, no simulator of human beings exists today which would be accurate enough to learn an action policy applicable in the real world. Learning to interact with humans by interacting with them would have to take place in the physical world, with real humans, and this implies that these issues would have direct impacts. 
	
	To gather informations about the environment, the agent needs to explore, trying out random actions to learn how the humans respond to them and if the agent should repeat them later. When interacting with humans, executing random actions can have dramatic effect on the users, presenting risk of physical harm as robot are often stiff and strong. This reliance on random exploration presents a clear violation of the first principle to interact with humans presented earlier (all actions need to be appropriate).
	
	Even if random behaviour were acceptable, humans are complex creatures, behaving stochastically, with personal preferences and desires. And as such, learning to interact with them from scratch would would require large number of datapoints and as interactions with humans are slow (not many actions per minutes). 
	
	\gls{rl} has been used in robotics despite these limitations\citep{kober2013reinforcement}, but similarly to \gls{lfd} mostly to manipulation, locomotion or navigation tasks but not to learn social behaviours for \gls{hri}. 
	
	
\subsection{Opportunities}  
	Despite the limitations presented in the previous section, some changes can be made to \gls{rl} to increase its application to \gls{hri}.
	  
	\cite{garcia2015comprehensive}, insist on \textit{safe} \gls{rl}, ways to ensure that even in the early stages of the interaction, when the agent is still learning about the world, its action policy still achieve a minimal acceptable performance. Authors present two ways to achieve this safety: either use a mechanism to prevent the execution of non-safe actions or provide the agent with enough initial knowledge to ensure that it is staying in a safe interaction zone. These two methods are not limited to \gls{rl} but can also be used with other machine learning techniques. 
	
	The first method (preventing the agent to execute undesired actions) can be implemented by explicitly preventing the agent to execute specific actions in predefined states, however it seems unlikely that every case could be specified in advance. As such, the easiest way in to include a human in the action selection loop, as a way to be sure that undesired actions are pre-empted before being executed. Learning by interacting with humans is called \acrlong{iml} and is described more in depth in Section \ref{sec:back_iml}.
	
	The second method (providing enough initial knowledge) can be achieved by carefully engineering the features used by the algorithm or starting from a initial action policy to build upon. \cite{Abbeel2004} propose to use humans demonstrations in a fashion similar to \gls{lfd} but to learn a reward function and an initial working action policy. This method, Inverse Reinforcement Learning has been applied successfully to teach a flying behaviour to a robotic helicopter. Once the initial policy and the reward function are estimated, \gls{rl} is applied around the provided policy to explore and optimise the policy. That way, authors can ensure that only small variation of the policy will happen around the demonstrated one. This small variations can ensure that policies leading to incorrect behaviours can be negatively reinforced and avoided before creating issues (such as crashing in the case of the robotic helicopter).
	
	%Similarly to autonomous online learning, these approaches score high in adaptivity in time and depending of the learning mechanism, sensory inputs and actions, they also do well in being adaptive in the search space and to changing user behaviour. The reliance on human input decreases the autonomy, but increases the appropriateness of actions as this guidance can help to reduce the use of random exploration. However, as the human has often only a partial control, the robot is not ensured to act correctly at every stage of the interaction preventing the appropriateness to be maximal.
	
	Whilst being promising and having been applied for agents in human environments (such as for personalised advertisement - \citealt{theocharous2015personalized})	these approaches have not been used social behaviour or to have robot interacting with humans.

\section{Interactive Machine Learning} \label{sec:back_iml}
%Mostly for supervised learning 

Machine learning is a promising method to provide a robot with an adequate action policy without having to implement in advance all the features used by the action selection mechanism. Offline learning is a technique allowing the robot to change its action policy over time by updating the action policy outside of the interaction. Between interactions, a learning algorithm is used to create a new action policy derived from the previous experiences.

However online learning (such as \gls{rl}) have the advantage of benefiting from multiple updates, constantly refining the agent behaviour, rather than a single monolithic definition of a behaviour. As mentioned in the previous section for the case of \gls{rl}, including humans in the action selection loop can provide tremendous advantages compared to fully autonomous learning.

\acrfull{iml}, as coined by \cite{fails2003interactive}, differs from \acrfull{cml} by integrating an expert end-user in the learning process. In classical supervised learning, such as deep learning \cite{lecun2015deep}, the learning phase happens offline once to obtain a classifier for later use. On the other hand, \acrshort{iml} is an iterative online process using a human to correct the errors made by the algorithm as they appear.

\cite{amershi2014power} presents an introduction to \gls{iml} by reviewing the work done and presenting classical approaches and challenges faced when using humans to support machine learning.

%Need to careful not to just rewrite power to people + need to add new material 

\subsection{Goal}

The main goal behind \gls{iml} is to leverage the human knowledge during the learning process to speed it up. To extend the use of classifiers from static algorithms trained only once, to evolving agents learning from humans and refining their policies over time. \gls{iml} can be applied in a \gls{rl} approach or \gls{sl} but tries to combine advantages from both worlds. As explained in \cite{fails2003interactive}, classifiers gain to be fast rather than highly inductive and \gls{rl} can gain from using humans to provide rewards \cite{knox2009interactively}.

By allowing a human user to see the output of an algorithms and provide additional inputs, the learning can be faster and tailored to this human desires. Using human expert knowledge and intuition, the system can achieve a better performance faster.

\subsection{Active learning} \label{ssec:back_active}

Active learning is a form of teaching used in education aiming to increase student achievement by giving them a more active role in the teaching process \citep{johnson1991active}. This approach has been transferred to machine learning, and especially classifiers by allowing the learner to ask questions, query labels from an oracle for specific datapoints with high uncertainty \citep{settles2012active}. The typical application case is when unlabelled data are plentiful, but labels can be limited in number or costly to obtained. As such a trade-off arises between the performance of the classifier and the quantity of queries made by the algorithm. Often this oracle would be a human annotator with the ability to provide a correct label to any datapoint.

%different challenges for classic active learning and robot teaching (inter)active learning
Using an oracle to provide the label of specific points aims to both improve accuracy and speed up the learning as obtaining label of point with high uncertainty increase the precision of the algorithm and should highlight missing features in the current classifier. However, this relation between the learner and the human teacher poses questions such as: 
\begin{itemize}
	\item Which points should be selected for the query?
	\item How often the human should be queried?
	\item Who controls the interaction? (i.e. who has the initiative to trigger a query?)
\end{itemize}

Researchers have explored optimal strategies for dealing with the relation between the learner and the oracle, and this research has been especially active in \gls{hri} with robots directly asking questions to human participants and how the robot's queries could inform the teacher about the knowledge of the learning \citep{chao2010transparent}. In a follow up study, \cite{cakmak2010designing} showed that most users preferred the robot to be proactive and involved in the learning process but they also wanted to be in control of the interaction, deciding when the robot could ask questions even if it imposed a higher workload on the teacher. Authors proposed that when teaching a complex task requiring a high workload on the teacher, the robot would probably be expected or should be encouraged to take a more pro-active stance requesting samples to take over some workload from the teacher.
%User are human and want to be considered as such, not oracle (want more control) not willing to be simple oracle - human preferred being in control of the rate and timing of question, rather than being simply a labeller.

Active learning, being able to select a specific sample for labelling, can dramatically improve the performance of the learning algorithm. However, in interaction, the learner is not in control of which sample to submit to an oracle to obtain a label. Datapoints are provide by the interaction and are influenced by the learner actions and the environment reaction. To tackle this issue: not being able to decide which sample to evaluate but still profiting from an active stance of the learner,  \cite{chernova2009} presented the Confidence Based Algorithm. The robot initially provided with demonstrations of a correct action policy and then has to interact in the world under supervision from an human user. Authors propose that the teacher should still be able to provide corrective demonstrations in case of incorrect behaviour, but the learner should also be able to request a demonstration if the confidence of which action to select is below a threshold. That way, the learner can mitigate autonomous behaviour and human support. However, the effectiveness of this approach is bounded by the capacity of the learner to estimate this confidence in the desired policy, to request demonstrations only to prevent incorrect behaviour without relying excessively on the human oracle.
%limit the risks of the exploration and increase the learning speed - demonstrations are not fully consistent so the robot might find states where there is no obvious action to select. 

%LImited application to learn hri as the robot cannot select its samples itself
\subsection{Human as a source of feedback on actions}

When the learner is agent learning an action policy (rather than a classifier) and the learner is expected to improve its behaviour by interacting with the environment, a intuitive way to steer the behaviour in the desired direction is to use human rewards. This approach is an adaptation of ``shaping'': tuning a animal's behaviour by providing rewards. In \gls{ml}, using rewards from a human teacher is a \textit{simple} way to bias the learning in the desired direction: the interface is easy, the teacher just need a way to provide a scalar or a binary evaluation of an action to steer the learning. Additionally, these approaches can profit from the research done in \gls{rl}. These human rewards can be used in a different ways: they can complement environmental ones \citep{knox2010combining} or be used on their own. 

When used on their own, human rewards enable an agent to learn an action policy even in the absence of any environmental rewards, which is specially interesting to robots as it can be complex to define a clear reward function applicable to \gls{hri}. Early work in that field came from \cite{isbell2006cobot} who designed an agent to interact with a community in the LambdaMOO text based environment. Cobot, the agent had a statistical graph of users and their interactions and could execute some actions in the environment. Users of LambdaMOO could either reinforce positively or negatively Cobot's action by providing rewards. Isbell et al. presented the first agent to learn social interactions in a complex human online social environment. 

While the goal of Cobot was to create an entity interacting with humans, \cite{knox2009interactively} explored how a human can teach an agent an action policy with TAMER (Training an Agent Manually via Evaluative Reinforcement). The agent uses a supervised learner to model the human reward function and then takes the action that would receive the highest reward from the supervisor. Other work explored how an agent can infer more knowledge from a reward than only its value. Advice \citep{griffith2013policy} models the confidence a learner can have in its teacher to make better use of rewards. \citep{loftin2016learning} explore the strategy used by the teacher in the reward delivery: the meaning of not rewarding an action can vary from teacher, from a tacit acknowledgement of the action to the active refusal to provide a positive reward (indicating the incorrectness of an action). \cite{macglashan2017interactive} proposed COACH (Convergent Actor-Critic by Humans) to adapt the interpretation of feedback to the current policy, for example, a suboptimal policy could receive positive feedback early on when it compare positively to the average behaviour, while receiving negative feedback later on the teaching when the average agent performance is better.

%Could read more about this
Other approaches combine traditional \gls{rl} with critique from humans. A human is requested to evaluate specific behaviours \citep{judah2010reinforcement} or compare to policies to select the most appropriate one \citep{christiano2017deep}.

%Human don't want to provide only label, they want to explain \cite{stumpf2007toward} + importance on transparency: help to achieve better results and improve user experience

\cite{thomaz2008teachable} aimed to explore how humans would use feedback to teach a robot how to solve a task,  here baking a cake. They used Interactive Reinforcement Learning as a way to directly combine environment rewards and human ones. However, during early studies, Thomaz et al. discovered that participants tried to use rewards to convey intention, informing the robot which part of the environment it should interact with. The next study involved to communication channels, a reward one to provide feedback on the actions and a guidance one to provide information about which part of the environment the robot should interact with. This guidance has been actively decided to be ambiguous, participants could not explicitly control the robot, but just reduce the exploration. Adding this second channel improved the performance of participants.

%present the , a method combining feedback from the environment and feedback from a human supervisor to learn a task in a non-social context. Similarly, \citet{knox2009interactively} propose to use RL in an environment where the feedback is not given by the environment itself, but by a human only. In these two cases, human feedback is used to scaffold the learning: it makes it faster and safer and can allow the use of RL in environments without explicit reward. However as the feedback is always given after the execution of the action, there is no guaranty that only expected actions will be executed. This might be a reason why these two approaches have not been used to learn an action policy for social human-robot interactions. But they are nevertheless relevant as they are using the interaction with a human to help an agent to learn an action policy faster and safer than pure RL.



conclusion of subpart??
%feedback not enough
%When given choice, human will never teach only using rewards  %See if %More recent stuff with deep comparison and things - still requires simulation and so on
%citation makes sense
\subsection{Importance of control}

Results from both active learning and research using human to provide feedback have shown that human teacher desire control \citep{amershi2014power}. Humans are not oracle, enjoying providing labels and evaluating an agent's actions they desire to be in control of the learning and provide richer information to the agent. \cite{kaochar2011towards} have shown than when given choice between different teaching methods, humans will never choose to limit themselves to only feedback, but they want to teach using more modalities.

In addition to improve the teacher's experience in the teaching, providing the teacher with more control can improve the learning. By allowing the teacher to demonstrate online an action policy or pre-empt undesired actions the robot is about to execute, the learner can interact mostly in useful states of the environment, learn faster and improve its performance in early stages of the learning.

However providing the teacher with this control presents challenges for designing the interaction. Unlike a simple scalar for reward, being able to control the robot requires the teacher to be able to ask the robot to execute any actions,  which can be complex when the action space is bigger than ten actions. Similarly, to give the opportunity to the teacher to pre-empt undesired actions, the learner needs to communicate its intentions to the teacher. %This control also impacts the algorithm itself

%\cite{amershi2014power} emphasise throughout their paper the desire of user to have more control over the learning progress (through timing, suggestions, corrections, decisions...).


%\subsection{Challenges}

%Humans are bad teachers, not consistent, making errors...

\section{Summary}

This chapter presented first a overview of fields where robots interact socially with humans. From these case of application, three principle have been defined that a robot controller should follow to interact efficiently with humans, the robot should:
\begin{enumerate}
   	\item Only execute appropriate actions.
   	\item Have a high level of adaptivity.
   	\item Have a high level of autonomy.
\end{enumerate}

A review of current controller for robots in \gls{hri} reported that no approach applied today in the field validates these principles. The review was extended to more general methods in \acrlong{ml} with potential to satisfy these principles. \acrlong{iml} show promises for enabling a robot to learn how to interact with humans, however while humans have been used to teach robot behaviours or concept, teaching them to interact with human in an interactive, online fashion has not been demonstrated in the field so far and could satisfy all these requirements.