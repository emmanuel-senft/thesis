\chapter{Background} \label{chap:background}
\glsresetall

This chapter describes social \gls{hri} and presents related research in agent and robot control. The first section introduces the different fields of application of social \gls{hri} and draws from them requirements for controlling a robot interacting with humans (the robot should: only execute appropriate actions, and have a high level of adaptivity and autonomy). The second section provides the current state of the art in robot control for \gls{hri} and analyses it through the requirements presented in the previous section. And finally, building on the lack of controller satisfying the requirements from Section 1, the third section presents \gls{iml}, an alternative method holding promise to teach agents how to interact, and how it could be applied to \gls{hri}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Social Human-Robot Interaction}

\subsection{Fields of Application} \label{ssec:back_hri}

Reviews of \gls{hri} and socially interactive robots have already been presented in \cite{fong2003survey}, \cite{goodrich2007human} and \cite{sheridan2016human}. However, these reviews are starting to be dated and/or did not have the same focus than this research. As mentioned in the introduction, this work is focused on social robots, robots perceived as social agents and interacting in a human-centred environment. Depending on the context of interaction, these social robots will have been designed to interact socially with humans, or sometimes, the perception of sociality and agency will simply emerge from the interaction~\citep{fincannon2004evidence}. As such, the following criteria on the interactions were used to select the subfields of \gls{hri} relevant to this research:
\begin{itemize}
	\item Presence of an interaction between a robot and a human (collocated or not): both the human and the robot are influencing each other's behaviour.
	\item The human partner considers the robot as a social agent, responding socially to the robot and potentially bonding with it.
\end{itemize}

We reorganised and edited Goodrich and Schultz's categories to fit with the criteria presented above, which resulted in five subfields involving social interactions between robots and humans: 
\begin{itemize}
	\item \acrfull{sar}
	\item Education
	\item Search and Rescue, and Military
	\item Hospitality, Home and Entertainment
	\item Collaborative Robots in Industry
\end{itemize}
We updated each category with recent research and highlighted the social component of the interaction and its impact on the controller used for the robot.

\subsubsection{Socially Assistive Robotics}

\gls{sar} is a term coined by \cite{feil2005defining} and refers to a robot providing assistance to human users through social interaction. This field has been defined by \cite{tapus2007socially} as one of the grand challenges of robotics.

One of the main applications of \gls{sar} is care for the elderly. In the near future, the ageing of population will have large impacts on the world, and societies will have to find solutions to tackle this challenge. The \cite{united2017world} reports that the ``population aged 60 or over is growing faster than all younger age groups''. This unbalance of growth will decrease the support ratio (number of workers per retiree) forcing societies to find ways to provide care to an increasing number of people using a decreasing staff. Robots represent a unique opportunity to compensate for this lacking workforce potentially allowing elderly to stay at home rather than joining elderly care centres~\citep{di2014web} or simply to support the nursing staff~\citep{wada2004effects}.

The second main application of \gls{sar} is \gls{rat}. Robots might be used to provide therapy or to follow and support a patient during their rehabilitation to improve their health, their acceptance in society or recover better from an accident. In therapies, robots were first used as physical platforms to help patient in rehabilitation therapy during the 80s~\citep{harwin1988robot}. During this period, robots were primarily used as mechatronic tools helping humans to accomplish repetitive task. But in the late 90s, robots started to be used for their social capabilities. For example the AuRoRA Project~\citep{dautenhahn1999robots} started in 1998 to explore the use of robots as therapeutic tools for children with \gls{asd}. Since AuRoRA, many other projects, such as the DREAM project\footnote{\url{https://www.dream2020.eu}}, have started all around the world to use robots to help patient with ASD~\citep{diehl2012clinical,esteban2017build}. 

\gls{rat} is not limited to \gls{asd} only, this use of robots is also explored in hospitals, for example to support children with diabetes~\citep{belpaeme2012multimodal}, to support elderly with dementia~\citep{wada2005psychological} and stroke recovering patients~\citep{mataric2007socially} or to monitor and provide encouragement in cardiac rehabilitation therapies~\citep{lara2017human}.	

These examples demonstrate that already today, robots are interacting with vulnerable populations (children, the elderly or patients in therapy). This implies that robots' social behaviours need to be constantly correct to ensure that no harm will be caused to these populations. And due to the shortage of workforce (e.g. nurses in the US; \citealt{nevidjon2001nursing}), these robots also need to be as autonomous as possible.

\subsubsection{Education} 

Robots are being used in education, supporting teachers to provide learning content to children and transforming the teaching process from passive learning to active learning~\citep{linder2001facilitating}. In education, social robots can take multiple roles such as peer, tutor or tool~\citep{mubin2013review}. It should be noted that Mubin et al. stress that the intention of the robotic community is not to replace teachers by robots, but provide them with a new form of technological teaching aid. Nevertheless, \cite{verner2016science} presented positive results from an early stage study using a tall humanoid robot to deliver a science lesson to children. The remaining of the section will describe more in details the roles of tutor and peer robots (the `tool' role has been excluded due its general lack of perceived gency and the lack social interaction between the robot and the students).

\paragraph{Robotics tutors providing tailored teaching content in 1 to 1 interaction.} 
Individualised feedback has been shown to increase the performance of students~\citep{cohen1982educational,bloom19842}. However, tutoring requires a larger trained staff and as such is more costly than large classes supervised by a single teacher. For this reason, tutoring is seldom used in public education today. Tutoring is however often available through private teacher at additional cost for the parents, potentially increasing social inequalities~\citep{bray2009confronting}. Robotic tutors could provide this powerful one to one tailored interaction to every student during school time, leading to higher learning gain for the children without dramatically increasing the workload on teachers or the price of education ~\citep{kanda2004interactive,leyzberg2012physical,kennedy2016social,gordon2016affective}. In addition to classroom uses, robots could also support children at home as they have been shown to elicit advantages over web or paper based instructions~\citep{han2005educational}. 

\paragraph{Peer robots learning alongside the children.}
Unlike other types of agents in education, peer robots have the opportunity to fit a special role seldom present in education today: the role of care-receiver rather than care-giver~\citep{tanaka2012children}. Peer learning has demonstrated benefits both for the helpers and those helped in \gls{hhi}~\citep{topping2005trends}. In \gls{hri}, a peer robot does not mentor a child to teach them new concepts, but learns alongside or from them, supporting them during the process and encouraging these children to produce behaviours improving their own learning. For example, in the Co-Writer project~\citep{hood2015children}, a child has to teach a robot how to write, and as the child demonstrates correct handwriting to the robot, they improve their own skills at drawing letters. Peer robots are able to leverage the concept of learning by teaching~\citep{frager1970learning} and peer learning~\citep{topping2005trends} in a way hardly matched by adults. The robot can take the role of a less knowledgeable agent with endless patience and encourage the student to perform repetitive tasks such as handwriting. In a similar position, an adult would not be a believable agent requiring learning and a younger student might not have the compliance and the patience to learn from another child.

To provide efficient tutoring or peer support, robots need to be able to personalise their behaviour to the humans they are interacting with in order to maximise the humans' learning gain~\citep{leyzberg2014personalizing}. Additionally, as pointed by \citet{kennedy2015robot}, a robot too social could decrease the children's learning compared to a less social robot if its behaviour is not congruent, consequently the robot's social behaviours have to be carefully managed to ensure its effectiveness. 

\subsubsection{Search and Rescue, and Military} 

Robots are already deployed in the real world, outside of labs and used during search and rescue missions~\citep{casper2003human,murphy2004human}. For instance, after a natural or artificial catastrophe, robots have been sent to analyse the damaged area and report or rescue the surviving victims of the incident. During these missions, robots have to interact socially with two kinds of human partners: the survivors and the rescue team. In both cases the social component of the interaction is key. The survivor is probably in a shocked state and the searching robot could be the first link they have with the external world after the accident~\citep{murphy2008search}. In this case, a social response is expected from the robot and it has to be carefully controlled. On the other side, the rescue team monitoring the robots is under high pressure to act quickly and faces traumatic events too. Even if the robot does not display a social behaviour, rescuers interacting with it might develop some feeling toward the robots they are using during these tense moments~\citep{fincannon2004evidence}.

Similar human behaviours (i.e. emotional bonding with a teleoperated robot) have also been observed in the army~\citep{singer2009wired}. Robots have been deployed as teleoperated drones and ground units alongside soldiers to complete scouting task or cleaning minefields. By interacting with a robot for extensive periods, some soldiers developed feelings toward this robot they used in a daily basis: taking pictures with it and introducing it to their friends. These relationships have gone as far as soldiers risking their own life to in order to save the robot used by their squad~\citep{singer2009wired}. 

These examples in these two fields demonstrate that even in interactions where the robot is not supposed to interact socially with humans, its users might consider it as a social agent. Consequently, the sociability of the robot has to be taken into account when interacting in such stressful environments. Overlooking the importance of social relationships human will form can lead to dramatic consequences (e.g. soldiers taking risks for the robot). As such, during the interaction, the robot's behaviour needs to always be appropriate not to create misleading expectations and to ensure that the goal of the interaction will be met.
	
\subsubsection{Hospitality, Home and Entertainment} 

Robots are also interacting with humans in hotels around the world; for example, the Relay robot (Savioke\footnote{\url{http://www.savioke.com/}}) delivers amenities directly to the guests' rooms in more than 70 hotels in the US, Europe and Japan\footnote{\url{https://www.spectrum.ieee.org/view-from-the-valley/robotics/industrial-robots/ces-2018-delivery-robots-are-fulltime-employees-at-a-las-vegas-hotel}}. Whilst the social interaction is still minimal today, these robots interact everyday with humans and have been seen evoking social reactions from them\footnote{\url{https://www.fastcodesign.com/3057075/how-savioke-labs-built-a-robot-personality-in-5-days}}. 

Since the late 90s, scientists explored how robots could guide visitors in museums and exhibitions~\citep{thrun1999minerva,burgard1999experiences}. These researches continue today to explore how to improve the social interaction between tourists and these guide robots~\citep{bennewitz2005towards}. In a similar context, researchers have designed and tested a robot as a receptionist in a hall at Carnegie Melon University~\citep{gockley2005designing}. Studies have also explored long term interactions and how humans perceive and interact with robots in shops and shopping malls~\citep{kanda2009affective} and how robots should behave with clients~\citep{kanda2008will}.

Finally, robots have already entered homes and family circles: from vacuum cleaners to companion passing by pet robots. A notable example is Aibo: twenty years ago, Sony created Aibo, a robotic dog to be used as pet in Japanese families and a new version was released in early 2018\footnote{\url{https://aibo.sony.jp/en/}}. An analysis of online discussions of owners published 6 years after Aibo's first introduction gives insights on the relationship that owners created with their robots~\citep{friedman2003hardware}. For example 42\% of the community members assigned intentionality to the robot, such as preferences, emotions or even feelings. Similar behaviours have also been observed when the robot was used not as a pet, but even just as a tool. For instance, \cite{fink2013living} reported that one of their participants was worried that their Roomba (a robotic vacuum cleaner) would feel lonely when they would be away on holiday. More recently, the Pepper robot has been sold as a social robotic companion to families in Japan\footnote{\url{https://www.softbankrobotics.com/emea/en/robots/pepper}}. However, as of early 2018, no study in English has reported results of the interactions with families or long term use and acceptance.

By entering our homes or hotel rooms, robots are penetrating some of our most intimate social spaces. These private spheres are ruled by a different set of social norms than public spheres such as streets or shopping mall~\citep{weintraub1997theory} and social faux-pas in these private environment can lead to an important loss of trust. As a consequence, robots' behaviours and policies needs to be especially appropriate and transparent when interacting in such private spaces. Additionally, robots in hospitality and entertainment will face a wide range of users' expectations, and will have to react to different unanticipated behaviours. Consequently, robots needs to be able to adapt their behaviours to these different users. Finally, robots in these fields will also interact with the same people over long periods~\citep{leite2013social}. And, to sustain engagement over such time scales, these robots need to change their behaviour over time to overcome possible boredom due to the vanishing of the novelty effect in repeated interactions~\citep{salter2004robots}.

\subsubsection{Collaborative Robots in Industry}

In industry, robots used to be locked behind cages to prevent humans to interact with them and getting hurt. However, recently social robots, such as Baxter~\citep{guizzo2012rethink}, have been designed to collaborate with humans; they share the same workspace and interact physically and socially with factory workers. 
However, to collaborate efficiently with humans, many challenges still need to be addressed. For example, the bidirectional communication between the robot and the humans needs to be as clear as possible. To interact efficiently and safely with humans, robots need to make their intentions clear to humans surrounding them~\citep{dragan2013legibility} and reciprocally, they also need to interpret humans' social cues to infer their goals and intentions~\citep{scheutz2007first}.

Beyond legibility and interpretation, another key challenge in \gls{hrc} is task assignment. If a goal has to be achieved by a human-robot team, the repartition of tasks should be carefully managed to  optimise the end result in term of task performance, but also to ensure comfort for the human. Explicit and implicit rules and personal preferences describe the expected role and behaviours of each participant and have to be taken into account in \gls{hrc}. As such, the task repartition system and other planners used in \gls{hrc} should be aware of these social norms and follow them~\citep{montreuil2007planning}. For example, recent work done in the 3\textsuperscript{rd} Hand project\footnote{\url{http://3rdhandrobot.eu/}} explored how a robot should support a human in a collaborative assembly task by adapting its behaviour to this human's personal preferences and how this adaptation improves the team's efficiency~\citep{munzer2017efficient}.

An last challenge related to the recent advances in \gls{ml}, and especially with the omnipresence of neural networks and deep learning, is \gls{xai}~\citep{wachter2017transparent}. As agents learning to interact will make mistakes and behave unexpectedly from time to time, they have to be able to provide explanations for these errors in a way understandable by humans. This challenge is especially visible in \gls{hrc} where both humans and robots aim to collaborate to complete a task together. \cite{hayes2017improving} propose to achieve transparency through policy explanation, by allowing robots to answer questions and explain their behaviour by self observation and logic deduction. This transparency aims at increasing trust between the agents involved in the interaction and improve the team's efficiency.

As demonstrated in \cite{munzer2017efficient}, intelligent systems involved in \gls{hrc} should adapt their behaviour to their interaction partners, be aware of preferences and rules to follow to ensure that the robot's behaviour is always appropriate, efficient and safe for the humans involved in the interaction. Furthermore, their autonomy needs to encompass behaviour explanation, be able to explain the reasoning steps and justify each of their actions.

\subsection{Requirements on Robots Interacting with Humans} \label{ssec:back_constraints}

The review in Section~\ref{ssec:back_hri} demonstrated that robots are already interacting with vulnerable populations: young children, the elderly, people requiring healthcare or in a stressful situations (victims of catastrophes or soldiers for example). Additionally, people tend to a create emotional bonding with these robots even if they are not interacting socially with their users. As such, the behaviour of robots interacting with humans need to be carefully controlled to manage humans' expectations and ensure their safety. In other words, robots need to constantly behave in socially acceptable manners, avoiding any confusing, inappropriate or dangerous behaviour. Failure to do so might prevent the interaction from fitting its goal, potentially elicit offence, anger, frustration, distress or boredom or even lead to physical injuries. 

These undesired behaviours may come from different origins: lack of sensory capabilities to identify necessary environmental features, lack of knowledge to interpret human behaviours appropriately, failure to convey intentions, impossibility to execute the required action or incorrect policies. Due to the wide range of origins of these potential social faux-pas, this research focuses only on the last point, obtaining an appropriate policy: assuming a set of inputs, finding a way to have the robot select an appropriate action. The other issues are either orthogonal and would lead to failure even with an `optimal' policy as external factors prevent the robot from solving the problem or could be handled by having a better policy (e.g. a policy generalising more efficiently or selecting suboptimal actions when the optimal ones are not available).

Appropriate actions are highly dependent on the interaction context: they could aim to match or reduce users' expectations of a robot's behaviour or complete a specific task. Additionally, robots have to follow social norms and actions correct in a certain context might be inappropriate in another one. Nonetheless, the intention behind being \textit{appropriate} here is that actions executed should be guaranteed not to present risks for the humans involved in the interaction (for example, preventing physical harm or mental distress), while helping the robot to move toward its goal and achieving its objectives.

Additionally, interactions with humans `in the wild'~\citep{belpaeme2012multimodal} do not happen in well defined environments or rigid laboratory setups. In the real world, robots have to interact in diverse environments, with a large number of different people, for extended periods of time or with initial incomplete or incorrect knowledge. As such, the policy also needs to be adaptable to the context and users as well as evolve over time. In summary, robots also need to be adaptive, to react to changing environments, cover a larger field of application and improve their policy over time.

Lastly, in many cases today, interactive robots are not autonomous but partially controlled by a human operator~\citep{riek2012wizard}. We argue that to have a real and useful \gls{hri}, the robot needs to be as autonomous as possible. 
Robots are expected to be used in areas where the workforce is already in shortage (e.g. healthcare) and requiring humans to control these robots decreases widely their applicability. As a community, \gls{hri} should strive toward more autonomy for robots interacting with humans.

Based on these considerations, we define three properties to evaluate how suited robot controllers are to interact with humans. Each axis is associated to a principle the robot has to follow to sustain meaningful social interactions:
\begin{enumerate}
   	\item Appropriateness of actions - The robot should only execute appropriate actions.
   	\item Adaptivity - The robot should be adaptivity to its environment and in time.
   	\item Autonomy - The robot should be as autonomous as possible.
\end{enumerate}
We will use these axes to analyse current robotic controller types in Section~\ref{sec:back_behaviour}.    

As \gls{hri} is a large field, other research axes are equally important, such as the complexity or the depth of the interaction, the constraints put on the environment, the ability of the robot to set its own goals, the dependence and knowledge of social rules or the range of application of a robot to cite only a few. However, we did not address these axes in the current work as they are more influenced by the goal and the context of the specific human-robot interaction taking place than the policy itself. Additionally, an appropriate, adaptive, and autonomous robot should be able to safely and autonomously learn to interact in deeper and more complex interactions and learn to extend its abilities beyond the ones it initially started with, and increase its range of applications.

\subsubsection{Appropriateness of Actions} \label{ssec:appropriateness} 

As argued previously, much of social human-robot interaction takes place in  stressful or sensitive environments, where humans have particular expectations about a robot's behaviour. Additionally, even in less critical situations, human-human interactions are subject to a large set of social norms and conventions resulting from precise expectations of the interacting partners~\citep{sherif1936psychology}. And some of these expectations are also transferred to interactions with robots~\citep{bartneck2004design}.

We define appropriate actions as actions correct in terms of both the task and social context, i.e. actions taking into account the social side of the interaction, and producing a correct robot behaviour at the right time. This behaviours needs to be safe for surrounding humans and help the robot to reach its goal. Failing to produce these appropriate actions, for example by not matching the users' expectations, may have a negative impact on the interaction, potentially compromising future interactions if the human feel disrespected, confused or annoyed. Furthermore, failing to behave appropriately can even harm the people interacting with the robot: not reminding an elderly to take their medication, not taking into account the state of mind of survivors after a disaster or behaving inconsistently with children with \gls{asd} might lead to dramatic consequences. Robots require a way to ensure that all the actions they execute do not present risks to the humans involved in the interaction while moving the robot closer to its goal.

For the review in Section~\ref{sec:back_behaviour}, the appropriateness of actions axis is a continuous spectrum characterising how much the system controlling the robot ensures that the robot constantly acts in a safe and useful way for the users. For example, a robot selecting its actions randomly would have a low appropriateness as no mechanism prevents the execution of unexpected or undesired actions. On the other hand, a robot continuously selecting the same action as a human expert would have a high appropriateness as domain experts would know which action is the correct one in this application domain.

\subsubsection{Adaptivity}	\label{ssec:adap}

Humans are complex, nondeterministic and unpredictable agents, as such an optimal robot behaviour is not likely to be known in advance or even programmable by hand~\citep{dautenhahn2004robots,argall2009survey}. Specifically, end users will express behaviours not anticipated by the designers, the interaction environment is often not perfectly definable and the desired behaviour might also need to be customisable by or for the end user or evolve over time. While many studies in \gls{hri} use robots following a static script, to interact meaningfully outside of lab settings or scientific studies, robots need this flexibility to extend their range of application and improve their interactions with users. In other words, robots interacting with people need to be able to adapt their policy to the environment and improve their behaviour over time. We use the term \emph{adaptivity} to represent this ability to express a behaviour suited to different conditions and refine it over time. 

We propose three components for this adaptivity. The basic one is the adaptivity to the environment, i.e. the generalisation of the behaviour (reacting accordingly to unseen inputs). The second one is personalisation and adaptation: being able to adapt a behaviour to the current user or context. Finally, the last component is the adaptivity in time which is, in essence, learning (the possibility to enrich and refine the policy over time). 
   
\paragraph{Generalisation:} 
Robots are interacting in human centred environments which are complex and highly stochastic. These environments are often under specified and robot designers cannot explicitly anticipate every single possible human reactions or occurring events. Furthermore, the state representations often use large vectors with multiple possibilities for each values. As such, predefining a specific robot reaction for each state possibility or each possible human behaviour is not feasible. Consequently, robots should have a policy able to generalise to unseen and unexpected situations and react appropriately to different environments.

\paragraph{Personalisation and adaptation:}
As robots are interacting with humans, they will encounter different type of environments, contexts of interactions and persons with different roles. For example a robot used as an assistant for elderly people will have to interact in the home of the owner, but also follow them in the street or in a supermarket. In these different interaction contexts, distinct behaviours will be expected from the robot. Similarly, different human beings might have distinct roles and the robot needs to adapt its policy to the type of person it is interacting with. For instance, in education, an autonomous robot would have to interact both with the students and the teacher, and its behaviour needs to take into account the role of the people it is interacting with. Additionally, the robot needs to personalise its behaviour to the person it interacts with: in entertainment or search and rescue, none of the users are known beforehand but providing a personalised behaviour adapted to the context may significantly impact the outcomes of the interaction. In summary, robots need to be able to adapt their actions policy to the environment and context they interact in and personalise their behaviour to the different users and their status.

\paragraph{Learning:} 
When deployed in the wild, robots will be expected to interact over extensive periods of time with the same user, e.g. companion robots for the elderly, military robots for a squad or robots used in \gls{rat}~\citep{leite2013social}. With these long-term social interactions, a key aspect in the engagement and efficiency is the co-adaptation between the user and the robot. Learning would allow the robot to tailor its behaviour to the current user and track the changes of preferences and desires occurring over long-term interactions. Additionally, providing a robot with the capacity to learn enables it to be used by non-experts in robotics, granting them a way to design their own human-robot interactions, and making use of their expertise and knowledge to have their robot interacting as they desire. This is crucial as many application of social robotics, such as \gls{rat}, happen in environment where non-technical people possess the domain expertise required to ensure that the robot is efficient. And, as stated by \cite{amershi2014power}, learning reduces the requirements of expensive and time consuming rounds-trips between domain-experts and engineers and additionally decreases the risks of confusion between these different communities. Finally, this adaptivity in time allow the robot to learn from its errors and improve its policy over time. Furthermore, this learning can enrich the robot's policy to allow it to tackle new tasks beyond its initial role, increasing its application and use.

In summary, for this review, adaptivity is a continuous scale ranging from no adaptivity at all (the robot has a linear script that it follows in all the interactions), to high adaptivity (the robot can generalise to unforeseen situations, dynamically changes its policy according to the context of interaction and its partners, learn new actions and tasks and improve its policy over time). 

\subsubsection{Autonomy}

Today, many experiments in \gls{hri} are conducted using a robot teleoperated by a human~\citep{riek2012wizard,baxter2016characterising}, and whilst having a human controlling the robot presents many advantages (e.g. the human provides the knowledge and the adaptivity required to interact efficiently and has sensing and reasoning capabilities not yet available for robots), multiple reasons push us away from this type of interaction~\citep{thill2012robot}. First, relying solely on teleoperation is not suited for deploying robots in the real world. Human control does not scale to interact for long periods of time or in many places. Robots are also expected to interact in fields already lacking workforce (e.g. healcare), so if robots need to be continuously controlled, the advantage of automation is highly reduced. Second, for research, using humans to control robots reduces the transferability of knowledge gain to the real world. The human-robot interaction tends to become ``a human-human interaction mediated by a `mechanical puppet' ''~\citep{baxter2016characterising}, which decrease the relevance of the robot as an agent and as such reduces the applicability of results to future interactions with fully autonomous robots. And finally, human control of a robot's actions introduces multiple biases in the robot's behaviour~\citep{howley2014effects}, and these biases from human operators will affect the robots' behaviour, decreasing the replicability of behaviours. For these reasons, among many, we argue that a robot used in social \gls{hri} should be as autonomous as possible. A limited human supervision could support the robot and be used to improve its behaviour, but the robot should not rely on humans for its action selection during the main parts of the interaction. 
	
To analyse the different robot controller's autonomy, we take inspiration from \citet{beer2014toward}. Beer et al. define three components of autonomy: sensory perception, analysis and action selection. As such, autonomy is organised following a spectrum of different levels from no autonomy at all: a human is totally controlling the robot (doing sensory perception, analysis and action selection) to a full autonomy: the robot senses and acts on its environment without relying on human inputs. Levels exist between these extremes where a human and a robot share perception, decision and/or action: for example the robot can request information from a supervisor or the supervisor can override the action or goal being executed~\citep{sheridan1978human}.

\subsubsection{Interdependence of Factors and Trade-Offs}

These three axes used for the review: appropriateness of action, adaptivity and autonomy are not independent. Especially, as a robot able to learn might be also able to improve its appropriateness of action and its autonomy as it refines its policy. This impact of adaptivity on the two other axes is fundamental to increase the robot's fields of application, performance and usability. However, while a learning robot could eventually reach an optimal, perfect and autonomous policy, the behaviour expressed by the robot in early stages of the learning, while the policy is not appropriate yet, is critical. Even during this learning phase, the robot's behaviour needs to be safe and useful for humans interacting with it. As such, adaptivity is a key element for a robot controller to improve and reach a correct and autonomous policy, but a mechanism must ensure that at every step of the interaction the robot's behaviour is appropriate regardless of the learning progress.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Current Robot Controllers in HRI} \label{sec:back_behaviour}

The previous section presented three axes to evaluate a robot controller: the appropriateness of actions, the range of adaptivity and the level of autonomy. Based on these three axes, this section presents and analyses high level categories of robot control currently used in \gls{hri} to define a robot behaviour. 
For each category, we will present the corresponding approach, indicate representative works done and qualitatively rate it on the three axes.

\subsection{Scripted Behaviour}

One of the simplest ways to have a robot interacting with a human is to have an explicit and fixed behaviour. In this case, the robot is fully autonomous and follows a script for action selection. Success in using this approach is dependent on having a well defined and predictable environment to have the interaction running smoothly. However, if the interaction modalities (possible range of behaviours and goals) are limited enough, a constant policy can be sufficient to handle all (sensible) human actions. 

This approach is followed in a large number of research in \gls{hri}: as many studies are human-centred, the focus is not in the complexity of the robot's behaviour but on how different humans would interact with and react to a robot displaying a behaviour with defined and controlled specificities. This also allows researchers to compare conditions with controlled differences and analyse the impact of small variations of behaviour. Whilst this has advantages when exploring people's reactions to robots, this method can hardly be used to deploy robots to interact with humans on a daily basis. Real world applications take place in undefined and open environments where potential human behaviours are almost infinite. Additionally, a fixed robot behaviour might also create boredom in users once the novelty effect vanishes~\citep{salter2004robots}.

By essence, this type of controller has no adaptivity as the robot is following a preprogrammed script, but is fully autonomous as no external human is required to control the robot; and when the application domain is highly specified, the behaviour can be mostly appropriate.

\subsection{Adaptive Preprogrammed Behaviour}

To go beyond a simple script, robots can also be programmed to react in predefined ways to expected human actions. By adaptive preprogrammed behaviour, we denote a behaviour programmed before the interaction, but explicitly (or implicitly) including ways to adjust the policy in reaction to anticipated human behaviours. This preprogrammed adaptation takes two forms: either having a fixed number of variables impacted by the actions of the partner and guiding the policy (for instance using homeostasis), or explicitly planning for specific behaviours to be produced if predefined conditions are met (for example using a finite state machine).

Homeostasis, the tendency to keep multiple elements at equilibrium, is constantly used by living systems to survive and is also a good example of the first case of preprogrammed adaptation used in social \gls{hri}. For instance, \citet{breazeal1998motivational} used a set of drives (social, stimulation, security and fatigue) which are represented by a variable each and have to be kept within a predefined range to represent a `healthy' situation. If these variables reach values outside the desired homeostatic range, the robot is either over or under-stimulated, this will affect the robot's emotional status and it will display an emotion accordingly. This behaviour have been shown to be enough to maintain human interacting with the robot for relatively long periods of time spanning more than ten minutes. Homeostasis approaches have also been extended to robotic pets~\citep{arkin2003ethological} or \gls{rat}~\citep{cao2017collaborative}.

On the other hand, a case of planned adaptation is clearly presented in \citet{leyzberg2014personalizing}. Participants have to play a cognitive game,  and a robot delivers predefined advises on strategies depending on the performance and the current lack of knowledge of the participant. With these anticipated human behaviours, the robot can provide personalised support as long as the participants behave within expectations. 

Similarly to other behaviour-based methods used in robotic control (such as the subsumption architecture; \citealt{brooks1986robust}), due to the indirect description of behaviours, homeostasis-based methods are more robust in unconstrained environments than a purely scripted controller, while remaining totally autonomous. However the policy is not adaptive in time and similarly, the fixed set of rules limits the adaptability to unexpected event. Furthermore, with this indirect description of actions, there is no guarantee against the robot acting in inconsistent way in specific cases which limits the appropriateness of actions. Similarly, planned adaptation provides adaptivity to the environment but only in highly limited cases expected by the designers. This limits the adaptability of such an approach as the robot does not learn; and, as the robot may face situations not expected by the designers, the maximum appropriateness of actions is also limited.

In summary, both predefined adaptation and homeostasis-based methods score highly in autonomy and can have a moderate to high level of appropriateness, but the adaptivity is low as they can only adapt to the environment within predefined, anticipated and limited boundaries and the robot does not learn.

\subsection{Wizard of Oz} \label{subsec:WoZ}

\acrfull{woz} is a specific case of teleoperation where the robot is not autonomous but at least partially controlled by an external human operator to create the illusion of autonomy in an interaction with a user. It outsources the difficulty of action selection and/or sensory interpretation to a human operator. This technique has emerged from the \gls{hci} field~\citep{kelley1983empirical} and is today common practice in \gls{hri}~\citep{riek2012wizard}. Similar to scripted behaviours, \gls{woz} is mostly used in human-centred studies to explore how humans react to robot and not as a realistic way to control robots in the wild. A second use of \gls{woz} is to safely gather data to develop a robot controller from human demonstrations (cf. Section~\ref{ssec:back_lfd}).

Even in \gls{woz}, part of the robot's behaviour is autonomous, and combining this robot autonomy and human control can be done in multiple ways. \cite{baxter2016characterising} define two levels of \gls{woz} related to the levels of autonomy presented by \cite{beer2014toward} and that correspond to the level of human involvement in the action selection process. Cognitive \gls{woz} aims to provide a robot with human-like cognition or deliberative capabilities; while in perceptual \gls{woz}, the human only replaces a sensory system and feeds information to the robot controller. Typically, perceptual \gls{woz} replaces challenging features of the controller required for a study, but not relevant to the research question. One of such typical challenge is \gls{nlp}. Despite all the progress made in speech recognition, \gls{nlp} is still a challenge in \gls{hri}, especially when interacting with children~\citep{kennedy2017child}. And as some studies require a limited speech recognition element to test an hypothesis, using a human for that part of the interaction allows to run the study without having to solve complex technical challenges (for instance, see \citealt{cakmak2010designing}).

This level of human control impacts the autonomy of a system: a robot relying on human only to do perception has a higher autonomy than a robot fully controlled by an operator. Controllers can also combine human control and predefined autonomous behaviour in mixed systems. For example, \citet{shiomi2008semi} propose a semi-autonomous informative robot being mainly autonomous, but with the ability to make explicit request to a human supervisor in predefined cases where the sensory input is not clear enough to make a decision. 

With \gls{woz}, the adaptivity and the appropriateness of actions are provided almost exclusively by the human, so these characteristics are dependent of the human expertise but are generally high. However, due to the reliance on human supervision to control the robot, the autonomy is low. For semi-autonomous robots, the picture is more complex: as explained by \cite{beer2014toward}, the initiative, the human's role and the quantity of information and control shared influence the level of autonomy. For example, in \citet{shiomi2008semi} the robot explicitly makes requests to the human, but the human cannot take the initiative to step in the interaction, thus limiting the adaptivity (especially as the robot policy is fixed). And as the human only has limited control over the robot's behaviour, no mechanism prevents the robot to make undesired decision. Overall, this would lead to a higher autonomy, but a lower appropriateness of actions and adaptivity compared to classical \gls{woz}.

\subsection{Learning from Demonstration} \label{ssec:back_lfd}

As stated by numerous researchers, explicitly defining a complex behaviour and manually implementing it on a robot can take a prohibitive amount of time or even could not be possible for complex behaviours~\citep{argall2009survey,billard2008robot,dautenhahn2004robots}. This statement applies equally well to manipulation tasks and social interaction. In both cases, humans have some knowledge or expertise that should be transferred to the robot. However in social robotics, experts in the application domain often do not have the technical knowledge to implement efficient behaviours on a robot, which results in numerous design iterations between the users and engineers to reach a consensus. 

The field of \gls{lfd} aims to tackle these two challenges: implementing behaviours too complex to be specified in term of code and empowering end-users with limited technical knowledge to transfer a policy to a robot. The learning process starts with a human demonstrations a correct behaviour~\citep{argall2009survey}, and then offline batch learning is applied to obtain a policy for the robot. Later, if required, reinforcement learning can complement the demonstrations to reach a successful policy~\citep{billard2008robot}.
In \gls{lfd}, the human-robot interaction is key, however in most of the cases this interaction is only in the learning interaction, the application interaction does not involve humans, but often manipulation or locomotion tasks such as grasping and moving an object, using a racket to hit a ball or throwing tasks~\citep{billard2008robot}.

However, two approaches have applied \gls{lfd} to teach robots a social policy to interact with humans.	The first one aims at learning directly from human-human interactions and replicate these human behaviour on a robot. For example, \citet{liu2014train} present a data driven approach taking demonstrations from human-human interactions to gather relevant features defining human social behaviours. Liu et al. recorded motion and speech from about 180 interactions in a simulated shopping scenario and then clustered these behaviours into high-level actions and implemented them on the robot. During the interaction, the robot uses a variable-length Markov model predictor to estimate the probability of a human executing each actions and finally winner-take-all is applied to select the most probable action. According to the authors, the final robot's behaviour was life like but not perfect. However, authors affirm that if this approach was scaled using a larger dataset gathered from more human-human interactions in the real world, the performance should improve and become closer to natural human behaviours.

In the second approach, the data is collected through a \gls{woz} setup and aims to learn to replicate the wizard's policy to reach an autonomous social behaviour. \cite{knox2014learning} coined this approach ``\gls{lfw}''. The method starts with a purely \gls{woz} control study to gather data, and then, a policy is derived by applying machine learning on the collected data. However, this original paper did not present a description of which algorithm could be used or how, and did not evaluate the approach, instead it only offered a reflection on the application of this idea. An implementation and evaluation is briefly discussed by the authors in \cite{knox2016learning}, but the lack of implementation details and results reduces the usability of the paper.

\gls{lfw} is widely used in \gls{hci} (especially dialogue management; \citealt{rieser2008learning}) and has also been implemented by other groups of researchers in robotics. For example, \citet{sequeira2016discovering} extended and tested this idea to a create a fully autonomous robot tutor. Their method is composed of a series of steps: 
\begin{enumerate}
   	\item Collect observations of a human teacher performing the task.
   	\item Define the different actions used by the teacher and the features used for the action selection.
   	\item Implement these actions and features on a robotic system.
   	\item Set up a restricted-perception \gls{woz} experiment where an operator uses only the identified features to select actions for the robot.
   	\item Combine machine learning applied on the data and hand-coded rules to create an autonomous robot controller.
   	\item Deploy the autonomous robot.
   	\item[(7.] If required, add offline refinement steps to fine tune the robot's behaviour.)
\end{enumerate}

Both \citet{knox2014learning} and \citet{sequeira2016discovering} stress the importance of using similar features for the Wizard of Oz control than the ones available to the robot during the autonomous part. Although this decreases the performance in the first interaction, it allows more accurate learning overall due to the similarity of inputs for the robot and the human controlling it.
    
\cite{clark2018deep} aimed to bypass these limitations by using a deep Q-network~\citep{mnih2015human} to learn an Applied Behaviour Analysis policy for \gls{rat}. They recorded videos, microphone inputs and actions selected in a \gls{woz} interaction with neurotypical participants to train the network with the raw inputs and the actions selected to obtain a controller able to deliver the therapeutic intervention. However, in their study, the autonomous robot required additional limited human input to inform the algorithm of the state of the therapy and only reached a behavioural intervention with an accuracy inferior to 70\%. This means that even with additional human input the robot would provide inconsistent feedback at some points in the interaction. However, this study only used a limited amount of data, and using more training examples should lead to better results.
 
\gls{lfd} methods are based on real interactions either between humans, or between humans and robots controlled by humans; and, with enough demonstrations the robot should be able to replicate a human policy, thus select appropriate actions. However, efficiency is limited by the type of inputs recorded, the capabilities of the learning algorithm and the quality of the demonstrations which limit the appropriateness of the policy (as seen in \citealt{clark2018deep}). Furthermore, after the learning phase, the robot's behaviour is  mostly static, without any additional learning provided. As such, while possessing a good generalisation capability, \gls{lfd} do not possess the adaptivity in time once the robot is deployed. \cite{sequeira2016discovering} propose to add offline learning steps, but online learning would allow for smoother transitions and improvements of the behaviours.
Finally, all these methods require the presence of humans in a first phase but the robots are fully autonomous later in the interaction, so the autonomy is low in the first phase and then high during the main part of the interaction.

\subsection{Planning} \label{ssec:planning}

An alternative way to interact in complex environments is to use planning~\citep{asada1986robot}. The robot has access to a set of actions with preconditions and postconditions and a defined goal it needs to reach. To achieve this goal state, it follows the three planning steps: sense, plan and act. The first step, \emph{sense}, consist on acquiring information about the current state of the environment. Then, based on the set of actions available and the goal, a \emph{plan} is created. This plan is a trajectory in the world, a succession of action and states which, according to the defined pre and postconditions, should lead to the goal. Finally, the last step is to \emph{act}, to execute the plan. The plan can be reevaluated at each time step or only if an encountered state differs from the expected one, in that case the robot updates its plan according to the new state of the environment and continues trying.

The efficiency of planning relies on having a precise and accurate set of pre and postconditions for each action. And as humans are complex and unpredictable, it is a serious challenge, if not impossible, to model them precisely. As such, planning have seen limited use for open social interactions with humans. However, due to the nature of planning, reaching a specific goal in a known environment, it has been applied successfully to \acrfull{hrc}. Additionally, by limiting the interaction to a joint task, \gls{hrc} also simplifies the modelling of the human: the interaction being more constrained and task-oriented, the human should limit its behaviour to a number of expected task-related actions. The Human Aware Task Planner~\citep{alili2009task} is an example of planning used to assign task between a robot and human in a \gls{hrc} scenario. One specificity of this planner is the ability to take into account predefined social rules (such as reducing human idle time) when creating a plan to allocate tasks to the human-robot team. Including these social norms in the plan construction is expected to improve the user experience and  maximise human compliance to the plan, which should lead to higher performance in the task.

A precise and correct model would ensure that the autonomously select the appropriate action whilst an incorrect one would lead to non appropriate behaviours. Similarly, the adaptivity depends on the model the robot has access to and whether it can update it in real time. However, in many cases when interacting with humans, the model is static, only covering the tasks the robot has to complete the different contexts and states it is expected to face and as such presents limited generalisation capabilities to unanticipated situations or non task-related human actions.

Planning has also been extended with learning, which then allows for more adaptive and personalised policies. This type of learning planner has been mostly used in manipulation and navigation to obtain better trajectories~\citep{jain2013learning,beetz2004rpllearn} but not exclusively. In \gls{hri}, \cite{munzer2017efficient} presented a planner adapting its decisions to human preferences in a \gls{hrc} scenario. With this approach, the robot estimates the risk of each actions and depending of the risk value will execute them, propose them (and waits for approval before executing an action), or wait for a human decision. Between repetitions of the task, the robot will update its planner to fit more precisely to the human preferences and improve its policy for the next iteration of the task. Munzer et al. adopted principles from \gls{lfd} to planning to improve quickly and efficiently the performance of the robot. However, while planning is well suited for strictly defined and mostly deterministic tasks, many social human-robot interactions cannot be totally specified symbolically with clear actions and outcomes and as such the application of planning to social \gls{hri} has been limited. Nevertheless, it does provide robots with an autonomous, partially adaptive and appropriate policy.
	
\subsection{Summary}

Table~\ref{tab:back_controller} presents a summary of the different approaches currently used in social \gls{hri} with their advantages and drawbacks for application in \gls{hri} and their evaluation on each of the three axes. The two most promising types of control are \gls{lfd} and planning, however, both of them have their drawbacks: \gls{lfd} is applied offline to create a monolithic controller with limited adaptivity after being deployed, and planning's reliance on a model of the world limits its application to open-ended social \gls{hri} in the wild.
	
\afterpage{%
	\clearpage
	\begin{landscape}
		\centering
		\bgroup
		\ra{1.2}
		\begin{tabular}{@{}>{\raggedright}m{.115\linewidth}>{\raggedright}>{\raggedright}m{.2\linewidth}>{\raggedright}m{.2\linewidth}>{\raggedright}m{.15\linewidth}>{\raggedright}m{.11\linewidth}>{\raggedright}m{.07\linewidth}>{\arraybackslash}m{.07\linewidth}@{}} \toprule
			Controller & Advantage & Drawbacks & Application in \gls{hri} & Appropriateness & Adaptivity & Autonomy \\ \midrule
			Fixed \linebreak preprogrammed\linebreak behaviour & Quick and easy to create \linebreak Clear specified and repeatable behaviour & Limited to highly constrained interactions & Human-centred studies in highly\linebreak constrained env. & \textcolor{red}{Low} & \textcolor{red}{Null}  & \textcolor{ForestGreen}{Maximal} \\[.5cm]
			Adaptive \linebreak preprogrammed\linebreak behaviour & Relatively simple to program \linebreak More robust and efficient than scripted behaviour & Only provide adaptability in\linebreak limited anticipated context & Human-centred\linebreak studies in constrained environments & \textcolor{Dandelion}{Medium} & \textcolor{red}{Low} & \textcolor{ForestGreen}{Maximal} \\ 
			Wizard of Oz & Use human expertise to select the best action & Require constant high\linebreak workload from human\linebreak Not scalable  & Human-centred\linebreak studies \linebreak Highly critical \gls{hri} & \textcolor{ForestGreen}{Maximal}  & \textcolor{ForestGreen}{Maximal} & \textcolor{red}{Null/Low}     \\ 
			Learning from Demonstration & Transfer knowledge from\linebreak human to robot in the real\linebreak application environment & Lack of learning once deployed & HRI case by case & \textcolor{ForestGreen}{High} & \textcolor{Dandelion}{Medium}     & \textcolor{ForestGreen}{High}     \\
			Planning & Complex behaviours and adaptable to variations in the environment & Human too complex to be clearly modelled \linebreak Limited application to social interaction & Complex defined environments \linebreak \acrshort{hrc} & \textcolor{Dandelion}{Medium} & \textcolor{ForestGreen}{High}       & \textcolor{ForestGreen}{Maximal} \\
			\bottomrule
		\end{tabular}
		\egroup
		\vspace{-.61\linewidth}
		\captionof{table}{Comparison of robot controllers in \gls{hri}}
		\label{tab:back_controller}
	\end{landscape}
	\clearpage
}

 Similarly to humans, an ideal robot controller would learn how to interact by interacting, by receiving feedback from the environment but also by being taught by humans. Robots have access to characteristics unique to artificial agents, and robotics should use them when designing robots that learn: endless patience, no risk of becoming tired and potential full control by another agent to learn faster from humans. However, while learning is important, an ideal robot behaviour should also ensure its actions are constantly appropriate. One way could be to use a human to control the robot in early stages of the learning, when the policy is not correct yet. This would ensure an initial appropriate policy in early stages. And in later stages, this robot could combine autonomous learning, and being taught by other agents.
  This supervised learning from interaction would be the approach with the most potential as this type of learning could validate the three requirements: appropriateness of actions, adaptivity and autonomy. By essence, this continuous online learning aims at providing open-ended adaptivity to the robot. Including a human with control over the robot's actions can also ensure that actions are appropriate. And finally, as the robot learns, accumulates datapoints and demonstrations from the teacher it improves its policy, reducing the reliance and workload on the human to reach high levels of autonomy while conserving the constant appropriateness of actions and the adaptivity. 

 This type of interaction is similar to \acrfull{iml}: learning from the interaction and using a human teacher to speed up the learning. As shown in the next section, researchers have explored how to teach agents interactively non-social policy~\citep{scheutz2017spoken,cakmak2010designing}; but as of early 2018, no controller exists in \gls{hri} applying \gls{iml} to the challenge of learning social interaction with humans in the real world.
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Interactive Machine Learning} \label{sec:back_iml}

In Section~\ref{ssec:adap}, we stated that to be adaptive enough, a robot should be able to learn. Consequently, robot controllers used in \gls{hri} should make use of \acrfull{ml}. 
\gls{ml} corresponds to a field of \gls{ai} aiming at providing artificial agents with learning capabilities to improve their behaviour and reach high level performances in a large range of tasks. \gls{ml} has two main trends referring to the synchronisation between the learning and the use of algorithm: offline and online learning.

In robotics, offline learning is a technique allowing the robot to change its policy over time by updating it outside of the interaction (such as Learning from the Wizard in Section~\ref{ssec:back_lfd}). Between or before the interactions, a learning algorithm is used on a dataset previously accumulated to create a new policy.

On the other hand, online learning (such as \gls{rl}; \citealt{sutton1998reinforcement}) learns during the interaction. Rather than single monolithic definitions or updates of the behaviour, this constant refinement of the agent's policy benefits from a high number of updates, allowing the robot to learn even during the first interaction with the environment and never stop improving its behaviour.

\acrfull{iml}, as coined by \cite{fails2003interactive}, is a type of online learning with two specificities:
\begin{itemize}
	\item Use of an end-user expert in the learning process.
	\item Learn by multitude of consecutive small updates of behaviour.
\end{itemize}
These two characteristics differ greatly from classical offline learning, such as deep learning~\citep{lecun2015deep} which uses costly monolithic learning steps without human influence to define a static behaviour. On the other hand, \gls{iml} is an iterative process where the behaviour is improved at each small step, and where the end-user can provide feedback on the learner's performance during all these iterations. \gls{iml} aims to learn faster, by continuously using the human expert to correct the errors made by the algorithm as they appear, provide additional useful information to the learner and improve the knowledge gained at each learning step.

\cite{amershi2014power} presents an introduction to \gls{iml} by reviewing the work done and presenting classical approaches and challenges faced when using humans to support machine learning.

\subsection{Goal}

The main goal behind \gls{iml} is to leverage the human knowledge during the learning process to speed it up, to extend the use of classifiers from static algorithms trained only once to evolving agents learning from humans and refining their policies over time. As explained in \cite{fails2003interactive}, classifiers would gain from using human knowledge to iterate quickly to reach a good solution and agents learning from the interaction would gain from using additional feedback from humans~\citep{thomaz2008teachable,knox2009interactively}. \gls{iml} aims to combine the advantages from both \gls{sl} and online learning and applies this new type of learner to classification or interaction tasks.

Furthermore, by allowing a human user to see the output of an algorithms and provide additional inputs, the learning has the potential to be faster and tailored to this human's desires. By using human expert knowledge and intuition, the system can achieve a good performance faster~\citep{thomaz2008teachable}. Additionally, a key advantage of \gls{iml} is also being able to empower end-users of robotic or learning systems. These users are often non-technical, but possess valuable knowledge about what the robot should do. \gls{iml} provides an opportunity to allow these users to design the behaviour of their robot, to teach it to behave the way they desire.

These human inputs take three forms: labels for specific datapoints (cf. active learning; Section~\ref{ssec:back_active}), feedback over actions (similarly to reward in \gls{rl}; cf. Sections~\ref{ssec:back_rl} and~\ref{ssec:back_feedback}) or demonstrations to reproduce (cf. \gls{lfd} -~\ref{ssec:back_ilfd}).

\subsection{Active Learning} \label{ssec:back_active}

Active learning is a form of teaching used in education aiming to increase students' achievement by giving them a more active role in the learning process~\citep{johnson1991active}. This approach has been transferred to machine learning, especially to classifiers, by allowing the learner to ask questions and query labels from an oracle for specific datapoints with high uncertainty~\citep{settles2012active}. The typical application case is when unlabelled data are plentiful, but labels can be limited in numbers or costly to obtain. As such, a trade-off arises between the performance of the classifier and the quantity of queries made by the algorithm. Often, the oracle would be a human annotator with the ability to provide a correct label to any datapoint, but their use should be minimised for reasons of cost, time or annoyance.

Using an oracle to provide the label of specific points with high uncertainty should highlight missing features in the current classifier resulting in improvements both in term of accuracy and learning speed. However, this specific relation between the learner and the human teacher raises new questions such as: 
\begin{itemize}
	\item Which points should be selected for the query?
	\item How often should the oracle be queried?
	\item Who controls the interaction? (i.e. who has the initiative to trigger a query - the agent or the oracle?)
\end{itemize}

Researchers have explored optimal strategies for dealing with this relation between the learner and the oracle. This research has been especially active in \gls{hri} with robots directly asking questions to human participants and exploring how the robot's queries could inform the teacher about the knowledge of the learner~\citep{chao2010transparent}. In a follow up study, \cite{cakmak2010designing} showed that most users preferred the robot to be proactive and involved in the learning process. On the other hand, they also wanted to be in control of the interaction, deciding when the robot could ask questions even if it lead to a higher workload for them. However, authors proposed that when teaching a complex task requiring a high workload on the teacher, the robot would probably be expected or should be encouraged to take a more proactive stance requesting samples to take over some workload from the teacher.

Active learning, being able to select a specific sample for labelling, can dramatically improve the performance of the learning algorithm~\citep{settles2012active}. However, when interacting in the world, the learner is not in control of which sample can be submitted to an oracle to obtain a label. Datapoints are provided by the interaction and are influenced by the learner's actions and the environment reactions. For agents learning during the interaction, the active learning approach working for classifiers is not applicable, so other methods have been applied such as \gls{rl}, learning from human feedback or \gls{lfd}.

\subsection{Reinforcement Learning} \label{ssec:back_rl}

The main framework to learn from interaction is \acrfull{rl}. \gls{rl} aims to solve the problem of finding the best policy (i.e. a policy maximising a notion of cumulated reward) by observing the environment reaction to the agent's action.

\subsubsection{Concept} 

	Young infants and adults learn by interacting with their environment, by producing actions and receiving a direct sensory motor feedback from their environment. By learning the impact of their actions, humans can learn how to achieve their goals. Similarly, the field of \gls{rl} aims to empower agents by making them learn by interacting, using results from trials and errors and potentially delayed rewards to reach an optimal policy~\citep{sutton1998reinforcement}. 

	Most of the \gls{rl} agents interact in a discretised version of the time, considering life as a sequence of states and actions. The simplest version of \gls{rl} is interacting in a finite \gls{mdp}, a discrete environment defined by the tuple $<(S, A, P_a(s,s'), R_a(s,s'), \gamma)>$~\citep{howard1960dynamic}, with:
	\begin{itemize}
		\item $S$: a finite set of states defining the agent and environment states.
		\item $A$: a finite set of actions available to the agent.
		\item $P_a(s,s')$: the probability of transition from state $s$ to $s'$ following action $a$.
		\item $R_a(s,s')$: the immediate reward following transition $s$ to $s'$ due to action $a$.
		\item $\gamma$: a discount factor applied to future rewards.
	\end{itemize}
	
	The goal of the \gls{rl} agent is to find the optimal policy $\pi_*$ (assigning an action from $A$ to each state in $S$) maximising the discounted sum of future rewards. The agent is not aware of all the parameters governing the environment, but only observes the transitions between states and the rewards provided by the environment and has to update its policy to maximise this cumulated reward. Different algorithms exist to reach this policy, but a challenge faced by most of them is to balance the exploration and the exploitation~\citep{sutton1998reinforcement}.
	
	\emph{Exploration} consists on trying out new actions to learn more about the environment and potentially gain knowledge to improve the policy; whilst \emph{exploitation} is the execution of the current best policy to maximise the current gain of rewards. RL algorithms have to balance these two objectives to reach an optimal policy. One way to deal with this trade-off is to start with high probability of exploration, to rapidly collect knowledge on the environment and then decrease this probability to settle on an efficient behaviour.
	
	The more complex the environment is, the longer the agent has to explore before converging to a good policy. Thus, using \gls{rl}, it is not uncommon to reach numbers such as millions of iterations before reaching an appropriate policy~\citep{sutton1998reinforcement}. And when the agent is exploring, its behaviour might seem erratic as the agent tries actions often randomly to observe how the environment is reacting.
	
	\subsubsection{Application to HRI}
	
	This approach presents many features relevant to \gls{hri}: it possesses the autonomy required for meaningful interactions with humans and provides the adaptivity desired for having a large impact. However, as explained in the previous section, traditional \gls{rl} has two main issues: requirement of exploration to gather knowledge about the environment and large number of iterations before reaching an efficient policy. Generally, \gls{rl} copes with these issues by having the agent interacting in a simulated world. This allows the agent to explore safely in an environment where its actions have limited impact on the real world (only time and energy) and where the speed of the interaction can be highly increased to gather the required datapoints in a reasonable amount of time. For example to solve heads-up limit poker~\citep{bowling2015heads}, an agent played two months while considering more than 24 trillions hands every second\footnote{4000 CPU considering 6 billions hands per second: \url{http://poker.srv.ualberta.ca/about}}. However, no simulator of human beings exists today which would be accurate enough to learn a policy applicable in the real world. Learning to interact with humans by interacting with them would have to take place in the physical world, with real humans, and this implies that these issues of time and random behaviours would have direct impacts. 
	
	To gather informations about the environment, the agent needs to explore, trying out random actions to learn how the environment responds to them and if the agent should repeat them later. However, when interacting with humans, executing random actions can have dramatic effect on the users, presenting risk of physical harm as robots are often stiff and strong or cause distress. This reliance on random exploration presents a clear violation of the first principle to interact with humans presented earlier (`Only execute appropriate actions').
	
	Even if random behaviours were acceptable, humans are complex creatures, not fully predictable, with personal preferences and desires. And as such, learning to interact with them from scratch would require large number of datapoints and as interactions with humans are slow (not many actions are executed per minute) the time required to reach an acceptable policy would be prohibitive. 
	
	Despite similar real-world constraints, \gls{rl} has been used in robotics~\citep{kober2013reinforcement}, but mostly applied to manipulation, locomotion or navigation tasks. For the reasons stated above, as of early 2018, \gls{rl} has never been used to fully autonomously learn rich social behaviours for \gls{hri}. 
	
	\subsubsection{Opportunities}  
	Despite the limitations presented in the previous section, changes can be made to \gls{rl} to increase its applicability to \gls{hri}. For example, combining \gls{rl} and \gls{iml} can ensure that the behaviour is appropriate to interactions with humans even in the learning phase.
	
	\cite{garcia2015comprehensive} insist on \textit{safe} \gls{rl}, ways to ensure that even in the early stages of the interaction, when the agent is still learning about the world, its policy still achieves a minimal level of acceptability. The authors present two ways to achieve this safety: either by using a mechanism to prevent the execution of non-safe actions or by providing the agent with enough initial knowledge to ensure that it is staying in a safe interaction zone. These two methods are not limited to \gls{rl} but are also applicable to other machine learning techniques to make them safer (for instance \gls{lfd}; \citealt{billard2008robot}). 
	
	The first method (preventing the agent to execute undesired actions) can be implemented by explicitly having `forbidden' actions in predefined states or by having a list of safe actions~\citep{alshiekh2017safe}. Using this method, the anticipated cases of errors can be prevented. However it seems unlikely that every case could be specified in advance, so such a method might not be sufficient for applying \gls{rl} to \gls{hri}. 
	
	The second method (providing enough initial knowledge) can be achieved by carefully engineering the features used by the algorithm or starting from a initial policy to build upon. For example, \cite{abbeel2004apprenticeship} proposed to use human demonstrations in a fashion similar to \gls{lfd} but to learn a reward function and an initial working policy. This method, Inverse Reinforcement Learning, has been applied successfully to teach a flying behaviour to a robotic helicopter. Once the initial policy and a reward function were learned from demonstrations, \gls{rl} was applied around the provided policy to explore and optimise the policy. That way, only small variations of the policy happened and only around the demonstrated one. These small variations ensured that policies leading to incorrect behaviours were negatively reinforced and avoided before creating issues (such as crashing in the case of the robotic helicopter). 
	
	Whilst being promising and having been applied for agents interacting in human environments (such as for personalised advertisement;  \citealt{theocharous2015personalized})	these approaches have not been used to learn social behaviours or to have robot interacting with humans.

\subsection{Human as a Source of Feedback on Actions} \label{ssec:back_feedback}

When an agent is learning in a \gls{rl} fashion and improves its behaviour by receiving rewards from the environment, an intuitive way to steer the agent's behaviour in the desired direction faster is to use human rewards. This approach is an adaptation of `shaping': tuning a animal's behaviour by providing rewards~\citep{bouton2007learning}. In \gls{ml}, using rewards from a human to bias and improve the learning presents multiple advantages which will be presented throughout this section. One notable advantage is the simplicity of the interface and its generalisability to any type of problem. As the teacher only needs a way to provide a scalar or a binary evaluation of an action to steer the learning, only a simple one-way interface is required.  However, this simplicity of interaction is associated with a limited efficiency and a complexity of interpretation: the issues of how to interpret human rewards and how to combine them with environmental ones if existent are an active research field today~\citep{knox2010combining}.

When used on their own, human rewards enable an agent to learn a policy even in the absence of any environmental rewards, which is specially interesting robotics as a clear reward function applicable to \gls{hri} or robotics in general can be complex to define. Early work in that field came from \cite{isbell2006cobot} who designed an agent to interact with a community in the LambdaMOO text based environment. Cobot, the agent, had a statistical graph of users and their relations and executed actions in the environment. Users of LambdaMOO could either reinforce positively or negatively Cobot's action by providing rewards. While the interaction between the agent and the users was limited, Isbell et al. presented the first agent to learn social interactions with humans in a complex and social online environment. 

While the goal of Cobot was to create an entity interacting with humans, \cite{knox2009interactively} explored how humans could actively teach an agent a policy in the absence of environmental rewards using TAMER (Training an Agent Manually via Evaluative Reinforcement). With this approach, the agent uses a supervised learner to model the human reward function and then takes the action that would receive the highest reward from the model. 

However, unlike environmental rewards, human rewards are subjective evaluations of an agent's behaviour. As such by knowing humans tendencies and intentions when providing rewards, an agent is able to obtain more information from these human rewards than by treating them the same way as environmental ones. Many researchers explored how to obtain more information from human reward. For instance, Advice~\citep{griffith2013policy} models how trustworthy the teacher is and as such how much importance the learner should give to their rewards. For example, rewards from inconsistent teachers will be reduced as the agent knows the source is not reliable. Alternatively, \cite{loftin2016learning} explored how to infer the strategy used by the teacher in the reward delivery. Similar behaviours from different teachers might have different meaning: not rewarding an action might reflect an implicit acknowledgement of the correctness of an action or the active refusal to provide a positive reward (indicating the incorrectness of an action). By modelling this intention, the real meaning of rewards can be inferred and used to further improve the learning. Another relevant feature explored by this community is the dependence in time of the human reward policy. While reward functions are generally constant in time with \gls{rl}, with humans they might vary according to the current performance of the agent. For example, a suboptimal policy could receive positive feedback early on, when it compares positively to the average behaviour; while receiving negative feedback later on, when the average agent's performance is better. \cite{macglashan2017interactive} proposed COACH (Convergent Actor-Critic by Humans) to model how humans adapt their rewarding scheme in function of the agent's performance and deal with this non-stationary reward function. Similarly to other factors biasing human rewarding strategies, this dependence of the reward function to the current agent's policy should be taken into account to maximise the knowledge gained from human rewards.

Even when environmental rewards are present, human rewards still have opportunities to improve the learning: they can enrich a sparse reward function, guide the robot faster to an optimal policy or correct incomplete or incorrect environmental rewards. \cite{knox2010combining} explore nine different ways to combine these two types of rewards and each methods' impacts on the learning. From this analysis, they explain how to select an approach according to the specificities of the environment and the reward function.

Teachers can also use rewards to communicate other information to the learner. For example, \cite{thomaz2008teachable} aimed to explore how humans would use feedback to teach a robot how to solve a task in a virtual environment. They used \acrfull{irl} as a way to directly combine environmental rewards and human ones. However, during early studies, Thomaz and Breazeal discovered that participants tried to use rewards to convey intentions, informing the robot which part of the environment it should interact with. The next study involved two communication channels, a reward one to provide feedback on the actions and a guidance channel to provide information about the action the robot should execute. This guidance has been actively decided to be ambiguous; participants could not explicitly control the robot, but just bias the exploration, and adding this second channel improved the performance of participants. This study presented a first attempt to combine environmental rewards, human ones and human guidance to teach an agent a policy and demonstrated the importance of giving additional ways for the teacher to impact the robot's behaviour.

While not being applied to robotics but mostly to learning non-social interactions, these implementations of \gls{iml} provide important research describing how robots could be taught to interact with humans. These human rewards are especially interesting when the environmental reward function is sparsely defined or non-existent, providing a way to teach robots in any environments. However, humans do not simply evaluate an agent's actions, they adopt teaching strategies influencing their way of rewarding and want to provide guidance, hints or commands to help the agent to learn better and faster. In summary, human teachers desire to go beyond simply evaluating what the agent is doing, they want to provide advices or commands about how it should behave~\citep{amershi2014power}.

\subsection{Interactive Learning from Demonstration} \label{ssec:back_ilfd}

As presented in \cite{argall2009survey} and \cite{billard2008robot}, \gls{lfd} is majoritarily used in an offline learning fashion to learn a defined task without extending the policy once the task is considered mastered. However, tasks such as social interaction are complex even for humans and probably will never be fully mastered for robots. As such (and as argued before), robots would highly profit from learning throughout all their life, not only once before being deployed, but learning new tasks and improving their skills as often as required~\citep{dautenhahn2004robots}.

With \gls{ilfd}, an agent receives demonstrations not only once, but as often as required after being deployed. \gls{ilfd} is related to Mixed Initiative Control~\citep{adams2004mixed} where an agent and a human share control on the agent's actions. The robot acts mostly autonomously, but in some cases (at the initiative of the human or the robot), the human takes over the robot control and make a demonstration that will be used by the robot to refine its policy for the future.

One approach giving teachers a total initiative on the interaction is Dogged Learning (DL)~\citep{grollman2007dogged}. With DL, an agent is autonomously interacting and a teacher has the power to override the agent behaviour at any time by selecting desired actions or outputs. Facing a potential difference between the algorithm's outputs and the teacher's ones, the robot executes the commands with highest confidence (often the human's one) and the learning component aims at reproducing the executed output. If the teacher does not provide any commands, the ones from the algorithm are used. DL does not provide the robot with the opportunity to request a demonstration, but instead, the robot can communicate its uncertainty to the teacher, indirectly asking for demonstrations. 

Alternatively, \cite{chernova2009interactive} propose a method with a more complex interaction between the learner and the teacher. The Confidence Based Algorithm (CBA) is composed of two components: the Confident Execution (CE) and the Corrective Demonstration (CD). The CE enables the agent to act autonomously when its confidence in its policy is high and on the other hand to actively request a demonstration when the confidence is low. The CD allows the teacher to provide a corrective demonstration when the agent executes an incorrect action, which provide more information to the agent than a classic negative reward. These two components aim to leverage the complementary capabilities of the learner and the teacher. CBA has demonstrated efficient teaching in diverse scenario such as simple driving simulators or other classification tasks. But the effectiveness of this approach is bounded by the capacity of the learner to estimate this confidence to be able to request demonstrations and prevent incorrect behaviour to be executed. Another limit of such an approach is the impossibility for the teacher to correct undesired actions before they negatively impact world. 

Both methods rely on the teacher being able to anticipate the robot's behaviour to provide demonstrations before an incorrect action is executed or before it impacts the agent and its environment. As such, the appropriateness of the robot controller is not at maximum as the teacher cannot ensure that no incorrect action will be executed during the learning, only that the robot would learn faster from its errors.

\subsection{Importance of Control}

Results from active learning, research using human to provide feedback and \gls{lfd} have shown that human teachers take an active stance during the training of an agent and want multiple ways to influence the learner's behaviour~\citep{amershi2014power}. Humans are not oracles, enjoying providing labels and evaluating an agent's actions, they desire to be in control of the learning and provide richer information to the agent. \cite{kaochar2011towards} have shown that when given choice between different teaching methods, humans will never choose to limit themselves to use only feedback, but they want to teach using more modalities.

In addition to improving the teacher's experience in the teaching process, providing the humans with more control improves the learning~\citep{thomaz2008teachable,chernova2009interactive}. By allowing the teacher to demonstrate a policy online, bias the action selection and preempt or correct undesired actions, the learner interacts mostly in useful states of the environment and with a correct policy. This lead to faster learning and would improving the robot's performance highly in early stages of the learning. Another fundamental feature added by this human control over the robot's actions is safety. If a domain expert can prevent a robot interacting with humans to make errors and can ensure that all its actions are efficient, it would increase greatly the quality of the interaction for the humans involved. This will further improve the applicability and use of the robot and would satisfy the two first principles: appropriateness of actions and adaptivity of the robot.

However providing the teacher with this control presents challenges for designing the interaction between the robot and its teacher. Unlike a simple scalar reward, being able to control the robot requires the teacher to be able to give commands or advice to the robot and to receive additional information about the learner beyond its observable behaviour. This enriched two-way communication might be complex to design, especially when the action space is bigger than a few actions or the learning mechanism not transparent. In addition to the communication interface, the time scales of the interaction are also key: to give the opportunity to the teacher to preempt undesired actions, the learner needs to communicate its intentions in a timely manner to the teacher which complexifies the relation between the learner and the teacher. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Summary}

This chapter presented first an overview of the fields of \gls{hri} where robots interact socially with humans. From these cases of application, we defined three principles a robot controller should follow. To interact efficiently with humans, the robot should:
\begin{enumerate}
   	\item Only execute appropriate actions.
   	\item Have a high level of adaptivity and learn.
   	\item Have a high level of autonomy.
\end{enumerate}

Secondly, a review of current controllers for robots in \gls{hri} reported that no approach applied today in the field validates these principles. The review was extended to more general methods in \gls{ml} with potential to satisfy these principles. \gls{iml} shows promises for enabling a robot to learn online how to interact with humans, especially when the teacher is given control over the robot's behaviour and can demonstrate a correct policy. However while humans have been used to teach robots behaviours or concepts, teaching them to interact with human in an interactive, online fashion has not been demonstrated in the field so far.