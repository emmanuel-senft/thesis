\chapter{Discussion} \label{chap:discussion}
\glsresetall

Chapter~\ref{chap:background} highlighted the lack of robot controllers which provide adaptivity to a robot while at the same time offering a reduced workload for the people controlling the robot and ensuring that the robot's behaviour is timely and appropriate. Based on this observation, Chapter~\ref{chap:sparc} presented the \gls{sparc}, an interactive teaching framework designed to allow robots to learn socially interactive behaviour by being supervised by humans. Then Chapters~\ref{chap:woz},~\ref{chap:control} and~\ref{chap:tutoring} evaluated this approach in three studies, the last of which evaluated \gls{sparc} in real \gls{hri} consisting of a learning activity involving 75 children. Combined together, these chapters sought support for the thesis of this research:

\begin{quote}
	\thesis.
\end{quote}

Chapter~\ref{chap:woz} presented a first a study comparing \gls{woz} and \gls{sparc}. Results from this study demonstrated that a learning robot could reduce the human workload required to have it interact in the world without impacting its performance in the application interaction. This provide support for \gls{sparc} as a method allowing a robot to become progressively autonomous. 

The second study presented in Chapter~\ref{chap:control} explored how the control provided to the teacher by \gls{sparc} impacted the teaching of an efficient policy. This study showed that by giving the human teacher control over the robot's actions, \gls{sparc} could ensure that the executed robot behaviour fits the teacher's desires which led to a faster, safer and easier teaching. 

As the first and second studies were focused on the relation between the teacher and the robot, they had to use a repeatable and controlled environment to study \gls{sparc}. In contrast, the last study applied \gls{sparc} to a real-world \gls{hri}: child tutoring. In that study, an adult had to teach a robot to tutor children in an educational game. Results demonstrated that after having been taught using \gls{sparc} in multidimensional and generic state and action spaces, an autonomous robot displayed an efficient social policy suited to interacting with children. The policies from the supervised and the autonomous robots presented similarities, and both policies resulted in more positive children behaviours compared to a passive robot. Whilst not reducing the workload on the human during the teaching process, \gls{sparc} demonstrated its applicability to complex, multimodal and high-stakes environments. During the teaching process, the teacher could ensure a useful robot policy, which was maintained even after the teacher exited the control loop.

This chapter provides an overarching perspective on the results presented in earlier chapters. It presents how the three studies in this work answer the research questions posed in Chapter~\ref{chap:intro}. Then, Section~\ref{sec:disc_limitations} presents the limitations of the approach proposed in this work, \gls{sparc}. Section~\ref{sec:disc_impact} discusses the more general impacts this research may have and the ethical questions raised by teaching robots to interact with humans.
Finally, the last section proposes axes where \gls{sparc} could be extended, to on the one hand learn more about how people can teach robots, and on the other hand improve \gls{sparc}'s usability and application to \gls{hri} and other fields.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Research Questions} \label{sec:disc_rq}

This section will revisit the research questions identified in Section~\ref{sec:intro_thesis} and explain how the work presented in this thesis addressed them.

\begin{itemize}
	\item [RQ1] \textbf{What are the requirements of a robot controller for social \gls{hri}?} 
	Based on a review of the different fields of application of social \gls{hri}, we defined three requirements a robot controller should meet to ensure an efficient interaction. First and foremost, the robot's behaviour needs to be constantly appropriate: as robots often interact with vulnerable populations, their behaviour needs to be safe for the people they interact with and appropriate to achieve a desired interaction outcome. Secondly, the robot should be adaptive, i.e. it should be able to generalise to unexpected situations, but should also personalise its behaviour to the different users it interacts with, and be able to learn, improving and extending its policy. Thirdly, the robot needs to be as autonomous as possible, or at least impose a low workload on its supervisor.
	
	This implies that the robot needs to find ways to learn about its environment and reach autonomy without relying on random exploration as this would violate the first principle. We think the robotic community, and especially \gls{hri}, should strive toward more autonomous robots, and could take a stronger inspiration from \gls{iml} as it shows strong promises for teaching robots and could enable them to learn complex tasks such as interacting with people.
	
	\item [RQ2] \textbf{What interaction framework would allow a human to teach a robot while meeting the requirements from RQ1?}
	To meet the three requirements expressed as answer to RQ1, we proposed \gls{sparc}, a new teaching framework for robots which provides control over the robot's actions to a teacher and use this control to learn in a safe way, validating the first and second requirements. Secondly, by allowing the teacher to passively accept the robot's propositions, we aim to decrease the workload on the teacher over time and progressively provide the robot with autonomy. By taking inspiration from \gls{iml}, \gls{sparc} aims to fill a void in the \gls{hri} research: online learning for interaction with humans.
	
	\item [RQ3] \textbf{Could a robot decrease its supervisor's workload by proposing actions based on observing the supervisor's earlier decisions?}
	Study 1 showed that providing a supervised robot with learning can reduce the workload on its supervisor. Furthermore, study 2 demonstrated that, compared to the literature, \gls{sparc} is an efficient way to enable safe teaching and requires a comparatively low workload. Similarly to methods from \gls{lfd}~\citep{liu2014train,sequeira2016discovering}, \gls{sparc} provides an alternative to \gls{woz} and opens new opportunities to have robots learn social behaviours from observing humans control. However, unlike classic \gls{lfd} methods, \gls{sparc} could produce results supporting the teacher during the learning process, informing them about the robot's knowledge and potentially creating trust between the robot and its teacher, qualities still lacking in most other learning frameworks.
	
	\item [RQ4] \textbf{When teaching a robot, how does the control over the robot's actions by the teacher impact on the teaching process in terms of performance, effort and teaching time?} 
	Results from study 1 and 2 indicated that by informing the teacher in advance of its actions, the robot ensures that its final behaviour is vetted by the teacher. This implies that even in early phases of the learning, when the robot behaviour is not adequate yet, the teacher can prevent the robot's lack of knowledge to negatively impact the world. Furthermore, this control helps the teacher to steer the robot toward useful parts of the environment and to demonstrate a better policy, making the teaching faster, safer, more efficient and lighter (as requiring a lower workload) compared to methods which do not use human supervision while learning. This finding is especially relevant to \gls{iml}, as often human teachers are offered limited control over the robot's policy~\citep{thomaz2008teachable,knox2009interactively}. While being a challenge, providing the teacher with this control can have significant positive results on the learning progress.
\end{itemize}

Research questions 5 and 6 apply to the special case when \gls{sparc} is used to teach a robot to interact with people. As such, the resulting interaction is as proposed in Figure~\ref{fig:concept}: a triadic interaction between a human-target, a robot and a human teacher.

\begin{itemize}
	\item [RQ5] \textbf{What impact does \gls{sparc} have on the performance of the child-robot interaction, the comfort for the teacher and the robot's learning?}
	When being used to teach a robot to interact with a person, \gls{sparc} does allow the robot to display an effective behaviour (as demonstrated by the improvement of children's engagement with the educational task in the supervised condition in Chapter~\ref{chap:tutoring}). However, this performance in the application interaction might come at a cost for the teaching interaction. As the teacher needs to monitor the human in the application interaction, they also have to react to the robot's suggestions, this dual task might lead to a heavier workload than classic tele-operation methods. This is one of the drawbacks of \gls{sparc} compared to methods based on \gls{lfd} to gather information, but as mentioned earlier, ways exist to mitigate this issue and allow this interaction between the teacher and the robot to lead to positive results for the teacher.
	
	\item [RQ6] \textbf{After having been taught using \gls{sparc}, could a robot behave autonomously in a social context?}
	In Chapter~\ref{chap:tutoring}, we used \gls{sparc} to teach the robot and then deployed the robot to interact autonomously. During this autonomous interaction, the robot applied a policy similar to the demonstrated one, and the impact on the children's behaviour was close to the one in the supervised condition. Consequently, in this study, the robot managed to behave socially in an autonomous fashion in a complex and multimodal environment after having been supervised by a person in a learning phase. This finding is one of the most important of the thesis as it demonstrates the potential of \gls{sparc} to teach robots complex social autonomous behaviours.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Limitations of SPARC} \label{sec:disc_limitations}

This section discusses the limitation of \gls{sparc}, specifically to the range of domains it could be applied to.

\subsection{Requirement of a Human in the Loop}

The first of these limitations is the potential requirement for a human to supervise the robot, even after it has learned a policy. As stated earlier, \gls{sparc} aims to move away from \gls{woz} or other tele-operation methods by learning an efficient policy from initial human supervision. In its original framing, \gls{sparc} did not aim to create a fully autonomous agent acting without supervision, but instead gradually learned from monitoring commands given to the robot, taking over from the supervisor as the interaction progresses. As such, the learning is not split into two distinct exploration and exploitation phases, but is a step in which the robot transitions smoothly from having no knowledge to having incorporated knowledge from observing the teacher's behaviour. Because of this, the workload on the supervisor would decrease as the robot learns, while a high performance in the domain application would be maintained due to the oversight provided to the teacher. However, \gls{sa} still requires a human in the loop and as such presents limited applicability to fields where robots are expected to be fully autonomous, for example to substitute human labour. 

Despite not being the original goal, \gls{sparc} can still be used to create a fully autonomous robot (as demonstrated in Chapter~\ref{chap:tutoring}). In this case, the training process would be similar to \gls{lfd}, with a training phase using \gls{sparc} and then the exploitation/deployment phase of the autonomous behaviour. However, with \gls{sparc}, differences remain. First, during the training phase, instead of passively receiving commands from the teacher, the robot would proactively make suggestions to the teacher. As presented in Section~\ref{sec:tuto_disc_teaching} this aims to reduce the workload on the teacher during the training phase, provide more datapoints for the learning and inform the teacher about the state of robot's knowledge, potentially creating trust between the robot and its teacher. Secondly, even after being deployed to interact autonomously, the teacher could still take back control using \gls{sparc}, thereby refining the policy. Alternatively, if a different behaviour has to be applied (e.g. if the robot interacts with a child with special needs rather than a typical one), the teacher can take over using \gls{sa} to ensure a personalised experience for this specific interaction.

\subsection{Reliance on Human's Attention}

A second limitation of \gls{sparc} lies in the constant need for the human's attention and the presupposition that human teachers will always ensure  appropriate robot behaviour if given the opportunity. Throughout this research, we assumed that even if the robot behaviour may be mostly correct, the supervisor would be attentive to the robot suggestions and ready to correct any error at any time. This assumption is similar to autonomous cars using a safety driver or Tesla Autopilot requiring continuous human supervision\footnote{``Every driver is responsible for remaining alert and active when using Autopilot, and must be prepared to take action at any time.'' \url{https://www.tesla.com/en_GB/autopilot}}. The agent is fully autonomous but may make mistakes and as such, a person needs to be ready to correct these errors before they impact the world. However, as demonstrated by the accidents in 2017 and early 2018 involving these supervised autonomous vehicles, this assumption is often violated, and a short moment of inattention may have dire consequences\footnote{Cf. the Tempe and Mountain view accidents reported by the media in early 2018.\label{foot:disc_danger}}. By observing a seemingly correct agent behaviour for an extended period of time, the human supervisor might start to overtrust the agent, missing the occasion to react in time to anticipable errors potentially leading to frustration or death in the case of autonomous vehicles\footref{foot:disc_danger}. Nevertheless, some ways exist to mitigate this limitation but have not been applied to autonomous driving or general \gls{iml}. For example, with \gls{sa} the agent in advance informs the supervisor about its actions, similarly a car could display the planned trajectory on a screen or in augmented reality. This would provide the supervisor with more time to analyse the situation potentially allowing them to react in a timely fashion. Alternatively, the agent could communicate a lack of confidence in its actions or its interpretation of the environment, informing the supervisor that attention is specially required in that moment.

\subsection{Time Pressure}

As pointed out in Section~\ref{ssec:sparc_time}, with \gls{sparc}, the presence of the \gls{cw} and the auto-execution of actions may lead to issues. The length of the \gls{cw} is a design decision and depends of the application, to be applicable and make use of the auto-execution of actions as a way to reduce workload, the \gls{vw} of an action needs to be wider than its \gls{cw}. That way, actions approved passively are still valid when executed. To increase the application of \gls{sparc} to a wider range of situations, the \gls{cw} has to be as narrow as possible. In contrast, the supervisor needs a \gls{cw} as wide as possible, to provide them with enough time to process the action and cancel it if required. Correction windows too narrow would put additional pressure on the supervisor to react in time or even prevent them to avert undesired actions. 
This results in two effects having opposite requirements on the \gls{cw}. However, while a longer \gls{cw} would only limit \gls{sparc}'s applicability in some situations, one too short could have negative consequences. As such, this need of a \gls{cw} wide enough to allow the teacher to react is probably one of the main limits of \gls{sparc} as it produces a significant delay in the robot's actions and reduces the range of domains \gls{sparc} can be applied to.

However, this requirement of a \gls{cw} wide enough for the teacher can be mitigated in multiple ways. For example, instead of communicating the action the robot is directly about to do, the robot could communicate a plan with multiple steps announced in advance. That way, the teacher would be informed beforehand of the next few steps and could anticipate their impact and react to any future actions instead of limiting their evaluation to the next one. Alternatively, the robot could adapt the length of the \gls{cw} to its confidence in the proposed action; for instance, an action with a low confidence would be given more time to be corrected. Likewise, each type of action could have a dedicated value for the \gls{cw}, for example actions needing to be executed quickly (such as emergency breaking for example) would have a much shorter \gls{cw} than other actions with less time constraints. Finally, a last possibility could be to allow the teacher to manually select the duration of this \gls{cw}, consequently letting them be in control of the pace of the interaction. In that case, the teacher might prefer to start with a long \gls{cw} and make a limited use of the auto-execution in the early phases of the interaction to be able to focus on the teaching process; but in later stage of the interaction, they might reduce this \gls{cw} to profit from the auto-execution of actions and reduce their workload.

\subsection{Overloading the Teacher}

By giving an active role to the robot in the teaching process (through proposing actions), \gls{sparc} requires  the teacher to simultaneously monitor two autonomous agents: the target user and the robot. This requirement might lead to another risk with \gls{sparc}: overloading the teacher, rather than reducing his workload. If the teacher has to correct more actions than they would have selected, their workload would not be reduced but increased. While still providing useful information for the learning algorithm (and more than only the actions selected by the supervisor), this supplementary workload on the teacher is not desired. As explained in Chapter~\ref{chap:tutoring}, this could lead to erroneous teacher's behaviours hindering the learning and potentially increasing the risks in the interaction. For example at some points in the study presented in Chapter~\ref{chap:tutoring}, the teacher just cancelled actions as soon as they arrived, even before she had time to evaluate them. While reducing the workload on the teacher by not requiring them to evaluate the proposed action, this behaviour might limit the efficiency of the learning algorithm by giving it incorrectly labelled datapoints. 

Overloading the teacher is a serious issue and might have negative consequences both for the robot's learning and for the experience of the user involved in the application interaction. As such, mechanisms must be present to ensure that this does not happen. The learning algorithm needs to have the right balance between suggestions and waiting periods to allow the teacher to assess and provide a correct evaluation of the proposed actions if needed. Alternatively, the teacher could be provided with a direct way to impact on the rate of suggestions and on the time before the auto-execution of actions. This would give them control over their workload and might allow them to teach the robot more efficiently.

\subsection{Interface}\label{sec:disc_lim_interface}

The interface between the teacher and the robot is key when applying \gls{sparc} or other \gls{iml} methods. Simple interfaces can be easy to create and are easy to use by the teachers, however they might only have limited efficiency in the learning process. For instance, approaches using only feedback (i.e. numeric evaluation) require a single one-way communication channel between the teacher and the robot, and this channel only needs to send a scalar evaluation of the agent's actions. Consequently, both the design of the interface for the teacher and the communication are simple but their efficiency is limited. On the other hand, to provide full control and accountability over the robot's actions, \gls{sparc} requires two-way communication. Firstly, the teacher needs to receive input from the robot, such as its intentions, to decide if the action is valid or not. Secondly, the teacher needs to send information to the robot: feedback about the intentions, cancelling and correcting actions if required, but also demonstrating by selecting actions for the robot to execute. As the robot may have access to hundreds of actions, assigning one button per action is not feasible, other ways of controlling the robot need to be found. Furthermore, as mentioned in Chapter~\ref{chap:sparc}, with \gls{sparc} the teacher can also provide additional information to the algorithm to speed up the learning. In summary, the interface between the robot and the teacher needs to provide the teacher with the robot's intentions, and allow the teacher to preempt proposed actions, select any action and provide additional information to the learning algorithm. An interface providing all these features can easily be overwhelming and difficult to use by naive users, increasing even more the required workload to control and teach the robot. As such, for applying \gls{sparc} to complex environments, an investment will need to be made in the interface to make it intuitive and clear. 

For instance, in Chapter~\ref{chap:tutoring}, the \gls{gui} used by the teacher represents the current state of the game, with some buttons for accessing a subpart of the action space, but the majority of actions were inferred by the way the teacher moved items on the screen and how they selected items. An alternative way could be to use natural language. Natural language is an intuitive channel for humans and its open-endedness makes it suited for applications of \gls{sparc} where the teacher can speak and where the time required to vocalise commands is not critical, such as a robot assistant at home.

%\subsection{Learning with Humans}
%
%\gls{sparc} has been designed to enable agents to learn safely \textit{in situ} for high-stakes environments where no simulator can be used to learn in a virtual world (e.g. \gls{hri}). However, learning \textit{in situ} for interaction with humans adds a serious time constraint: by including a person in the learning interaction, this interaction needs to go at a human pace. This entails that gathering datapoints through \gls{sparc} is a relatively slow process. 
%Consequently, recent algorithms relying on large datasets (or ease to collect large amount of data)\footnote{For instance ImageNet contains more than 14 millions of images~\citep{russakovsky2015imagenet} and \gls{rl} methods still rely on millions or more of datapoints (cf. Section~\ref{ssec:back_rl})} have limited applicability for learning online to interact in the real world. 
%As such, algorithms using \gls{sparc} need to be data efficient, be able to learn from a low number of datapoint and generalise quickly. However, as mentioned previously, by including the human in the loop and learning in the real world, we have access to important additional features. First, the points accumulated should be relevant to the learning process: they come from the desired interaction and as they have been evaluated by a human, their label should be correct. Secondly, the human can also provide additional information on the selection to quicken the learning (as implemented in Chapter~\ref{chap:tutoring}). This fuller use of the human teacher might help to mitigate the limited number of datapoints available when learning in the real world to interact with humans.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Impact} \label{sec:disc_impact}

\subsection{Pushing the State of the Art}

At the time of writing, few other methods have been applied to teach robots to interact with humans. As mentioned in Chapter~\ref{chap:background}, the main other approaches were \gls{lfw} and \gls{lfd}, they used demonstrations from a \gls{woz} setup or from interactions between humans, and from these demonstrations learned a policy offline~\citep{knox2014learning,liu2014train,sequeira2016discovering}. Due to the challenges when learning during the interaction (high stakes of actions, limited datapoints and complexity to maintain a policy providing useful data), online learning and \gls{iml} were seldomly used to teach robot to interact with humans, these approaches were mostly focused on teaching agents to interact in other non-social environments. Additionally, most of the previous methods in \gls{iml} only provide limited control to the human teachers, reducing them to ``feedback providers'' while humans could and should provide much more information to learning agents~\citep{amershi2014power}
	
By proposing \gls{sparc}, we wanted to push \gls{iml} to give more power to the teacher, by making better use of people's ability to steer the learning process, and by making it safer and more comfortable for the teachers. By keeping a human in control of the robot's actions, \gls{sparc} enables robots to learn online in sensitive environments. 
Furthermore, with the combination of proposition, correction and selection of actions, \gls{sparc} has the opportunity to reduce the workload on the teacher. Hence, this approach is fit to control and teach robots to interact with humans as it follows the requirements presented in Section~\ref{ssec:back_constraints}. With this method, we successfully demonstrated in Chapter~\ref{chap:tutoring} that robots can be taught online to interact efficiently with humans, while ensuring a constantly appropriate policy. By using a new algorithm and keeping the teacher in control, \gls{sparc} allowed a robot to learn a social and technical policy in a high dimensional and multimodal sensitive environment.

By demonstrating its applicability to teach robots to interact with humans from \textit{in situ} supervision, \gls{sparc} pushes the state of the art both in \gls{hri} and \gls{iml}. By building on \gls{lfd}, \gls{sparc} keeps its advantages: allowing to teach behaviours complex to define or not known in advance. Furthermore, by including an online learning component and keeping the teacher in control, this new method might allow robots to be deployed in new contexts where they are absent today.

\subsection{Empowering Non-Experts in Robotics}

Other methods used to teach robots to interact with people use post processing on previously gathered demonstrations to learn a policy offline. While leading to positive results, these approaches require significant engineering after the demonstrations have been recorded to learn a robot behaviour, and provide limited opportunities to refine the behaviour after the learning is over~\citep{liu2014train,sequeira2016discovering}. This implies that they cannot be used solely by end-users not knowledgeable in machine learning, experts in robotics have to interpret the data provided by the domain expert to design the behaviour. In contrast, by mixing together the collection of data and the learning, \gls{sparc} removes this barrier between data collection and use. That way, end-users can themselves directly teach a robot to interact as they desire.
\gls{sparc} provides an opportunity for anyone to personalise their robot without requiring technical skills. As demonstrated in Chapter~\ref{chap:woz} and \ref{chap:control}, from a single algorithm and state representation, \gls{sparc} can lead to different behaviours adapted to the teacher's strategy and preferences. Combined with efficient interfaces and learning algorithms, approaches such as \gls{sparc} have the potential to democratise robotics by allowing anyone to teach a robot to interact efficiently in a wide range of domains. Robot developers and designers could use this learning ability to deploy robots as \emph{blank slates}, with just a way to perceive the world, act on it and interact with a teacher, and let their behaviour be defined by their users. These users would start filling the blanks, creating their own robot behaviour, teaching their robot how to fulfil their personal needs. As defended by \cite{fails2003interactive} and \cite{amershi2014power}, by allowing end-users to teach an agent to behave as they desire, \gls{iml} methods have the potential to ease the deployments of technology and reach new application domains faster. This might allow users currently excluded from using robots (due to lack of interest from developers and lack of technical skills from the users) to profit from this new technology.

While providing many opportunities, deploying a blank robot able to learn complex policies would require significant engineering pre-deployment to have a wide enough state and action space, and a learning algorithm and interface efficient enough to reach useful policies. However, as demonstrated by the study in Chapter~\ref{chap:tutoring}, this is achievable. A robot can be deployed with large state and action spaces and then using algorithms designed to learn quickly from teachers in complex environment, a non-technical person can teach a robot a complex social policy. Furthermore, by providing additional tools to the users to widen or refine the state and action space, this teaching could be applied to even more applications.

\subsection{Robots and Proactivity}

Another way to interpret \gls{sparc} is as a way to provide anticipatory powers to a robot. By using \gls{ml} and proposing to execute actions, the robot is actually taking the initiative to take an action without executing it straight-away. For instance, a proactive robot assistant would anticipate its user's needs and desires and would propose help or services without having to be asked~\citep{mason2011robot}. This capability has two applications: first it allows robot users not to have to ask supportive behaviour from the robot every time they need it, and second, it means that the robot could even support its user when they do not realise help would be useful.

By having the ability to learn new actions, or what action it should do, such a robot would move from a simple tool to an adaptive partner able to support its user in a large number of tasks. Finally by informing the nearby people of its actions, such a robot assistant would only execute actions deemed useful by its users, limiting the chance of negative outcomes.

%\subsection{SPARC Beyond Human-Robot Interaction}\label{sec:disc_beyond}
%
%Interactions with humans are complex: they require social behaviours in large multimodal environments, with high stakes and where gathering datapoints is a tedious process. As \gls{sparc} demonstrated its efficiency on such an environment, it should be possible to apply it to many other domains outside of \gls{hri} where constraints on the learning process are lighter. For example, it could be used to teach robot manipulation or navigation task: a simulator could present the expected trajectory leaving the teacher time to correct it if needed. Alternatively, and similarly to other \gls{iml} approaches~\citep{fails2003interactive}, \gls{sparc} could also be applied to classification tasks, maintaining the user informed about the state of the algorithm's knowledge and involving them in the learning process. For example, for semi-supervised image classification, the algorithm could automatically present to the user a subset of classified images between learning steps, and this user could step in when a misclassification happens. In these cases, where incorrect actions have limited impacts, the corrections can happen in hindsight, as proposed in \cite{chernova2009interactive}. Finally, the principles of \gls{sparc} could also be used to support agents using \gls{rl} in the real world. The supervisor could provide a safeguard preventing the agent to make errors, bringing it back to the correct parts of the environment or guiding the agent to the relevant actions in complex environments.

%\subsection{Ethical Questions} \label{sec:disc_ethics}
%
%Having robots interact in human environment and allowing people to teach them, raises multiple ethical questions~\citep{lin2014robot}.
%
%The first one concerns people's jobs. Since the industrial revolution, jobs have been automated and more are expected to disappear in the near future due to robotic automation~\citep{frey2017future}. As \gls{sparc} allows non-experts in computing to teach robots, it might foster the deployment of robots in social environments, such as schools or care facilities. The arrival of robots in such sectors might lead teachers or social workers to fear for their jobs. However, in many of such social environments with no direct quantifiable return on investment, the workforce is already reduced (e.g. nurses in the US; \citealt{nevidjon2001nursing}) and this shortage is expected to grow in the future. Consequently, robots provide an opportunity not to replace a workforce already in shortage of workers, but on the other hand to support these people in their tasks, making these jobs safer and more pleasant for the workers and providing additional support for the clients or patients~\citep{wada2005psychological}. However, the \gls{hri} community as a whole needs to be aware of these concerns, ensure that robots have a positive impact on society and communicate their vision of robots helping the human population.
%
%As mentioned in the previous paragraph, by allowing anyone to teach a robot, \gls{sparc} might increase robots' range of application, potentially leading to more interactions with vulnerable populations, such as elderly people or children in school. However, having robots interact in these environments raises multiple questions. \cite{sharkey2012granny} identify that using robots in elder care might ``reduce the amount of human contact'', ``increase the feelings of objectification'', create ``a loss of privacy'' and ``personal liberty'' and elicit ``deception and infantilisation''. They also insist that the conditions were an elderly should be in control of a robot are to be carefully identified. Similarly, \cite{sharkey2016should} expresses concerns about deploying robots in classrooms. As such, roboticists need to work with domain experts to ensure that robots do help their users and have positive impacts where they are deployed.
%
%A third major ethical question concerning learning robots, not exclusive to this work, is privacy. As robots will be deployed in private settings, and especially will learn from and about individuals, issues concerning privacy arise. To have meaningful interactions with their users, robots need to collect information about them. The type of information collected, the storage, the use by third parties and the users' perceptions about this all have to be carefully considered before deploying a robot in the real world~\citep{syrdal2007he}. This  is further exacerbated when robots learn from humans. Robots learning, using \gls{sparc} or other methods, are not any more passively collecting data, but their interactions are designed to gather more information about the user, about their preferences, their desires and their needs. Finally, this effect is even amplified when interacting with vulnerable populations. If robots take an important role in education and care, humans interacting with them will tend to be children or patients, and these people might not be able to ensure their privacy alone. The question of sharing these data and this knowledge between robots and beyond, to the manufacturer or to  governments, needs to be addressed before robots can be deployed on large scales. An additional ethical question linked with privacy is security. These data accumulated by robots needs to be protected from malicious attacks. This issue is even more visible with the recent world wide hacks of the Internet of Things devices (such as Mirai\footnote{\url{https://en.wikipedia.org/wiki/Mirai_(malware)}}). This year, \cite{giaretta2018adding} presented a report of numerous basic security flaws in the Pepper robot and expressed the idea that robotics has moved too quickly from research to market product and that they often do not have the required security to ensure they users' privacy and security.
%
%A last concern resides in the responsibility for the actions executed by the robot~\citep{asaro2007robots}. In a mixed-initiative interaction when both the autonomous agent and the human supervisor can impact the robot's policy, the responsibility of actions is complex to analyse. This effect is even increased when the robot can learn from its user. In that case, the role of the company or the entity distributing this robot and the role of the user can be hard to define when looking for a legal entity accountable for the robot's actions. As argued by \cite{wachter2017transparent}, \gls{ applied to robotics needs to be more transparent, this could both help people to interact and understand robots and provide a clearer accountability for robots' actions. When defining \gls{sparc}, we assumed that the human teacher was constantly in control of the robot's behaviour, however specific conditions might break this assumption. For example, combinations of human behaviours and algorithm specificities might lead to undesirable robot behaviour, such as overloading the teacher with proposition. This might prevent the teacher to control the robot efficiently, and if an error happens, finding the root of the error and assigning responsibility might be a complex task.
%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Future Work}

The work conducted in this thesis explored how robots could be taught to interact with humans and proposed a novel interaction framework, \gls{sparc}, to enable such a learning. However, \gls{sparc} could be extended in many ways and its principles applied to other applications.

\subsection{Application Domains}

\gls{sparc} emerged from the \acrshort{dream} project, a European project aiming to develop new Robot Enhanced Therapies~\citep{thill2012robot,esteban2017build}. Due to the limitations when working with children with \gls{asd}, \gls{sparc} has only been applied to \gls{hri} in the context of tutor robots, to teach a robot to support child learning. Nevertheless, the principles underlying \gls{sparc} could be applied to a much wider range of applications in \gls{hri} and in other fields (cf. Section~\ref{sec:disc_beyond}). For instance, \gls{sparc} could be applied to teach robots a therapeutic behaviour for \gls{rat} while keeping the therapist in control of the interaction (as aimed by \acrshort{dream}). By learning from the therapists, robots could be applied more easily in different therapeutic scenario without requiring a workload as high as \gls{woz}. 

\gls{sparc} would also show promises in numerous other applications in social \gls{hri}: from assistant robots at home to collaborative robotics, including robots in hospitality, the military or industry. For example a robot could learn the preferences of a user and act as an embodied personal assistant, connected to devices in the house, calendar on the internet and supporting its users in routine tasks. Such a robot could learn to anticipate its user's needs and propose to provide support proactively. In \gls{hrc}, similarly to the work presented in \cite{munzer2017efficient}, a robot could learn its partner's preferences, informing them of its actions and helping them to complete the task faster and easier. As such, future work should apply \gls{sparc} to other use cases of \gls{hri} and explore how it could be used to teach agents to interact in other complex or high-stakes environments in \gls{hri} or outside.

\subsection{Learning Beyond Imitation}\label{sec:disc_beyond}

Another potential feature of \gls{sparc}, and other methods based on demonstrations, not evaluated in this work is reaching capabilities beyond the demonstrations. As \gls{sparc} uses demonstrations and corrections from a human teacher to learn, by applying \gls{sl}, the optimal outcome would be to match the teacher's performance. However, if the algorithm possesses or learns a value function or the teacher's goal, instead of reproducing the teacher's policy, the agent could improve its policy around the demonstrated one and potentially become better than the teachers themselves. This achievement have been accomplished by \cite{abbeel2004apprenticeship}, by using Inverse Reinforcement Learning. 

Alternatively, instead of having the robot reaching capabilities beyond human ones on its own, a human-robot team could also together reach this kind of performance. For examples, \cite{kasparov2010chess} proposed ``Advance Chess'', a new type of chess were players have access to a computer to help them during their decisions. This combination of human and machine, aims at profiting from the best of both worlds and would allow humans to make better use of their intuition and creativity while using computer's certainty to save human calculations. Similarly, a learning agent could interact with the human in a mixed initiative framework, such as \gls{sparc}, where the agent could suggest actions (such as moves in a Go or Chess game) and the human could accept them or refuse them. That way the human would be in control of the interaction, preventing potential errors from the artificial agent to have negative impact, while still being open to opportunities they did not anticipate. Hence, the team could reach together super-human capabilities while ensuring, with the presence of the human in control of the interaction, that the behaviour would be at least of human performance. Additionally, this mixed initiative interaction might provide the human with the opportunity to learn from the robot, if the robot proposes better than anticipated actions.

However, to reach these super-human behaviours, the agent requires a way to learn in addition to the human. The agent needs to have access to a second level of learning, for example through receiving rewards directly from the environment or through learning a reward function from the human demonstrations (such as with Inverse Reinforcement Learning). For example, the human could provide a mixture of demonstrations, rewards and high-level goals which could be used to learn such a reward function or to update a planner with more correct models allowing the robot to reach the goals faster. Alternatively, the robot could learn simultaneously from the human and in simulation. The human would use \gls{sparc} to stay in control of the interaction in the real world; and the environmental and human feedback from these real interactions could help refine the simulation. That way, in the real world, the robot's behaviour would stay appropriate, as a human would be in control of it, while the robot would have freedom to explore in simulation.

\subsection{Sustained Learning}

This work, and most of the general research in robotics, considers the problem of learning single tasks in isolation. However, once deployed, robots need to address the challenges of lifelong learning~\citep{thrun1995lifelong}, being able to continuously learn in different domains and transfer knowledge from one situation to another. When interacting in the real-world for extended periods of time, robots cannot rely on offline sessions with engineers to improve their behaviour. By using online learning, methods such as \gls{sparc} could provide this online refinement of behaviour and potentially help the robot to know which parts of the policy are transferable. Furthermore, by allowing the end users to provide commands and information about the desired policy at runtime, \gls{sparc} reduces the need of computing experts once the robot is deployed. 

\gls{sparc} also assumes that the teacher is constantly supervising the robot, however, when deployed in the real world, the teacher might have to leave the robot interact unsupervised for limited periods of time, and the robot needs to adapt its policy to this change. Work has been done in that direction \citep{faulkner2018policy} and could be combined with \gls{sparc}.

Another challenge of learning over long periods of time is the policy's dependency in time. For example, in Chapter~\ref{chap:tutoring}, the temporal aspects of the interaction were taken into account only by including some notions of time since events in the state definition. The robot was not doing any temporal planning or explicitly taking time into account when behaving. However, to sustain continuous long term interaction, spanning multiple hours or days, the dependency in time of the policy has to be taken into account through other means. One way is to learn spatio-temporal features relevant to the human's behaviours, expectations and desires(cf. STRANDS project; \citealt{hawes2017strands} and \citealt{soh2015spatio}). For instance, a robot assistant at home should know its users are typically going to work every morning, except weekends and holidays, and a human teacher could help the robot to interpret these elements related to time. Finally, to sustain learning over long periods of time, robots also need algorithms that can scale well with a large number of data. 

\subsection{Interface With the Teacher}\label{sec:disc_future_interface}

Another axis to improve \gls{sparc}, and which is critical to consider when tackling new applications, is the interface with the teacher. One of the main limitation of \gls{sparc} is also what provides its strength: the inclusion of a human in the action selection process. Including this person and giving them the opportunity to preempt and select any actions comes with limitations on the interaction. The robot needs to communicate its intentions and the human needs enough time to correct them before they impact negatively the environment or other humans interacting with the robot and the teacher needs to inform which actions the robot should execute. 

Consequently, the interface used by the teacher to control the robot is key and should mitigate these limitations. Further work should explore how to provide the best communication between the teacher and the robot. For example, in the case of a \gls{gui} on a tablet or a phone, the interface could combine buttons and a representation of the world where the robot could describe how it plans to act or its expected trajectories. Similarly, the teacher could use this representation of the world to select actions for the robot to execute (as used in Chapter~\ref{chap:tutoring} but also including long-term information). Designing these \gls{gui} for \gls{sparc} could use the knowledge obtained from designing application for phone for example~\citep{joorabchi2013real}. 

However, \gls{gui} might not be the optimal way to communicate with the robot, as they might not scale well with a high number of actions and might require additional hardware. Future work should explore alternatives interfaces, such as natural language, to control a robot through \gls{sparc}. This would raise many challenges, such as natural language understanding, or creating verbal commands describing the robot's actions, intentions or explanation clearly yet concisely~\citep{hayes2017improving}. Despite these challenges, language possesses the qualities required to communicate between a teacher and a robot: familiarity for humans, open-endedness of description and precision for example. Another area of research which would improve \gls{sparc} is Brain Computer Interfaces. For example, when witnessing an error, the brain creates a specific pattern which can be detected using EEG~\citep{gehring1993neural}, that way instead of an explicit \gls{cw}, such an interface could automatically detect errors in robot suggestions without requiring the teacher to explicitly cancel the action. By having a much smaller delay between the proposition and its execution, \gls{sparc} could be applied to more applications.

\subsection{Algorithms}

In the future, \gls{sparc} should be combined with richer learning algorithms. The three examples provided in this thesis represent only a small part of the algorithms \gls{sparc} can be used with. More advanced \gls{ml} have the potential to allow \gls{sparc} to learn faster and more efficiently; and, as mentioned previously, potentially reach super-human performance in the task. However, as mentioned in Chapters~\ref{chap:background} and~\ref{chap:sparc}, many challenges remain when using learning for \gls{hri}. The first one, the main one tackled by \gls{sparc}, is the high stakes of the interaction: errors might lead to disastrous consequences. In addition, the data efficiency is fundamental: when interacting with people, collecting information about human's reaction can be costly or take a large amount of time. As such, each datapoint should be used with high efficiency and the human included in the loop should provide additional knowledge to deal with this scarcity of data. Alternatively, the algorithm could combine data accumulated from different teachers to learn a more general policy. However, this could lead to a transfer problem, assumptions and policies correct with one user might not be valid anymore with another one. Doing this generalisation might decrease the potential for personalisation that \gls{ml} provide. One way to address this personalisation vs generalisation issue is to group people by similarity and learn to detect the group of a person and a policy adapted to this group to reach a better policy~\citep{brunskill2014pac}. Alternatively, the robot could learn or already possess a general policy and then use \gls{sparc} to refine it and adapt it to its user. 

\gls{sparc} and humans in general could also be used to teach hierarchical strategies~\citep{barto2003recent}. With hierarchical learning, agents learn subpolicies used to complete subgoals, and then combine these subpolicies to reach higher goals. By helping an agent to create subpolicies and informing it which ones are relevant to specific context, a human could allow an agent to learn to solve complex tasks much quicker. This teaching on multiple levels presents a challenge for \gls{sparc}, as in the current implementation, it only considers actions one by one and the teacher cannot inform about goals. This would require a way for the teacher to create higher level actions, organise them and switch between them. However, using a human provides a strong potential to quicken the learning of complex and rich policies.

Additionally, instead of providing demonstrations of a policy to follow, the teacher could also give symbolic rules defining the robot's behaviour. This alternative way of teaching could generalise faster than simple demonstrations and might allow teachers to define complex behaviours easily and without having to encounter each situation to show the robot how to behave. 

Another challenge for algorithms used with people is the fact that people are not static entities. As mentioned in Section~\ref{ssec:back_feedback}, different people will use different teaching strategies. Furthermore, as seen in Chapters~\ref{chap:woz},~\ref{chap:control} and~\ref{chap:tutoring}, in \cite{thomaz2008teachable} and \cite{macglashan2017interactive}, human teachers adapt their teaching strategy overtime. Human policies and feedback are moving targets, and algorithms used to learn from people need to take into account these variations and evolutions of behaviours.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Summary} \label{sec:disc_summary}

This chapter started by revisiting the research questions identified in Section~\ref{sec:intro_thesis}, describing how this work addresses them. We then presented the main limitations identified for \gls{sparc} (requirement of a \gls{cw} and attention from the teacher, potential increase of workload on the teacher and complexity of the interface). We presented potential ways to address these limitations when designing interactions involving human teachers. We continued with discussing the potential impact of \gls{sparc} on social \gls{hri} and presenting how it pushes the state of the art further. Finally, we proposed directions to extend the research about \gls{sparc}, exploring how to increase its range of application and its efficiency.