%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Discussion} \label{chap:discussion}
\glsresetall

Chapter \ref{chap:background}, put the light on the absence of robot controllers today providing adaptivity to a robot with a low workload from humans and while ensuring that the robot's behaviour is constantly appropriate. Based on this observation, Chapter \ref{chap:sparc} presented the \gls{sparc}, an interactive teaching framework designed to allow robots to learn a social interactive behaviour by begin supervised by humans. Then Chapters \ref{chap:woz}, \ref{chap:control} and \ref{chap:tutoring} evaluated this approach in three studies, the last of which evaluated \gls{sparc} in real \gls{hri} consisting of a learning activity involving 75 children. Combined together, these chapters sought support for the thesis of this research:

\begin{quote}
	A robot can learn how to interact meaningfully with humans by receiving supervision from a human teacher in control of the robot's behaviour, this supervision will lead to an efficient, safe and low human-workload teaching and autonomous behaviour.	
\end{quote}

This chapter will combine the results from the different studies presented in this research to discuss the findings and will start by presenting the limitations of the approach proposed in this work, \gls{sparc}, and of its evaluation in the three studies. Ethical questions raised when humans teach robots to interact with humans will be discussed in Section \ref{sec:disc_ethics}, and Section \ref{sec:disc_impact} will presents the potential impacts of this research. Finally, the last section will propose axes where \gls{sparc} could be extended to increase its usability and application to \gls{hri}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Limitations} \label{sec:disc_limitations}

\subsection{Limitations of SPARC}

\gls{sparc}, as presented in Chapter \ref{chap:sparc} presents several limitations restricting the range of domains it could be applied to.

The first of these limitations is the requirement of a human to supervised the robot, even once it has learn an action policy. As stated earlier, \gls{sparc} aims to move away from \gls{woz} or other tele-operation method by learning an efficient action policy from the human commands. In its original framing, \gls{sparc} did not aim to create a fully autonomous agent behaving without supervision, but to smooth the teaching phase and the use phase in one single interaction using \gls{sa}. In this single phase, the workload on the supervisor would decrease as the robot learns while a high performance in the domain application would be maintained. However, \gls{sa} still requires a human involved in the supervision and as such presents limited applicability where robots are expected to fill gaps in human workforce or to reduce the requirements on humans. 

Despite not being the original goal, and as demonstrated in Chapter \ref{chap:tutoring}, \gls{sparc} can still be applied to create a fully autonomous behaviour. In that case, the training process would be similar to \gls{lfd}, with a training phase using \gls{sparc} and then the testing/deployment phase of the autonomous behaviour. However, by using \gls{sparc} two differences remain. During the training phase, instead of passively receiving commands from the teacher, the robot would proactively make suggestions to the teacher. As presented in Section \ref{sec:tutoring_opportunities} this aims at reducing the workload on the teacher during the training phase, would provide more datapoints for the learning and would inform the teacher about the state of robot's knowledge, potentially creating trust between the robot and its teacher. Furthermore, even after being deployed to interact autonomously, the teacher could still step in using \gls{sparc} to refine the action policy, or if a different behaviour has to be applied (e.g. if the robots interact with a child with special needs rather than a typical one), the teacher can take over using \gls{sa} to ensure a personalised for this specific interaction.

A second limitation of \gls{sparc} corresponds to the constant need for the human's attention. Throughout this research, we assumed that even if the robot behaviour may be mostly correct, the supervisor would be attentive to the robot suggestions and ready to correct them at any time. This assumption is similar to autonomous car using safety driver\ESc{maybe not}{ or the `AutoPilot' of Tesla requiring constant human supervision}. The agent is fully autonomous but may make mistakes and as such, a human needs to be ready to correct these errors before they impact the world. However, as demonstrated by the accidents in 2017 and early 2018 involving these supervised autonomous vehicles, this assumption is often violated, and a short moment of inattention may have dire consequence\footnote{As reported by media in early 2018}. By observing seemingly correct agent's behaviour, the human supervisor might start to overtrust the agent, missing the occasion to react in time to anticipable errors potentially leading to frustration or death in the case of autonomous vehicles\ES{obvious but ref?}. Some ways exist to potentially mitigate this limitation. For example, with \gls{sa} the agent informs in advance the supervisor about its actions, similarly a car could display the planned trajectory. This would provide the supervisor with more time to analyse the situation and potentially allowing them to react in time. Alternatively, the agent could communicate a lack of confidence in its actions or its interpretation of the environment, informing the supervisor that attention is specially required in that moment.

Similarly, as pointed already in Section \ref{ssec:sparc_time}, with \gls{sparc}, the presence of the \gls{cw} and the auto-execution of actions lead to several issues. To be applicable and make use of the auto-execution of actions as a way to reduce workload, the \gls{vw} of an action needs to be wider than its \gls{cw}. That way, actions approved passively are still valid when executed. To increase the application of \gls{sparc} to a wider range of application, the \gls{cw} has to be as narrow as possible. In contrast, the supervisor needs a \gls{cw} as wide as possible, to provide them with enough time to process the action and cancel it if required. Correction windows too narrow would put additional pressure on the supervisor to react in time. To cope with this additional time pressure, the supervisor might find ways to prevent the suggestions of actions or simply refuse each action proposed by the robot. This results in two effect having opposite requirements on the \gls{cw}. This need of a \gls{cw} wide enough to allow the teacher to react is probably on of the main limits of \gls{sparc}. Providing the teacher with enough time to correct undesired actions impacts the usability of \gls{sparc}. One way of mitigating it, could be to adapt the time window to the type of actions executed or to the algorithm's confidence in the validity of actions or allow the teacher the duration of this \gls{cw}. As such, specific actions with a short \gls{vw} (such as emergency breaking for autonomous vehicles) could be executed in time; and actions the robot is unsure about would be given more time to be corrected. 

Another risk with \gls{sparc} is overloading the supervisor with suggested actions. If the supervisor has to correct more actions than it would have selected, the workload is not reduced but increased. While still providing useful information for the learning algorithm (and more than only the actions selected by the supervisor), this supplementary workload on the teacher is not desired. The algorithm needs to adapt the rate of suggestion to the pace of the interaction not to overload the teacher.

A last limit of \gls{sparc} is the interface between the learning algorithm and the teacher. Other \gls{iml} methods using only feedback require a single scalar to evaluate actions and the communication needs to be only one way: from the teacher to the robot. On the other hand, to provide full control and accountability on the robot's actions, \gls{sparc} requires a way to inform the teacher about the robot intention and allow them to pre-empt actions. Furthermore, the teacher needs to be able demonstrate any action the robot should do, they require a way to transmit any action to the robot. As the robot may have access to hundreds of actions, assigning one button per action is not feasible, other ways of commanding the robot need to be found. Furthermore, as mentioned in Chapter \ref{chap:sparc}, with \gls{sparc} the teacher can also provide additional information to the algorithm to speed up the learning. In summary, the interface between the robot and the teacher needs to provide the teacher with the robot's intention, allow the teacher to pre-empt actions, select any action and provide additional information to the learning algorithm. An interface providing all these features can easily be bloated and difficult to use by humans, increasing even more the required workload to control and teach the robot. For applying \gls{sparc} to complex environments, efforts need to be invested in the interface to make it intuitive and clear. For instance, in Chapter \ref{chap:tutoring}, the \gls{gui} used by the teacher represents the game, with some buttons for a sub part of the action set, but the majority of actions is inferred by the way the teacher move items on the screen and the items selected. An alternative way could be to use natural language. Humans are expert in using natural language to communicate and the open-endedness of this tool makes it suited for applications of \gls{sparc} where the teacher can speak and the time required to vocalise commands is not critical.

\subsection{Experimental Limitations} \label{sec:disc_experiments} 



%Similarly to the method proposed in this research, the studies evaluating \gls{sparc} also presented some limitation. 

%The first study presented in Chapter \ref{chap:woz} used participants familiar with robots. One could argue that this population is not representative of the general population expected to interact with robots. However, and as explained in that chapter, in many case in \gls{hri}, the wizards teleoperating robots are trained experts knowing the robot used in the interaction and how to control it. Thus, the participants involved in that study are representative of a significant part of the population expected to supervise a robot to interact with other humans. Additionally, the small sample of the study (N=10 for a within participants design) explains why no significance testing have been done in the study. However, valuable information have been collected and helped us to design the two other evaluation of \gls{sparc}. A last potential limitation of the evaluation relates to the discretisation of time. In this study, the \gls{woz} condition was not a real \gls{woz} as the participants needed to select an action (or accept a random action) at each step. However, this made the two conditions more comparable and the amount of workload on the supervisor in the \gls{woz} condition should have been similar to a real \gls{woz}.

%The second study, presented in Chapter \ref{chap:control} aimed to address some limitations of the first one by using a larger sample (N=40) and drawing participants from a more general population non-expert in robotics or \gls{ml}. However by using the tool provided by the university, a majority of the participants were students from the university;  as such it produced another bias on the participants. However, as robots are by essence technological tools, they will probably be mostly used by people familiar with technology. Thus, a university student might also be a good representation of the more general population expected to use robots. Another limit of the study was that it did not exactly replicate the study presented in \cite{thomaz2008teachable}, due to the absence of access to the source code, the experiment had to be reimplemented, leading to differences in the experimental setup between the original study and ours. Additionally, this study compared \gls{sparc} only to a single other method used in \gls{iml}, other methods exist and would have been interesting to compare with \gls{sparc}. However, while \gls{sparc} has only be compared with one approach and differences remained between our implementation and the original one, the results obtained by that study still provide us with useful insight about how \gls{sparc} could be combined with \gls{rl} and how the amount of control provided to the teacher impacts the teaching process.

%The last study presented in Chapter \ref{chap:tutoring} evaluated how \gls{sparc} could be applied to teach robot to interact socially with children. In this study, the robot's teacher was a single human following the robot through many teaching sessions. Furthermore, this supervisor was a PhD student in psychology and not a trained teacher for children. As such, there might be a potential bias of her behaviour on the success of the teaching process. However, she was not knowledgeable of \gls{ml} and was no aware of the type of algorithm used in the study or the inputs the algorithm used for the learning. As such, she possess many similarities with the type of users with the potential of using robots in education contexts. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Impact} \label{sec:disc_impact}

\subsection{Teaching Robot to Interact with Humans}

With \gls{sparc} we proposed a new way to teach robots to interact with humans. By following the principles presented in Section \ref{sec:sparc_principles}, a teacher can safely teach a robot an action policy suitable to interact with humans. This method provide a robot with an adaptive policy while ensuring the its behaviour is constantly appropriate. Furthermore, with the combination of proposition, correction and selection of actions, \gls{sparc} aims at reducing the workload on the teacher. Hence, this approach would be fit to control robots in \gls{hri} as it follows the requirements presented in Section \ref{ssec:back_constraints}. And, as demonstrated in Chapter \ref{chap:tutoring}, when applied to a real \gls{hri}, \gls{sparc} provided the robot with an efficient social action policy. 

By demonstrating its applicability to teach robots to interact with humans from in-situ supervision, \gls{sparc} opens new opportunities to provide robots with action policy complex to define or not known in advance. Providing robot with the capability to learn to interact with human might allow robots to be deployed in new contexts where they are absent today.

\subsection{Empowering Non-Experts in Robotics}

By allowing end-users non-expert in robotics to teach a robot how to interact, \gls{sparc} provides an opportunity for anyone to personalise their robot. If combined with efficient interfaces and learning algorithms, approaches such as \gls{sparc} have the potential to democratise the use of robotics by allowing anyone to teach a robot to interact efficiently in a wide range of domains. Robot developers and designers can use this learning ability to deploy robots as \emph{blank slate}, with just a way to perceive the world and act on it, and their behaviour could be defined by anyone. Users would start filling this blank slate with how they want their robot to behave, teaching their robot how to fulfil their personal needs.

\subsection{Robots and Proactivity}

Another way to interpret \gls{sparc} is a sa way to provide proactivity to a robot. By using \gls{ml} and proposing to execute actions, the robot is actually taking the initiative to do an action without executing it straight-away. Proactive robot assistant would anticipate their user's needs and desires and would reduce their workload. By having the capability to learn new actions, or what action they should do, they would be adaptive companion able to solve a large quantity of tasks. Finally by informing the surrounding humans of their actions, such robots would only execute action deemed safe by their users.

\subsection{Ethical Questions} \label{sec:disc_ethics}

Having robots interacting in human environment and being able to teach them to behave raise multiple ethical questions.

The first one concerns people's jobs. Throughout robots and machines history, many jobs have been automated and more are expected to disappear in the next years \citep{frey2017future}. As such, deploying robots in social environments, such as education or care facilities, might lead teacher or social worker to fear for their jobs. However, in many of such social environments with no direct quantifiable return on investment, workforce is already lacking (e.g. nurses in the US; \citealt{nevidjon2001nursing}) and this shortage is expected to grow in the future. Consequently, robots provide an opportunity not to replace a workforce already in shortage of workers, but on the other end to support them in their job, making these jobs safer and more pleasant for the workers or providing additional support for the clients or patients \citep{belpaeme2012multimodal}\ES{not sure}. However, the community as a whole needs to be aware of these fears and ensure by their work that robots have a positive impact on society.

The second question is privacy. As robot interact with the general population, and especially learn from and about them, issues concerning privacy arise. To have meaningful interaction with users, robots need to collect information about them, and the type of information collected, the storage and the use by third parties has to be carefully considered before deploying a robot. This effect is further increased when robots learn from humans, robots are not any more passively collecting data, but the interaction itself aims to gather more information about the user, about their preferences, their desires and their needs. Finally, this effect is amplified when interacting with vulnerable populations. As robots are expected to take an important role in education and care, humans interacting with them will tend to children or patient, and as they might not be able to ensure their privacy, this issue has to be considered beforehand. The question of sharing these information and this knowledge between robots and beyond, to the manufacturer or to the governments, needs to be addressed.

A last concern resides in the responsibility for actions executed by the robot. In a mixed-initiative interaction when both the autonomous agent and the human supervisor can impact the robot action policy, the responsibility of actions is complex to analyse. This effect is even increased when the robot can learn from its user. In that case, the role of the company or the entity distributing this robot and the role of the user have to be considered when looking for a legal entity accountable for the robot actions. To have this clearer accountability, \gls{ai} applied for robotics needs to be more transparent \citep{wachter2017transparent}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Future Work}

The work conducted in the thesis explored how robot could be taught to interact with humans and proposed a novel interaction framework, \gls{sparc}, to enable such a learning. However, \gls{sparc} could be extended in many ways and its principles applied to other applications.

\subsection{Application Domains}

Through this thesis, \gls{sparc} has only been applied to \gls{hri} in the context of tutor robots, to teach them to support child learning. However, the principles underlying \gls{sparc} could be applied to a much wider ranges of application in robotics and other \gls{ai}. In robotics, \gls{sparc} would show promises in numerous fields of social \gls{hri} from assistant robot at home to collaborative robotics including robots in hospitality, military or industry. For example a robot could learn the preferences of a user and act as an embodied personal assistant, connected to devices in the house, calendar on the internet and supporting its users in routine tasks. Such a robot could learn to anticipate its user's need and propose to provide support proactively. In \gls{hrc}, similarly to the work presented in \cite{munzer2017efficient}, a robot could learn its partner preference, informing them of its actions and help them to complete the task faster. As explained in \cite{feil2005defining}, \gls{iml} approaches, and by extend \gls{sparc}, could be applied to classification tasks, maintaining the user informed about the state of the algorithm's knowledge and involving them in the learning process. For example, for semi-supervised image classification, the algorithm could automatically present a subset of classified images between learning steps to the user who could step in when a misclassification happen. In this case, where incorrect actions have limited impacts, the correction can happen in hindsight, as proposed in \cite{chernova2009interactive}. Alternatively, the principles of \gls{sparc} could be also used to support agents using \gls{rl} in the real world. The supervisor could provide a safeguard preventing the agent to make errors, bringing it back to the correct parts of the environment or guiding the agent to relevant actions or features in complex environments.

%\subsection{Multiple Teachers}

\subsection{Learning Beyond Imitation}
\ES{rephrase}
Another potential feature of \gls{sparc}, and other methods based on demonstrations, not evaluated in this research is reaching capabilities beyond the demonstrations. \gls{sparc} uses demonstrations and corrections from a human teacher to learn. By applying \gls{sl}, the optimal outcome would be to match the teacher's performance. However, if the algorithm learn a value function or the teacher's goal, instead of reproducing the teacher policy, the agent could learn around the demonstrated policy and potentially become better than the teacher themselves. This capability has been reached in \cite{abbeel2004apprenticeship}, by using Inverse Reinforcement Learning. The agent learns a reward function from the human demonstrations and then, by applying \gls{rl} around the demonstrated policy, the agent improves beyond the demonstrations and reaches super-human capabilities. Similarly to their methods, using the shared control provided by \gls{sparc}, a robot could interact under supervision, progressively proposing action to the supervisor. And the teacher could still direct the exploration, preventing the robot to reach dangerous parts of the world, while being open to better propositions than the ones expected. An example could be an agent learning a game such as under supervision, which could propose moves to a human player. That way, the agent could propose moves the human would not have thought about, and then the human could let them be tried or if they appear to risky, override them. 

However, to reach these super-human behaviours, the agents requires a way to learn in addition to the human. The agent needs to have access to a second level learning, such as a reward function directly from the environment or learnt from the human demonstrations (such as with Inverse Reinforcement Learning). Alternatively, the agent could learn in simulation, only using the human supervision when acting in the real world to prevent potential mistakes.

\subsection{Sustained Learning}

In the two first studies presented in this research, the robot learn an action policy from a short term interaction (inferior to 25 minutes). In contrast, robots deployed to interact with humans on a daily basis will learn over much longer periods of time. This aspect, repeated learning, has been partial explored in Chapter \ref{chap:tutoring}, where the robot learns through 25 sessions. However, challenges are still present to learn over the robot's all life. For example, the STRANDS project \cite{hawes2017strands}, explores this challenge by taking into account the spaciotemporal representation of the world and using it to select the robot actions.

\ES{talk more about life-long learning}

\subsection{Teacher Interface}

Another axis to improve \gls{sparc} is the interface with the teacher. As mentioned in the previous section, one of the main limitation of \gls{sparc} is also what provide \gls{sparc} its strength: the inclusion of a human in the action selection process. Including this human and giving them the opportunity to pre-empt actions and select any actions come with limits the environments where \gls{sparc} is applicable. The robots needs to communicate its intentions and the human needs enough time to correct them before they impact negatively the environment or the humans interacting with the robot. Despite these limits in time ---

However, the interface used by the teacher to control the robot could mitigate these limitations, and further could explore how to provide the best communication between the teacher and the robot. For example, in the case of a \gls{gui} in a tablet or a phone, the interface can combine buttons and a representation of the world where the robot could describe how it plans to act. Similarly, the teacher could use this representation of the world to select actions for the robot to execute (as used in Chapter \ref{chap:tutoring}). The type of \gls{gui} to control \gls{sparc} could use the knowledge obtained from designing application for phone for example. Alternatively, future work could explore how natural language could be used to control a robot using \gls{sparc}. Many challenges would have to be tackle such as natural language understanding, or creating a string to describe clearly yet concisely, actions the robot intend to execute. Despite this challenges, language possess the qualities required to communicate between a teacher and a robot: high bandwidth, familiarity for humans, open-endedness of description. As such using combining language and \gls{sparc} is a relevant direction of work...

\subsection{Algorithms}

In the future, \gls{sparc} should also be combine with richer learning algorithms. The three examples provided in this thesis represent only a small part of the algorithms \gls{sparc} can be combined with. More advanced \gls{ml} would allow \gls{sparc} to learn faster and more efficiently, and ,as mentioned previously, potentially reach super-human performance in the task. However, as mentioned in Chapters \ref{chap:background} and \ref{chap:sparc}, many challenges remains when using learning for \gls{hri}. The first one, the main one tackled by \gls{sparc}, is the high stakes of the interaction, incorrect errors might lead to disastrous consequences. In addition, the data efficiency is fundamental: when interacting with humans, collecting information about human's reaction can be costly or take a large amount of time. As such, each datapoint should be used with high efficiency and the human included in the loop could provide additional knowledge to deal with this scarcity of data. Alternatively, the algorithm can combine data accumulated with different people to learn a more general policy. However, this lead to a transfer problem, assumptions and policy correct with one user might not be valid anymore for another. And doing this generalisation decreases the potential for personalisation that \gls{ml} provide. One way to address this personalisation vs generalisation issue is to group people by similarity and learn to detect the group of a person and an action policy adapted to this group to reach a better action policy \citep{brunskill2014pac}. Alternatively, the robot could learn or already possess a general action policy and then use \gls{sparc} to refine it and adapt it to its user. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Summary} \label{sec:disc_summary}

\ES{I should probably state the things - difference with introduction}

This chapter started by presenting the main limitations identified for \gls{sparc} and how they could be address when designing an interaction involving \gls{sparc}. We then 

%. We defined the impact of each evaluation limitation on the claims made to support \gls{sparc} and its applicability. 
%Similarly, the limitations of \gls{sparc} as a teaching methods have been presented, and ways to mitigate these limits were discussed. 
Using robots in human environments and teaching them raise a number of ethical issues that were examined. Finally, we proposed directions to extend the efficiency and applicability of \gls{sparc} that could be addressed in future work.

\ES{mention that SPARC not magical tool solving every thing, but useful method applicable for a number of situations}