%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Discussion} \label{chap:discussion}
\glsresetall

Chapter~\ref{chap:background}, put the light on the absence of robot controllers today providing adaptivity to a robot with a low workload from humans and while ensuring that the robot's behaviour is constantly appropriate. Based on this observation, Chapter~\ref{chap:sparc} presented the \gls{sparc}, an interactive teaching framework designed to allow robots to learn a social interactive behaviour by being supervised by humans. Then Chapters~\ref{chap:woz},~\ref{chap:control} and~\ref{chap:tutoring} evaluated this approach in three studies, the last of which evaluated \gls{sparc} in real \gls{hri} consisting of a learning activity involving 75 children. Combined together, these chapters sought support for the thesis of this research:

\begin{quote}
	A robot can learn how to interact meaningfully with humans by receiving supervision from a human teacher in control of the robot's behaviour, this supervision will lead to an efficient, safe and low human-workload teaching and autonomous behaviour.	
\end{quote}

This chapter will combine the results from the different studies presented in this research to discuss the findings and will start by presenting the limitations of the approach proposed in this work, \gls{sparc}. Then, we will present how the three studies executed in this research work answered the research questions raised in Chapter~\ref{chap:intro}. We will continue with discussing the more general impact this research may have and the ethical questions raised by teaching robots to interact.
%ethical questions raised when humans teach robots to interact with humans will be discussed in Section~\ref{sec:disc_ethics}, and Section~\ref{sec:disc_impact} will presents the potential impacts of this research. 
Finally, the last section will propose axes where \gls{sparc} could be extended to increase our knowledge in how humans teach robots and improve \gls{sparc}'s usability and application to \gls{hri}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\section{Limitations} \label{sec:disc_limitations}

\section{Limitations of SPARC} \label{sec:disc_limitations}

\gls{sparc}, as presented in Chapter~\ref{chap:sparc} is a promising framework to teach robots to interact with humans or in high stake environments, but presents several limitations restricting the range of domains it could be applied to.

\subsection{Requirement of a Human in the Loop}
The first of these limitations is the requirement of a human to supervised the robot, even once it has learn an action policy. As stated earlier, \gls{sparc} aims to move away from \gls{woz} or other teleoperation methods by learning an efficient action policy from the human commands. In its original framing, \gls{sparc} did not aim to create a fully autonomous agent behaving without supervision, but to smooth the teaching phase and the use phase into one single interaction using \gls{sa}. In this single phase, the workload on the supervisor would decrease as the robot learns while a high performance in the domain application would be maintained. However, \gls{sa} still requires a human involved in the supervision and as such presents limited applicability to fields where robots are expected to fill gaps in human workforce or to reduce the requirements on humans. 

Despite not being the original goal, and as demonstrated in Chapter~\ref{chap:tutoring}, \gls{sparc} can still be used to create a fully autonomous behaviour. In that case, the training process would be similar to \gls{lfd}, with a training phase using \gls{sparc} and then the testing/deployment phase of the autonomous behaviour. However, by using \gls{sparc} two differences remain. During the training phase, instead of passively receiving commands from the teacher, the robot would proactively make suggestions to the teacher. As presented in Section~\ref{sec:tutoring_opportunities} this aims to reduce the workload on the teacher during the training phase, provide more datapoints for the learning and inform the teacher about the state of robot's knowledge, potentially creating trust between the robot and its teacher. Furthermore, even after being deployed to interact autonomously, the teacher could still step back in control using \gls{sparc} again to refine the action policy. Alternatively, if a different behaviour has to be applied (e.g. if the robots interact with a child with special needs rather than a typical one), the teacher can take over using \gls{sa} to ensure a personalised experience for this specific interaction.

\subsection{Reliance on Human's Attention}
A second limitation of \gls{sparc} corresponds to the constant need for the human's attention. Throughout this research, we assumed that even if the robot behaviour may be mostly correct, the supervisor would be attentive to the robot suggestions and ready to correct them at any time. This assumption is similar to autonomous cars using a safety driver\ESc{maybe not}{ or the `AutoPilot' of Tesla requiring constant human supervision}. The agent is fully autonomous but may make mistakes and as such, a human needs to be ready to correct these errors before they impact the world. However, as demonstrated by the accidents in 2017 and early 2018 involving these supervised autonomous vehicles, this assumption is often violated, and a short moment of inattention may have dire consequence\footnote{Cf. the Tempe accident reported by media in early 2018.}. By observing a seemingly correct agent's behaviour for an extended period, the human supervisor might start to overtrust the agent, missing the occasion to react in time to anticipable errors potentially leading to frustration or death in the case of autonomous vehicles\ES{obvious but ref?}. However, some ways exist to mitigate this limitation. For example, with \gls{sa} the agent informs in advance the supervisor about its actions, similarly a car could display the planned trajectory. This would provide the supervisor with more time to analyse the situation and potentially allowing them to react in time. Alternatively, the agent could communicate a lack of confidence in its actions or its interpretation of the environment, informing the supervisor that attention is specially required in that moment.

\subsection{Time Pressures}
Similarly, as pointed already in Section~\ref{ssec:sparc_time}, with \gls{sparc}, the presence of the \gls{cw} and the auto-execution of actions may lead to issues. To be applicable and make use of the auto-execution of actions as a way to reduce workload, the \gls{vw} of an action needs to be wider than their \gls{cw}. That way, actions approved passively are still valid when executed. To increase the application of \gls{sparc} to a wider range of application, the \gls{cw} has to be as narrow as possible. In contrast, the supervisor needs a \gls{cw} as wide as possible, to provide them with enough time to process the action and cancel it if required. Correction windows too narrow would put additional pressure on the supervisor to react in time or even prevent them to prevent undesired actions. To cope with this additional time pressure, the supervisor might find ways to prevent the suggestions of actions or simply refuse each action proposed by the robot. This results in two effects having opposite requirements on the \gls{cw}. This need of a \gls{cw} wide enough to allow the teacher to react is probably on of the main limits of \gls{sparc}. One way of mitigating it, could be to adapt the time window to the type of actions executed or to the algorithm's confidence in the validity of actions. As such, specific actions with a short \gls{vw} (such as emergency breaking for autonomous vehicles) could be executed in time; and actions the robot is unsure about would be given more time to be corrected. Alternatively, the teacher could also manually select the duration of this \gls{cw}, and be also in control of the timescale of the interaction.

\subsection{Overloading the Teacher}
Another risk with \gls{sparc} is overloading the teacher with suggested actions. If the teacher has to correct more actions than they would have selected, the workload is not reduced but increased. While still providing useful information for the learning algorithm (and more than only the actions selected by the supervisor), this supplementary workload on the teacher is not desired. As explained before, this could lead to erroneous teacher's behaviour hindering the learning and potentially increasing the risk in the interaction. The algorithm needs to adapt the rate of suggestion to the pace of the interaction not to overload the teacher.

\subsection{Interface}

The interface between the teacher and the robot is key when applying \gls{sparc} or other \gls{iml} methods. Approaches using only feedback require a single scalar to evaluate actions and the communication needs to be only one way: from the teacher to the robot. On the other hand, to provide full control and accountability on the robot's actions, \gls{sparc} requires a way to inform the teacher about the robot intention and allow them to pre-empt actions. Furthermore, the teacher needs to be able demonstrate any action the robot should do, they require a way to select and transmit each action to the robot. As the robot may have access to hundreds of actions, assigning one button per action is not feasible, other ways of commanding the robot need to be found. Furthermore, as mentioned in Chapter~\ref{chap:sparc}, with \gls{sparc} the teacher can also provide additional information to the algorithm to speed up the learning. In summary, the interface between the robot and the teacher needs to provide the teacher with the robot's intention, allow the teacher to pre-empt actions, select any action and provide additional information to the learning algorithm. An interface providing all these features can easily be bloated and difficult to use by humans, increasing even more the required workload to control and teach the robot. For applying \gls{sparc} to complex environments, efforts need to be invested in the interface to make it intuitive and clear. 

For instance, in Chapter~\ref{chap:tutoring}, the \gls{gui} used by the teacher represented the current state of the game, with some buttons for accessing to a subpart of the action space, but the majority of actions is inferred by the way the teacher move items on the screen and the items selected. An alternative way could be to use natural language. Humans are expert in using natural language to communicate and the open-endedness of this tool makes it suited for applications of \gls{sparc} where the teacher can speak and the time required to vocalise commands is not critical.

\subsection{Learning in the Real World}

As justified in Chapter~\ref{chap:background}, no simulator of humans precise enough exists, so learning for \gls{hri} has to happen in the real world. In addition to this \emph{space} constraint, learning from humans also adds a \emph{time} constraint. By including a human in the action selection loop, the interaction needs to go at a human pace. This implies that gathering datapoints for \gls{sparc} is a relatively slow process. As such, algorithms in combination to \gls{sparc} need to be data efficient, and able to learn from a low number of datapoint. However, by including the human in the loop and learning in the real world, we can have important additional features. First, the points accumulated are sure to be relevant to the learning process: they come from the desired interaction and as they have been validated by a human, their label must be correct. And secondly, the human can also provide additional information on the selection to quickens the learning (as implemented in Chapter \ref{chap:tutoring}).
%\subsection{Experimental Limitations} \label{sec:disc_experiments} 
%\ES{Need to be sure that all these limitations are mentioned in the corresponding parts}
%Similarly to the method proposed in this research, the studies evaluating \gls{sparc} also presented some limitation. 

%The first study presented in Chapter~\ref{chap:woz} used participants familiar with robots. One could argue that this population is not representative of the general population expected to interact with robots. However, and as explained in that chapter, in many case in \gls{hri}, the wizards teleoperating robots are trained experts knowing the robot used in the interaction and how to control it. Thus, the participants involved in that study are representative of a significant part of the population expected to supervise a robot to interact with other humans. Additionally, the small sample of the study (N=10 for a within participants design) explains why no significance testing have been done in the study. However, valuable information have been collected and helped us to design the two other evaluation of \gls{sparc}. A last potential limitation of the evaluation relates to the discretisation of time. In this study, the \gls{woz} condition was not a real \gls{woz} as the participants needed to select an action (or accept a random action) at each step. However, this made the two conditions more comparable and the amount of workload on the supervisor in the \gls{woz} condition should have been similar to a real \gls{woz}.
%\ES{low number of datapoint and slower interaction}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Research Questions}
This section will revisit the research questions identified in Section~\ref{sec:intro_thesis} and explain how the work presented in this thesis addressed them.
\begin{itemize}
\item [RQ1] \textbf{What are the requirements of a robot controller to ensure a behaviour suited to \gls{hri}?} 
Based on a review of the different fields of application of social \gls{hri}, we defined three requirements a robot controller should follow to ensure an efficient interaction. First and foremost, the robot's behaviour needs to be constantly appropriate: as robots often interact with vulnerable populations, their behaviour needs to be constantly safe for the humans they interacting with. Secondly, the robot should be adaptive, be able to generalise to unexpected situations, but also personalise its behaviour to the different humans it interacts with and be able to learn, improving and extending its action policy. Finally, the robot needs to be as autonomous as possible, or at least require a low human workload to interact in the world.

\item [RQ2] \textbf{What interaction framework would allow a human to teach a robot while validating the requirements from RQ1?}
To validate the three principles expressed as answer to RQ1, we proposed \gls{sparc}, a new teaching framework for robots which provides control over the robot's actions to a teacher and use this control to learn in a safe way, validating the first and second requirements. Secondly, by allowing the teacher to passively accept the robot's propositions, we aim to decrease the workload on the teacher over time and progressively provide the robot with autonomy. 

\item [RQ3] \textbf{Could a robot decrease its supervisor's workload by learning from their supervision?}
Study 1 showed that providing a supervised robot with learning can reduce the workload on its supervised. Furthermore, study 2 demonstrated that, compared to other methods, \gls{sparc} is an efficient way to enable a safe teaching and requires a comparatively low workload.

\item [RQ4] \textbf{How providing the teacher with control over the learner's actions impacts the teaching process?} 
Results from study 1 and 2 indicate that by informing the teacher in advance of its actions, the robot ensures that its final behaviour will be correct. This implies that even in early phases of the learning, when the robot behaviour is not adequate yet, the teacher can prevent the robot's lack of knowledge to negatively impact the world. Furthermore, this control improves the teaching process by making it faster, safer, more efficient and lighter (as requiring a lower workload) than other methods lacking control.

\item [RQ5] \textbf{How teaching a robot to interact socially impacts the two humans involved in the overall triadic interaction?}
When being used to teach a robot to interact with a human, \gls{sparc} does allow the robot to constantly have an efficient behaviour (as demonstrated by the improvement of children's behaviours in the supervised condition in Chapter~\ref{chap:tutoring}). However, this improvement of the application interaction might come with a cost for the teaching interaction. As the teacher needs to react to the robot's suggestions, they do have to monitor a second autonomous agents, which might lead to a heavier workload than other supervision methods.

\item [RQ6] \textbf{After receiving supervision from a human, could a robot behave autonomously in a social context?}
In Chapter~\ref{chap:tutoring}, we used \gls{sparc} to teach the robot in the supervised condition, and then deployed the robot to interact autonomously. During this autonomous interaction, the robot applied a policy similar to the demonstrated one, and the impacts on the children's behaviour were close to the ones in the supervised condition. Consequently, in this study, the robot could behave socially in an autonomous fashion after having been supervised by a human in a learning phase.

\end{itemize}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Impact} \label{sec:disc_impact}

\ES{Make better!}

\subsection{Teaching Robot to Interact with Humans}

With \gls{sparc} we proposed a new way to teach robots. By following the principles presented in Section~\ref{sec:sparc_principles}, a teacher can safely teach a robot an action policy suitable to interact with humans. This method provides a robot with an adaptive policy while ensuring that its behaviour remains constantly appropriate. Furthermore, with the combination of proposition, correction and selection of actions, \gls{sparc} aims to reduce the workload on the teacher. Hence, this approach would be fit to control robots in \gls{hri} as it follows the requirements presented in Section~\ref{ssec:back_constraints}. And, as demonstrated in Chapter~\ref{chap:tutoring}, when applied to a real \gls{hri}, \gls{sparc} provided the robot with an efficient social action policy and ensured a safe teaching process. 

By demonstrating its applicability to teach robots to interact with humans from in-situ supervision, \gls{sparc} opens up new opportunities to provide robots with action policy complex to define or not known in advance. Providing robot with the capability to learn to interact with human might allow robots to be deployed in new contexts where they are absent today.

As demonstrated in Chapter~\ref{chap:control}, \gls{sparc} can also teach different behaviours using the same algorithm and state representation. As different teachers have various preferences, they might teach the robots in different ways; therefore, with \gls{sparc} the resulting autonomous behaviour can be adapted to each situation and represents the teacher's desired policy. %\gls{sparc} can be a way to allow non-experts in robotics to personalise their robot, to have it learn to behave in the way they each desire.

\subsection{Empowering Non-Experts in Robotics}

By allowing end-users non-expert in robotics to teach a robot how to interact, \gls{sparc} provides an opportunity for anyone to personalise their robot. If combined with efficient interfaces and learning algorithms, approaches such as \gls{sparc} have the potential to democratise the use of robotics by allowing anyone to teach a robot to interact efficiently in a wide range of domains. Robot developers and designers can use this learning ability to deploy robots as \emph{blank slate}, with just a way to perceive the world, act on it and interact with a teacher, and let their behaviour be defined by their users. These users would start filling this blank slates, creating their own robot behaviour, teaching their robot how to fulfil their personal needs.

\subsection{Robots and Proactivity}

Another way to interpret \gls{sparc} is as a way to provide proactivity to a robot. By using \gls{ml} and proposing to execute actions, the robot is actually taking the initiative to do an action without executing it straight-away. For instance, a proactive robot assistant would anticipate its user's needs and desires and would reduce their workload. By having the capability to learn new actions, or what action it should do, such a robot would be an adaptive companion able to solve a large quantity of tasks. Finally by informing the surrounding humans of its actions, a robot assistant would only execute action deemed useful by their users.

\subsection{Ethical Questions} \label{sec:disc_ethics}

Having robots interacting in human environment and allow any human to teach them, raise multiple ethical questions.

The first one concerns people's jobs. Throughout robots and machines history, many jobs have been automated and more are expected to disappear in the next years~\citep{frey2017future}. As such, deploying robots in social environments, such as education or care facilities, might lead teachers or social workers to fear for their jobs. However, in many of such social environments with no direct quantifiable return on investment, workforce is already lacking (e.g. nurses in the US; \citealt{nevidjon2001nursing}) and this shortage is expected to grow in the future. Consequently, robots provide an opportunity not to replace a workforce already in shortage of workers, but on the other end to support these workers in their job, making these jobs safer and more pleasant for the workers or providing additional support for the clients or patients~\citep{wada2005psychological}. However, the \gls{hri} community as a whole needs to be aware of these fears and ensure by their work that robots have a positive impact on society.

The second question is privacy. As robots will interact with the general population, and especially will learn from and about them, issues concerning privacy arise. To have meaningful interactions with users, robots need to collect information about them. And the type of information collected, the storage and the use by third parties has to be carefully considered before deploying a robot. This effect is further increased when robots learn from humans, robots are not any more passively collecting data, but the interaction itself aims to gather more information about the user, about their preferences, their desires and their needs. Finally, this effect is amplified when interacting with vulnerable populations. As robots are expected to take an important role in education and care, humans interacting with them will tend to children or patient, and as they might not be able to ensure their privacy. The question of sharing these information and this knowledge between robots and beyond, to the manufacturer or to the governments, needs to be addressed before robots are deployed on large scales.

A last concern resides in the responsibility for actions executed by the robot. In a mixed-initiative interaction when both the autonomous agent and the human supervisor can impact the robot action policy, the responsibility of actions is complex to analyse. This effect is even increased when the robot can learn from its user. In that case, the role of the company or the entity distributing this robot and the role of the user have to be considered when looking for a legal entity accountable for the robot actions. To have this clearer accountability, \gls{ai} applied for robotics needs to be more transparent~\citep{wachter2017transparent}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Future Work}
The work conducted in the thesis explored how robots could be taught to interact with humans and proposed a novel interaction framework, \gls{sparc}, to enable such a learning. However, \gls{sparc} could be extended in many ways and its principles applied to other applications.

\subsection{Application Domains}

Through this thesis, \gls{sparc} has only been applied to \gls{hri} in the context of tutor robots, to teach them to support child learning. However, the principles underlying \gls{sparc} could be applied to a much wider range of applications in robotics and other \gls{ai}. In robotics, \gls{sparc} would show promises in numerous fields in social \gls{hri}: from assistant robot at home to collaborative robotics including robots in hospitality, military or industry and this applicability could be extended to other robotic applications. For example a robot could learn the preferences of a user and act as an embodied personal assistant, connected to devices in the house, calendar on the internet and supporting its users in routine tasks. Such a robot could learn to anticipate its user's needs and propose to provide support proactively. In \gls{hrc}, similarly to the work presented in \cite{munzer2017efficient}, a robot could learn its partner preferences, informing them of its actions and help them to complete the task faster. As explained in \cite{feil2005defining}, \gls{iml} approaches, and by extend \gls{sparc}, could also be applied to classification tasks, maintaining the user informed about the state of the algorithm's knowledge and involving them in the learning process. For example, for semi-supervised image classification, the algorithm could automatically present a subset of classified images between learning steps to the user who could step in when a misclassification happen. In this case, where incorrect actions have limited impacts, the correction can happen in hindsight, as proposed in \cite{chernova2009interactive}. Alternatively, the principles of \gls{sparc} could be also used to support agents using \gls{rl} in the real world. The supervisor could provide a safeguard preventing the agent to make errors, bringing it back to the correct parts of the environment or guiding the agent to relevant actions or features in complex environments.

%\subsection{Multiple Teachers}

\subsection{Learning Beyond Imitation}
\ES{rephrase}
Another potential feature of \gls{sparc}, and other methods based on demonstrations, not evaluated in this research is reaching capabilities beyond the demonstrations. \gls{sparc} uses demonstrations and corrections from a human teacher to learn. By applying \gls{sl}, the optimal outcome would be to match the teacher's performance. However, if the algorithm learn a value function or the teacher's goal, instead of reproducing the teacher policy, the agent could improve its action policy around the demonstrated policy and potentially become better than the teacher themselves. This capability has been reached in \cite{abbeel2004apprenticeship}, by using Inverse Reinforcement Learning. In their work, the agent learned a reward function and a basic action policy from the human demonstrations and then, by applying \gls{rl} around the demonstrated policy, the agent improved it beyond the demonstrations and reached super-human capabilities. Similarly to their methods, using the shared control provided by \gls{sparc}, a robot could interact under supervision, progressively proposing action to the supervisor. And the teacher could still direct the exploration, preventing the robot to reach dangerous parts of the world, while being open to better propositions than the ones expected. An example could be an agent learning a game such as Go with a combination of exploration in simulation and action in the real world under supervision. The agent could optimise the policy offline and propose move to the human. Based on these suggestions, the human could correct them or accept them. That way, the agent could propose moves the human would not have thought about, and then the human could let them be tried or if they appear to risky, override them. This approach: mixing optimisation in simulation and actions under supervision in the real world could lead to efficient learning while ensuring that the executed behaviours are correct and be applied to a wide range of other problems such as navigation, manipulation or \gls{hri}. %Alternatively, the agent could learn in simulation, only using the human supervision when acting in the real world to prevent potential mistakes.

However, to reach these super-human behaviours, the agent requires a way to learn in addition to the human. The agent needs to have access to a second level learning, such as a reward function directly from the environment or learnt from the human demonstrations (such as with Inverse Reinforcement Learning). For example, the human could provide a mixture of demonstration, rewards and high levels goals which could be used to learn such a reward function.

\subsection{Sustained Learning}

\ES{could be easily improved}

In the two first studies presented in this research, the robot learn an action policy from a short term interaction (inferior to 25 minutes). In contrast, robots deployed to interact with humans on a daily basis will learn over much longer periods of time. This aspect, repeated learning, has been partially explored in Chapter~\ref{chap:tutoring}, where the robot learned through 25 sessions. While progressing toward repeated learning, this study did not explore the challenges of life long learning~\citep{thrun1995lifelong}. 

In the implementation of \gls{sparc} in Chapter~\ref{chap:tutoring}, the temporal aspects of the interaction were taken into account only by including some time decay in the state definition. However, to sustain continuous long term interaction, spanning multiple hours or days, the dependence in time of the action policy will have to be taken into account through other means, as explored by the STRANDS project~\citep{hawes2017strands}, which deployed a learning robot for week-long continuous autonomous operation in human environments. Additionally, the learning algorithm would have to scale to handle a constantly increasing number of datapoints (a feature not supported by instance based learning).

\ES{Learning from humans but also when they are not here to teach}

\subsection{Teacher Interface}

Another axis to improve \gls{sparc} and which is critical for to reach other applications is the interface with the teacher. As mentioned in the previous section, one of the main limitation of \gls{sparc} is also what provides it its strength: the inclusion of a human in the action selection process. Including this human and giving them the opportunity to preempt and select any actions comes with limits on the interaction. The robots needs to communicate its intentions and the human needs enough time to correct them before they impact negatively the environment or other humans interacting with the robot. %Despite these limits in time ---

However, the interface used by the teacher to control the robot could mitigate these limitations, and further work could explore how to provide the best communication between the teacher and the robot. For example, in the case of a \gls{gui} on a tablet or a phone, the interface could combine buttons and a representation of the world where the robot could describe how it plans to act or its expected trajectories. Similarly, the teacher could use this representation of the world to select actions for the robot to execute (as used in Chapter~\ref{chap:tutoring}). Designing these \gls{gui} for \gls{sparc} could use the knowledge obtained from designing application for phone for example~\citep{joorabchi2013real}. Alternatively, future work could explore how natural language could be used to control a robot through \gls{sparc}. This would raise many challenges, such as natural language understanding, or creating a string to describe the robot's actions, intentions or explanation clearly yet concisely~\citep{hayes2017improving}. Despite these challenges, language possess the qualities required to communicate between a teacher and a robot: high bandwidth, familiarity for humans and open-endedness of description for example. As such, combining language and \gls{sparc} would be an interesting future work of this research.

\subsection{Algorithms}

In the future, \gls{sparc} should also be combined with richer learning algorithms. The three examples provided in this thesis represent only a small part of the algorithms \gls{sparc} can be combined with. More advanced \gls{ml} would allow \gls{sparc} to learn faster and more efficiently; and, as mentioned previously, potentially reach super-human performance in the task. However, as mentioned in Chapters~\ref{chap:background} and~\ref{chap:sparc}, many challenges remain when using learning for \gls{hri}. The first one, the main one tackled by \gls{sparc}, is the high stakes of the interaction, incorrect errors might lead to disastrous consequences. In addition, the data efficiency is fundamental: when interacting with humans, collecting information about human's reaction can be costly or take a large amount of time. As such, each datapoint should be used with high efficiency and the human included in the loop could provide additional knowledge to deal with this scarcity of data. Alternatively, the algorithm could combine data accumulated from different teachers to learn a more general policy. However, this would lead to a transfer problem, assumptions and policies correct with one user might not be valid anymore for another. And doing this generalisation might decrease the potential for personalisation that \gls{ml} provide. One way to address this personalisation vs generalisation issue is to group people by similarity and learn to detect the group of a person and an action policy adapted to this group to reach a better action policy~\citep{brunskill2014pac}. Alternatively, the robot could learn or already possess a general action policy and then use \gls{sparc} to refine it and adapt it to its user. 

\gls{sparc} and humans in general could also be used to teach hierarchical strategies~\citep{botvinick2012hierarchical}. By helping to create subpolicies and informing the agent which policy is more appropriate, a human could allow an agent to learn to solve complex task much quicker. Additionally, as mentioned earlier, \gls{sparc} could be combined with \gls{rl} in simulation, using the human only to ensure a correct behaviour in the real world, while including the humans commands back in the simulation to improve the learning process.

Another challenge that algorithms used with humans need to take into account is that humans are not static entities. As mentioned in Section~\ref{ssec:back_feedback}, different humans will use different teaching strategies. Furthermore, as seen in Chapters~\ref{chap:woz},~\ref{chap:control} and~\ref{chap:tutoring}, in \cite{thomaz2008teachable} and \cite{macglashan2017interactive}, even a single human will adapt their teaching strategy overtime. Human policies and feedback are moving targets, and algorithms used to learn from humans need to take into account this variations and evolutions of behaviours.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Summary} \label{sec:disc_summary}

%\ES{I should probably state the things - difference with introduction}

This chapter started by presenting the main limitations identified for \gls{sparc} (requirement of a \gls{cw} and attention from the teacher, potential increase of workload on the teacher, complexity of the interface, low number of datapoint and slower interaction). Each limitation was described and we presented how they could be addressed when designing an interaction involving a human teacher. We then addressed the research questions identified in Section~\ref{sec:intro_thesis} and continued with discussing the potential impacts of \gls{sparc} on the wider field of \gls{hri} and potential for deploying robots in the real world, as well as the ethical questions raised by having humans teach robots. Finally, we proposed directions to extend the efficiency and applicability of \gls{sparc} that could be addressed in future work.
