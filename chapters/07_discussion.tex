%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Discussion} \label{chap:discussion}
\glsresetall

Chapter~\ref{chap:background} put the light on the absence of robot controllers today providing adaptivity to a robot with a low workload from humans and while ensuring that the robot's behaviour is constantly appropriate. Based on this observation, Chapter~\ref{chap:sparc} presented the \gls{sparc}, an interactive teaching framework designed to allow robots to learn a social interactive behaviour by being supervised by humans. Then Chapters~\ref{chap:woz},~\ref{chap:control} and~\ref{chap:tutoring} evaluated this approach in three studies, the last of which evaluated \gls{sparc} in real \gls{hri} consisting of a learning activity involving 75 children. Combined together, these chapters sought support for the thesis of this research:

\begin{quote}
	A robot can learn to interact meaningfully with humans in an efficient and safe way by receiving supervision from a human teacher in control of the robot's behaviour.
\end{quote}

This chapter combines the results from the different studies presented in this research to discuss the findings. It starts by presenting the limitations of the approach proposed in this work, \gls{sparc}. Then, Section~\ref{sec:disc_rq} presents how the three studies executed in this research work answered the research questions raised in Chapter~\ref{chap:intro}. Section~\ref{sec:disc_impact} discusses the more general impact this research may have and the ethical questions raised by teaching robots to interact.
%ethical questions raised when humans teach robots to interact with humans will be discussed in Section~\ref{sec:disc_ethics}, and Section~\ref{sec:disc_impact} will presents the potential impacts of this research. 
Finally, the last section proposes axes where \gls{sparc} could be extended to increase our knowledge in how humans teach robots and improve \gls{sparc}'s usability and application to \gls{hri}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\section{Limitations} \label{sec:disc_limitations}

\section{Limitations of SPARC} \label{sec:disc_limitations}

\gls{sparc}, as presented in Chapter~\ref{chap:sparc}, is a promising framework to teach robots to interact with humans or in high stake environments, but presents several limitations restricting the range of domains it could be applied to.

\subsection{Requirement of a Human in the Loop}
The first of these limitations is the requirement of a human to supervised the robot, even once it has learn an action policy. As stated earlier, \gls{sparc} aims to move away from \gls{woz} or other teleoperation methods by learning an efficient action policy from the human commands. In its original framing, \gls{sparc} did not aim to create a fully autonomous agent behaving without supervision, but to smooth the teaching phase and the use phase into one single interaction using \gls{sa}. In this single phase, the workload on the supervisor would decrease as the robot learns while a high performance in the domain application would be maintained due to the control provided to the teacher. However, \gls{sa} still requires a human involved in the supervision and as such presents limited applicability to fields where robots are expected to fill gaps in human workforce or to reduce the requirements on humans. 

Despite not being the original goal, \gls{sparc} can still be used to create a fully autonomous behaviour (as demonstrated in Chapter~\ref{chap:tutoring}). In that case, the training process would be similar to \gls{lfd}, with a training phase using \gls{sparc} and then the testing/deployment phase of the autonomous behaviour. However, by using \gls{sparc} two differences remain. First, during the training phase, instead of passively receiving commands from the teacher, the robot would proactively make suggestions to the teacher. As presented in Section~\ref{sec:tutoring_opportunities} this aims to reduce the workload on the teacher during the training phase, provide more datapoints for the learning and inform the teacher about the state of robot's knowledge, potentially creating trust between the robot and its teacher. Second, even after being deployed to interact autonomously, the teacher could still step back in control using \gls{sparc} again to refine the action policy. Alternatively, if a different behaviour has to be applied (e.g. if the robots interact with a child with special needs rather than a typical one), the teacher can take over using \gls{sa} to ensure a personalised experience for this specific interaction.

\subsection{Reliance on Human's Attention}
\ES{reread}
A second limitation of \gls{sparc} corresponds to the constant need for the human's attention and the presupposition that human teachers will always ensure an appropriate robot behaviour if given the opportunity. Throughout this research, we assumed that even if the robot behaviour may be mostly correct, the supervisor would be attentive to the robot suggestions and ready to correct them at any time. This assumption is similar to autonomous cars using a safety driver or Tesla Autopilot requiring constant human supervision. The agent is fully autonomous but may make mistakes and as such, a human needs to be ready to correct these errors before they impact the world. However, as demonstrated by the accidents in 2017 and early 2018 involving these supervised autonomous vehicles, this assumption is often violated, and a short moment of inattention may have dire consequence\footnote{Cf. the Tempe and Mountain view accident reported by the media in early 2018.\label{foot:disc_danger}}. By observing a seemingly correct agent's behaviour for an extended period, the human supervisor might start to overtrust the agent, missing the occasion to react in time to anticipable errors potentially leading to frustration or death in the case of autonomous vehicles\footref{foot:disc_danger}. However, some ways exist to mitigate this limitation but have not been applied to autonomous driving or general \gls{iml}. For example, with \gls{sa} the agent informs in advance the supervisor about its actions, similarly a car could display the planned trajectory. This would provide the supervisor with more time to analyse the situation and potentially allowing them to react in time. Alternatively, the agent could communicate a lack of confidence in its actions or its interpretation of the environment, informing the supervisor that attention is specially required in that moment.

\subsection{Time Pressure}
\ES{check}
As pointed already in Section~\ref{ssec:sparc_time}, with \gls{sparc}, the presence of the \gls{cw} and the auto-execution of actions may lead to issues. To be applicable and make use of the auto-execution of actions as a way to reduce workload, the \gls{vw} of an action needs to be wider than its \gls{cw}. That way, actions approved passively are still valid when executed. To increase the application of \gls{sparc} to a wider range of situations, the \gls{cw} has to be as narrow as possible. In contrast, the supervisor needs a \gls{cw} as wide as possible, to provide them with enough time to process the action and cancel it if required. Correction windows too narrow would put additional pressure on the supervisor to react in time or even prevent them to avert undesired actions. %To cope with this additional time pressure, the supervisor might find ways to prevent the suggestions of actions or simply refuse each action proposed by the robot. 
This results in two effects having opposite requirements on the \gls{cw}. However, while a longer \gls{cw} would only limits \gls{sparc}'s applicability to some situation, one too short could have negative consequences. As such, this need of a \gls{cw} wide enough to allow the teacher to react is probably on of the main limits of \gls{sparc} as it includes a significant delay in the robot's actions. 

\ES{reread}
However, this requirement of a \gls{cw} wide enough for the teacher can be mitigated in multiple ways. For example, instead of communicating the action the robot is about to do at this instant, the robot could communicate a plan where each step is progressively executed. That way, the teacher is informed beforehand of the next $n$ and can anticipate their impact and react to all the actions instead of the next one only. Alternatively, the robot could adapt the \gls{cw} to its confidence in its action policy: actions the robot is unsure about would be given more time to be corrected. Or, each type of action could have a dedicated value for the \gls{cw}, for example actions needing to be executed quickly (such as emergency breaking for example) would have a much shorter \gls{cw}. Finally, a last possibility could be to allow the teacher to manually select the duration of this \gls{cw}, and so be in control of the pace of the interaction.

\subsection{Overloading the Teacher}

\ES{?}

By deciding to have the robot learning online an action policy and proposing actions to the teacher, we forced the teacher to monitor at the same time two autonomous agents: the human target and the robot. This requirement might lead to another risk with \gls{sparc}: overloading the teacher with suggested actions. If the teacher has to correct more actions than they would have selected, their workload is not reduced but increased. While still providing useful information for the learning algorithm (and more than only the actions selected by the supervisor), this supplementary workload on the teacher is not desired. As explained before, this could lead to erroneous teacher's behaviour hindering the learning and potentially increasing the risks in the interaction. For example in the study presented in Chapter~\ref{chap:tutoring}, some times, the teacher just cancel actions as soon as they arrived, even before she had time to evaluate the action, which probably limits the efficiency of the learning algorithm. This indicates that the algorithm needs to adapt the rate of suggestion to the pace of the interaction not to overload the teacher.

\subsection{Interface}\label{sec:disc_lim_interface}
\ES{check if makes sense}
The interface between the teacher and the robot is key when applying \gls{sparc} or other \gls{iml} methods. Approaches using only feedback require a single scalar to evaluate actions and the communication needs to be only one way: from the teacher to the robot. On the other hand, to provide full control and accountability on the robot's actions, \gls{sparc} requires both ways communication. First, the teacher needs to receive inputs from the robot, such as its intentions, to decide if the action is valid or not. Second, the teacher needs to send information to the robot: feedback about the intentions, to preempt actions if required, but also demonstrations, to select actions for the robot to execute. As the robot may have access to hundreds of actions, assigning one button per action is not feasible, other ways of commanding the robot need to be found. Furthermore, as mentioned in Chapter~\ref{chap:sparc}, with \gls{sparc} the teacher can also provide additional information to the algorithm to speed up the learning. In summary, the interface between the robot and the teacher needs to provide the teacher with the robot's intention, allow the teacher to preempt actions, select any action and provide additional information to the learning algorithm. An interface providing all these features can easily be bloated and difficult to use by humans, increasing even more the required workload to control and teach the robot. For applying \gls{sparc} to complex environments, efforts need to be invested in the interface to make it intuitive and clear. 

For instance, in Chapter~\ref{chap:tutoring}, the \gls{gui} used by the teacher represented the current state of the game, with some buttons for accessing a subpart of the action space, but the majority of actions was inferred by the way the teacher move items on the screen and the items selected. An alternative way could be to use natural language. Humans are expert in using natural language to communicate and the open-endedness of this tool makes it suited for applications of \gls{sparc} where the teacher could speak and the time required to vocalise commands is not critical, such as a robot assistant at home.

\subsection{Learning in the Real World}

\ES{?}

As justified in Chapter~\ref{chap:background}, today, there is no simulator of humans and social norms precise enough to be used to train robot to interact with humans. Consequently, learning for \gls{hri} has to happen in the real world. In addition to this \emph{space} constraint, learning from humans also adds a \emph{time} constraint. By including a human in the action selection loop, the interaction needs to go at a human pace. This implies that gathering datapoints through \gls{sparc} is a relatively slow process. As such, algorithms using \gls{sparc} need to be data efficient, be able to learn from a low number of datapoint and generalise quickly. However, by including the human in the loop and learning in the real world, we can have important additional features. First, the points accumulated are sure to be relevant to the learning process: they come from the desired interaction and as they have been validated by a human, their label must be correct. And secondly, the human can also provide additional information on the selection to quicken the learning (as implemented in Chapter \ref{chap:tutoring}).
%\subsection{Experimental Limitations} \label{sec:disc_experiments} 
%\ES{Need to be sure that all these limitations are mentioned in the corresponding parts}
%Similarly to the method proposed in this research, the studies evaluating \gls{sparc} also presented some limitation. 

%The first study presented in Chapter~\ref{chap:woz} used participants familiar with robots. One could argue that this population is not representative of the general population expected to interact with robots. However, and as explained in that chapter, in many case in \gls{hri}, the wizards teleoperating robots are trained experts knowing the robot used in the interaction and how to control it. Thus, the participants involved in that study are representative of a significant part of the population expected to supervise a robot to interact with other humans. Additionally, the small sample of the study (N=10 for a within participants design) explains why no significance testing have been done in the study. However, valuable information have been collected and helped us to design the two other evaluation of \gls{sparc}. A last potential limitation of the evaluation relates to the discretisation of time. In this study, the \gls{woz} condition was not a real \gls{woz} as the participants needed to select an action (or accept a random action) at each step. However, this made the two conditions more comparable and the amount of workload on the supervisor in the \gls{woz} condition should have been similar to a real \gls{woz}.
%\ES{low number of datapoint and slower interaction}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Research Questions} \label{sec:disc_rq}
This section will revisit the research questions identified in Section~\ref{sec:intro_thesis} and explain how the work presented in this thesis addressed them.
\begin{itemize}
\item [RQ1] \textbf{What are the requirements of a robot controller to ensure a behaviour suited to \gls{hri}?} 
Based on a review of the different fields of application of social \gls{hri}, we defined three requirements a robot controller should follow to ensure an efficient interaction. First and foremost, the robot's behaviour needs to be constantly appropriate: as robots often interact with vulnerable populations, their behaviour needs to be constantly safe for the humans they interact with. Secondly, the robot should be adaptive, be able to generalise to unexpected situations, but also personalise its behaviour to the different humans it interacts with and be able to learn, improving and extending its action policy. Finally, the robot needs to be as autonomous as possible, or at least require a low human workload to interact in the world.

\item [RQ2] \textbf{What interaction framework would allow a human to teach a robot while validating the requirements from RQ1?}
To validate the three principles expressed as answer to RQ1, we proposed \gls{sparc}, a new teaching framework for robots which provides control over the robot's actions to a teacher and use this control to learn in a safe way, validating the first and second requirements. Secondly, by allowing the teacher to passively accept the robot's propositions, we aim to decrease the workload on the teacher over time and progressively provide the robot with autonomy. 

\item [RQ3] \textbf{Could a robot decrease its supervisor's workload by learning from their supervision?}
Study 1 showed that providing a supervised robot with learning can reduce the workload on its supervisor. Furthermore, study 2 demonstrated that, compared to other methods, \gls{sparc} is an efficient way to enable a safe teaching and requires a comparatively low workload.

\item [RQ4] \textbf{How providing the teacher with control over the learner's actions impacts the teaching process?} 
Results from study 1 and 2 indicate that by informing the teacher in advance of its actions, the robot ensures that its final behaviour is correct. This implies that even in early phases of the learning, when the robot behaviour is not adequate yet, the teacher can prevent the robot's lack of knowledge to negatively impact the world. Furthermore, this control helps the teacher to steer the robot toward useful parts of the environment and a good policy, making the teaching faster, safer, more efficient and lighter (as requiring a lower workload) than other methods lacking control.

\item [RQ5] \textbf{How teaching a robot to interact socially impacts the two humans involved in the overall triadic interaction?}
When being used to teach a robot to interact with a human, \gls{sparc} does allow the robot to constantly have an efficient behaviour (as demonstrated by the improvement of children's behaviours in the supervised condition in Chapter~\ref{chap:tutoring}). However, this performance in the application interaction might come with a cost for the teaching interaction. As the teacher needs to react to the robot's suggestions, they do have to monitor a second autonomous agent, which might lead to a heavier workload than other supervision methods.

\item [RQ6] \textbf{After receiving supervision from a human, could a robot behave autonomously in a social context?}
In Chapter~\ref{chap:tutoring}, we used \gls{sparc} to teach the robot in the supervised condition, and then deployed the robot to interact autonomously. During this autonomous interaction, the robot applied a policy similar to the demonstrated one, and the impact on the children's behaviour was close to the one in the supervised condition. Consequently, in this study, the robot could behave socially in an autonomous fashion after having been supervised by a human in a learning phase.

\end{itemize}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Impact} \label{sec:disc_impact}

\ES{Make better!}

\subsection{Teaching Robot to Interact with Humans}

With \gls{sparc} we proposed a new way to teach agents. By following the principles presented in Section~\ref{sec:sparc_principles} (combining \gls{sa} and \gls{ml}), this method provides a robot with an adaptive policy while ensuring that its behaviour remains constantly appropriate. Furthermore, with the combination of proposition, correction and selection of actions, \gls{sparc} aims to reduce the workload on the teacher. Hence, this approach is fit to control and teach robots to interact with humans as it follows the requirements presented in Section~\ref{ssec:back_constraints}. Furthermore, as demonstrated in Chapter~\ref{chap:tutoring}, \gls{sparc} has been used to teach a robot to interact safely and efficiently in a real \gls{hri}. 

By demonstrating its applicability to teach robots to interact with humans from in-situ supervision, \gls{sparc} opens up new opportunities to provide robots with action policies complex to define or not known in advance. This new method to teach robot safely to interact with humans might allow robots to be deployed in new contexts where they are absent today.

%; therefore, with \gls{sparc} the resulting autonomous behaviour can be adapted to each situation and represents the teacher's desired policy. %\gls{sparc} can be a way to allow non-experts in robotics to personalise their robot, to have it learn to behave in the way they each desire.

\subsection{Empowering Non-Experts in Robotics}

By allowing end-users non-expert in robotics to teach a robot how to interact, \gls{sparc} provides an opportunity for anyone to personalise their robot. Combined with efficient interfaces and learning algorithms, approaches such as \gls{sparc} have the potential to democratise the use of robotics by allowing anyone to teach a robot to interact efficiently in a wide range of domains. Robot developers and designers could use this learning ability to deploy robots as \emph{blank slate}, with just a way to perceive the world, act on it and interact with a teacher, and let their behaviour be defined by their users. These users would start filling these blank slates, creating their own robot behaviour, teaching their robot how to fulfil their personal needs. 

As demonstrated in Chapter~\ref{chap:control}, even when using the same algorithm and state representation, \gls{sparc} can teach different behaviours adapted to the teacher's strategy. As different teachers have various preferences, they will have different expectations for the robot's behaviour. And \gls{sparc} could be a way to allow each user to teach their robot a personalised interaction behaviour.


\subsection{Robots and Proactivity}
\gls{bad}
Another way to interpret \gls{sparc} is as a way to provide proactivity to a robot. By using \gls{ml} and proposing to execute actions, the robot is actually taking the initiative to do an action without executing it straight-away. For instance, a proactive robot assistant would anticipate its user's needs and desires and would propose help or services without having to be asked. This capability has two uses: first it allows robot users not to have to ask supportive behaviour for the robot every time they need it and second, it means that even if the user forgets to ask the robot, the robot itself could remind it to the user by proposing to help.

By having the capability to learn new actions, or what action it should do, such a robot would be an adaptive companion able to support its user in a large quantity of task. Finally by informing the surrounding humans of its actions, a robot assistant would only execute action deemed useful by their users.

\subsection{Ethical Questions} \label{sec:disc_ethics}
%\ES{add more ref to support the questions}
Having robots interacting in human environment and allowing any human to teach them, raise multiple ethical questions~\cite{lin2014robot}.

The first one concerns people's jobs. Throughout robots and machines history, many jobs have been automated and more are expected to disappear in the next years~\citep{frey2017future}. As such, deploying robots in social environments, such as schools or care facilities, might lead teachers or social workers to fear for their jobs. However, in many of such social environments with no direct quantifiable return on investment, workforce is already lacking (e.g. nurses in the US; \citealt{nevidjon2001nursing}) and this shortage is expected to grow in the future. Consequently, robots provide an opportunity not to replace a workforce already in shortage of workers, but on the other end to support these people in their job, making these jobs safer and more pleasant for the workers or providing additional support for the clients or patients~\citep{wada2005psychological}. However, the \gls{hri} community as a whole needs to be aware of these fears and ensure by their work that robots have a positive impact on society and communicate their vision of robots helping the human population.

By allowing robots to learn, we might increase the range of places where they are being used. However, having robots interacting with vulnerable populations such as elderly people or children in school raise multiple questions. \cite{sharkey2012granny} identify that using robots in elder care might ``reduce the amount of human contact'', ``increase the feelings of objectification'', create ``a loss of privacy'' and ``personal liberty'' and elicit ``deception and infantilisation''. They also add the the conditions were an elderly should be in control of a robot are to be carefully identified. Similar, \cite{sharkey2016should} expresses concerns about deploying robots in classrooms. As such, roboticists need to work with domain experts to ensure that robots do help their user and not harm them.

A major ethical questions concerning learning robots is privacy. As robots will interact with the general population, and especially will learn from and about it, issues concerning privacy arise. To have meaningful interactions with users, robots need to collect information about them. And the type of information collected, the storage, the use by third parties and the users' perception about this collection have to be carefully considered before deploying a robot~\citep{syrdal2007he}. This effect is further increased when robots learn from humans. These learning robots are not any more passively collecting data, but the interaction itself is designed to gather more information about the user, about their preferences, their desires and their needs. Finally, this effect is amplified when interacting with vulnerable populations. If robots take an important role in education and care, humans interacting with them will tend to be children or patient, and these people might not be able to ensure their privacy alone. The question of sharing these information and this knowledge between robots and beyond, to the manufacturer or to the governments, needs to be addressed before robots are deployed on large scales. Another ethical question linked with privacy is security. These data accumulated by robots need to be protected from malicious attacks. This issue is even more visible with the recent world wide hacks of the Internet of Things devices (such as Mirai\footnote{\url{https://en.wikipedia.org/wiki/Mirai_(malware)}}). Recently, \cite{giaretta2018adding} present a report of numerous basic security flaws in the Pepper robot and express the idea that robot have moved too quickly from research to market product and that they often do not present the required security to ensure they users' privacy and security.

A last concern resides in the responsibility for the actions executed by the robot~\citep{asaro2007robots}. In a mixed-initiative interaction when both the autonomous agent and the human supervisor can impact the robot action policy, the responsibility of actions is complex to analyse. This effect is even increased when the robot can learn from its user. In that case, the role of the company or the entity distributing this robot and the role of the user have to be considered when looking for a legal entity accountable for the robot actions. To have a clearer accountability, \gls{ai} applied for robotics needs to be more transparent~\citep{wachter2017transparent}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Future Work}

\ES{overlap with limitation...}

The work conducted in this thesis explored how robots could be taught to interact with humans and proposed a novel interaction framework, \gls{sparc}, to enable such a learning. However, \gls{sparc} could be extended in many ways and its principles applied to other applications.

\subsection{Application Domains}

Through this thesis, \gls{sparc} has only been applied to \gls{hri} in the context of tutor robots, to teach them to support child learning. However, the principles underlying \gls{sparc} could be applied to a much wider range of applications in robotics and other \gls{ai}. In robotics, \gls{sparc} would show promises in numerous fields in social \gls{hri}: from assistant robot at home to collaborative robotics including robots in hospitality, military or industry and this applicability could be extended to other classical robotic applications. For example a robot could learn the preferences of a user and act as an embodied personal assistant, connected to devices in the house, calendar on the internet and supporting its users in routine tasks. Such a robot could learn to anticipate its user's needs and propose to provide support proactively. In \gls{hrc}, similarly to the work presented in \cite{munzer2017efficient}, a robot could learn its partner preferences, informing them of its actions and helping them to complete the task faster and easier. As explained in \cite{fails2003interactive}, \gls{iml} approaches, and by extend \gls{sparc}, could also be applied to classification tasks, maintaining the user informed about the state of the algorithm's knowledge and involving them in the learning process. For example, for semi-supervised image classification, the algorithm could automatically present a subset of classified images between learning steps to the user who could step in when a misclassification happen. In this case, where incorrect actions have limited impacts, the correction can happen in hindsight, as proposed in \cite{chernova2009interactive}. Alternatively, the principles of \gls{sparc} could be also used to support agents using \gls{rl} in the real world. The supervisor could provide a safeguard preventing the agent to make errors, bringing it back to the correct parts of the environment or guiding the agent to relevant actions in complex environments.

%\subsection{Multiple Teachers}

\subsection{Learning Beyond Imitation}\label{sec:disc_beyond}
\ES{check}
Another potential feature of \gls{sparc}, and other methods based on demonstrations, not evaluated in this research is reaching capabilities beyond the demonstrations. As \gls{sparc} uses demonstrations and corrections from a human teacher to learn, by applying \gls{sl}, the optimal outcome would be to match the teacher's performance. However, if the algorithm learn a value function or the teacher's goal, instead of reproducing the teacher policy, the agent could improve its action policy around the demonstrated policy and potentially become better than the teacher themselves. This achievement have been accomplished by \cite{abbeel2004apprenticeship}, by using Inverse Reinforcement Learning. In their work, the agent learned a reward function and a basic action policy from the human demonstrations and then, by applying \gls{rl} around the demonstrated policy, the agent improved it beyond the demonstrations reaching super-human capabilities. 

Alternatively, instead of having the robot reaching capabilities beyond human ones on its own, a human-robot team could also together reach this kind of performance. For examples, \cite{kasparov2010chess} propose ``Advance Chess'', a new type of chess were players have access to a computer to help them during their decisions. This combination of human and machine, aims at profiting from the best of both worlds and would allow humans to make better use of their intuition and creativity while using computer's certainty to save human calculations. Similarly, a learning agent could interact with the human in a mixed initiative framework, such as \gls{sparc}, where the agent could suggest actions (such as moves in a Go or Chess game) and the human could accept them or refuse them. That way the human would be in control of the interaction, prevent errors from the artificial agent to have negative impact, while still being open to opportunities they did not anticipate. That way the team could reach together super-human capabilities, while ensuring with the presence of the human in control of the interaction that the behaviour would be at least of human performance. Another feature of this mixed initiative interaction, it that it provides the human with the opportunity to learn from the robot, if the robot proposes better than anticipated actions.

%Similarly to their methods, using the shared control provided by \gls{sparc}, a robot could interact under supervision, progressively proposing action to the supervisor. And the teacher could still direct the exploration, preventing the robot to reach dangerous parts of the world, while being open to better propositions than the ones expected.

%An example could be an agent learning a game such as Go with a combination of exploration in simulation and action in the real world under supervision. The agent could optimise the policy offline and propose moves to the human. Based on these suggestions, the human could correct them or accept them. That way, the agent could propose moves the human would not have thought about, and then the human could let them be tried or if they appear to risky, override them. 

%The approach proposed above goes further by mixing the initiative between both agents and having the computer continuously learning. This mixing of optimisation in simulation and actions under supervision in the real world could lead to efficient learning while ensuring that the executed behaviours are correct and be applied to a wide range of other problems such as navigation, manipulation or \gls{hri}. This could even further and potentially allow humans to learn online from the autonomous agent. %Alternatively, the agent could learn in simulation, only using the human supervision when acting in the real world to prevent potential mistakes.

However, to reach these super-human behaviours, the agent requires a way to learn in addition to the human. The agent needs to have access to a second level learning, such as a reward function directly from the environment or learned from the human demonstrations (such as with Inverse Reinforcement Learning). For example, the human could provide a mixture of demonstration, rewards and high levels goals which could be used to learn such a reward function or to update a planner with more correct sets allowing them to reach the goals faster. Alternatively, the robot could learn from the human and in simulation, while leaving the human in control when interacting in the real world and using the feedback from the human and the environment during these real interactions to refine the simulation.

\subsection{Sustained Learning}

\ES{could be easily improved - but how?}

In the two first studies presented in this research, the robot learned an action policy from a short term interaction (inferior to 25 minutes). In contrast, robots deployed to interact with humans on a daily basis will learn over much longer periods of time and with temporal periodicity. This aspect, repeated learning, has been partially explored in Chapter~\ref{chap:tutoring}, where the robot learned through 25 sessions. While progressing toward repeated learning, this study did not explore the challenges of life long learning~\citep{thrun1995lifelong}. In the implementation of \gls{sparc} in Chapter~\ref{chap:tutoring}, the temporal aspects of the interaction were taken into account only by including some notion of time since events in the state definition. The robot was not doing any temporal planning or explicitly taking time into account when behaving.% and using a feature based algorithm does not scale well with large quantity of data, limiting the efficiency in case of longer or more numerous teaching sessions. 

To sustain continuous long term interaction, spanning multiple hours or days, the dependency in time of the action policy has to be taken into account through other means. The robot needs to learn spacio temporal features relevant to the human's behaviours and expectations (cf. STRANDS project; \citealt{hawes2017strands}). For instance, a robot assistant at home should know its users are typically going to work every morning, except weekends and holidays. To sustain learning over long periods of time, robots needs algorithm that can scale well with large number of data and take into account the impact of time on the action policy.
%Additionally, such robots should be able to learn from their users, but also when they are absent, as it generally happens a major part of the day, but humans might still have expectations for the robot during that time.

\subsection{Interface With the Teacher}

Another axis to improve \gls{sparc} and which is critical for to reach other applications is the interface with the teacher. As mentioned in the Section~\ref{sec:disc_lim_interface}, one of the main limitation of \gls{sparc} is also what provides it its strength: the inclusion of a human in the action selection process. Including this human and giving them the opportunity to preempt and select any actions comes with limits on the interaction. The robots needs to communicate its intentions and the human needs enough time to correct them before they impact negatively the environment or other humans interacting with the robot. %Despite these limits in time ---

However, the interface used by the teacher to control the robot could mitigate these limitations, and further work could explore how to provide the best communication between the teacher and the robot. For example, in the case of a \gls{gui} on a tablet or a phone, the interface could combine buttons and a representation of the world where the robot could describe how it plans to act or its expected trajectories. Similarly, the teacher could use this representation of the world to select actions for the robot to execute (as used in Chapter~\ref{chap:tutoring} but including long-term information). Designing these \gls{gui} for \gls{sparc} could use the knowledge obtained from designing application for phone for example~\citep{joorabchi2013real}. Alternatively, future work could explore how natural language could be used to control a robot through \gls{sparc}. This would raise many challenges, such as natural language understanding, or creating a string to describe the robot's actions, intentions or explanation clearly yet concisely~\citep{hayes2017improving}. Despite these challenges, language possesses the qualities required to communicate between a teacher and a robot: familiarity for humans, open-endedness of description and precision for example. 

\ES{mention eeg error signal which would reduce the need of the correction window}
iturrate2010robot
gehring1993neural
\subsection{Algorithms}
\ES{read}
In the future, \gls{sparc} should be combined with richer learning algorithms. The three examples provided in this thesis represent only a small part of the algorithms \gls{sparc} can be combined with. More advanced \gls{ml} would allow \gls{sparc} to learn faster and more efficiently; and, as mentioned previously, potentially reach super-human performance in the task. However, as mentioned in Chapters~\ref{chap:background} and~\ref{chap:sparc}, many challenges remain when using learning for \gls{hri}. The first one, the main one tackled by \gls{sparc}, is the high stakes of the interaction, incorrect errors might lead to disastrous consequences. In addition, the data efficiency is fundamental: when interacting with humans, collecting information about human's reaction can be costly or take a large amount of time. As such, each datapoint should be used with high efficiency and the human included in the loop should provide additional knowledge to deal with this scarcity of data. Alternatively, the algorithm could combine data accumulated from different teachers to learn a more general policy. However, this would lead to a transfer problem, assumptions and policies correct with one user might not be valid anymore for an other. And doing this generalisation might decrease the potential for personalisation that \gls{ml} provide. One way to address this personalisation vs generalisation issue is to group people by similarity and learn to detect the group of a person and an action policy adapted to this group to reach a better action policy~\citep{brunskill2014pac}. Alternatively, the robot could learn or already possess a general action policy and then use \gls{sparc} to refine it and adapt it to its user. 

\gls{sparc} and humans in general could also be used to teach hierarchical strategies~\citep{botvinick2012hierarchical}. With hierarchical learning, agents learn subpolicies used to complete subgoals, and then combine these subpolicies to reach higher goals. By helping an agent to create subpolicies and informing it which ones are more appropriate in specific context, a human could allow an agent to learn to solve complex task much quicker. This teaching on multiple levels present a challenge for \gls{sparc}, as in the current implementation, the teacher only selection actions. This would require a way for the teacher to create higher level actions, organise them and switch between them. However, using a human provides a strong potential to quicken the learning of complex and rich policies.

%Additionally, as mentioned earlier, \gls{sparc} could be combined with \gls{rl} in simulation, using the human only to ensure a correct behaviour in the real world, while including the humans commands back in the simulation to improve the learning process.
Additionally, instead of providing demonstrations of a policy to follow, the teacher could also give symbolic rules defining a behaviour for the robot. This alternative way of teaching could generalise faster than simple demonstrations and might allow teachers to define a complex a complex behaviour quicker, and without having to encounter each situations to show the robot how to behave.

Another challenge that algorithms used with humans need to take into account is that humans are not static entities. As mentioned in Section~\ref{ssec:back_feedback}, different humans will use different teaching strategies. Furthermore, as seen in Chapters~\ref{chap:woz},~\ref{chap:control} and~\ref{chap:tutoring}, in \cite{thomaz2008teachable} and \cite{macglashan2017interactive}, human teachers adapt their teaching strategy overtime. Human policies and feedback are moving targets, and algorithms used to learn from humans need to take into account this variations and evolutions of behaviours.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Summary} \label{sec:disc_summary}

%\ES{I should probably state the things - difference with introduction}

This chapter started by presenting the main limitations identified for \gls{sparc} (requirement of a \gls{cw} and attention from the teacher, potential increase of workload on the teacher, complexity of the interface, low number of datapoint and slower interaction). Each limitation was described and we presented how they could be addressed when designing an interaction involving a human teacher. We then addressed the research questions identified in Section~\ref{sec:intro_thesis} and continued with discussing the potential impacts of \gls{sparc} on the wider field of \gls{hri} and potential for deploying robots in the real world, as well as the ethical questions raised by having humans teach robots. Finally, we proposed directions to extend the efficiency and applicability of \gls{sparc} that could be addressed in future work.
