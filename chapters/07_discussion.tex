%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Discussion} \label{chap:discussion}
\glsresetall

Chapter \ref{chap:sparc} presented \gls{sparc}, an interactive teaching framework designed to allow robots to learn a social interactive behaviour by begin supervised by humans. Then Chapters \ref{chap:woz}, \ref{chap:control} and \ref{chap:tutoring} evaluated this approach in three studies. The last study evaluated \gls{sparc} in real \gls{hri} and involved 75 children. These chapters seek support for the thesis of this research:

\begin{quote}
	A robot can learn how to interact meaningfully with humans by receiving supervision from a human teacher in control of the robot's behaviour, this supervision will lead to an efficient, safe and low human-workload teaching and autonomous behaviour.	
\end{quote}

This chapters gather results from the different studies presented in the research work to discuss the findings of this research, present the limitations of the approach presented, \gls{sparc} and the evaluation in the three studies. Ethical questions raised with teaching robots to interact with humans will be discussed in Section \ref{sec:disc_ethics}. And finally, the last section will present four axis where \gls{sparc} could be extended to increase its used and application to \gls{hri}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Limitations} \label{sec:disc_limitations}

\subsection{Limitations of SPARC}

\gls{sparc}, as presented in Chapter \ref{chap:sparc} presents several limitations restricting the range of domains it could be applied to.

The first of these limitations is the requirement of a human attentive to the interaction and able to correct the robot's actions before their executions. As stated earlier, \gls{sparc} aims to move away from \gls{woz} or other operation method by learning from the human commands and obtaining an efficient action policy. In its original framing, \gls{sparc} does not aim create a fully autonomous agent behaving without supervision, but to smooth the teaching phase and the use phase in one single interaction where the workload on the supervisor decreases as the robot learns. However, this type of interaction, \gls{sa}, still require a human involved in the supervision and as such presents limited applicability where robots are expected to fill gaps in human workforce to accomplish a task. However, while not being the original goal, \gls{sparc} can still be applied to create a fully autonomous behaviour, as demonstrated in Chapter \ref{chap:tutoring}. While the autonomous behaviour is different from the taught one, it still presents many similarities in the distribution of actions executed by the robot and the children's reaction to this behaviour. More training data and a better learning algorithm should even smoothen this difference of behaviour between the supervised and the autonomous robot. Furthermore, the concepts of \gls{sparc} and the \gls{sa} could still be applied for specific cases where a human supervisor needs to be included in the control process, such as when interacting with children with special needs for example.

A second limitation corresponds the attention of the supervisor. Throughout this research, we assumed that even if the robot behaviour is mostly correct, the supervisor would be attentive to the robot suggestions and ready to correct them at any time. This assumpltion is similarly to autonomous car using safety driver or the `AutoPilot' of Tesla requiring constant human supervision. The agent is fully autonomous but might make mistakes and as such, a human needs to be ready to correct these errors before they impact the world. However, as demonstrated by the few crashes in 2017 and early 2018 involving these partially autonomous vehicles, this assumption is often violated. By observing seemingly correct agent's behaviour, the human supervisor might start to overtrust the agent, missing the occasion to react in time to anticipable errors potentially leading to frustration or death in the case of autonomous vehicles. Some ways exist to potentially mitigate this limitation. For example, the agent could inform more in advance the supervisor about its actions, or expected trajectory. This would provide the supervisor with more time to react and might allow the human react in time, negating this effect. Alternatively, the agent could communicate a lack of confidence in its actions, or interpretation of the environment, informing the supervisor that attention is specially requested in that specific case.

Similarly, as pointed already in Section \ref{ssec:sparc_time}, with \gls{sparc}, the presence of the correction window and the auto execution of actions leads to several issues. To be applicable and make use of the autoexecution of actions as a way to reduce cognitive workload, the validity window of action should be wider than the correction windows. That way, actions approved passively are still valid when executed. Two main effects have different requirements on this correction window. On one hand, to improve the applicability of \gls{sparc}, these corrections windows need to be as narrow as possible. On the other, a correction window too narrow would put additional pressure on the supervisor to react in time. And as observed in Chapter \ref{chap:tutoring}, the supervisor might prefer to cancel every action coming just for not having to face the pressure to react in a short time. Alternatively, the supervisor might also find ways to prevent the suggestions of actions to reduce (as preparing an action in Chapter \ref{chap:tutoring}) this pressure. This limitation is probably on of the main limits of \gls{sparc}, the requirement of providing the teacher with enough time to correct undesired actions impacts the usability of \gls{sparc}. One way of mitigating it, could be to adapt the time window to the type of actions executed or to the algorithm's confidence in the validity of actions. As such, specific actions with a short validity window (such as emergency breaking for autonomous vehicles) could be executed in time; and actions the robot is unsure about would have more time to be corrected. 

Another potential issue is overloading the supervisor with suggested actions. If the supervisor has to correct more actions than it would have selected, the workload is not reduced but increased. While still providing useful information for the learning algorithm (and more than only the actions selected by the supervisor), this supplementary workload on the teacher is not desired. The algorithm needs to adapt the rate of suggestion to the pace of the interaction not to overload the teacher.

A last limit of \gls{sparc} is the interface between the learning algorithm and the teacher. Other \gls{iml} methods using only feedback require a single scalar to evaluate actions. On the other hand, to provide full control and accountability on the robot's actions, \gls{sparc} requires a way to inform the teacher about the robot intention and allow them to pre-empt actions. Furthermore, the teacher needs to be able demonstrate any action the robot should do, they require a way to transmit any action to the robot. As in some applications, the robot has access to hundreds of actions, assigning a button per action is not feasible, other ways of commanding the robot need to be found. Furthermore, as mentioned in Chapter \ref{chap:sparc}, the teacher can also provide additional information to the learning algorithm to speed up the learning. In summary, the interface needs to provide the teacher with the robot's intention, allow them to pre-empt actions, select any action and provide additional information to the learning algorithm. An interface providing all these features can be bloated and difficult to use by the teacher, increasing even more the workload. For applying \gls{sparc} to complex environment, efforts need to be invested in the interface to make it intuitive and clear. In Chapter \ref{chap:tutoring}, the \gls{gui} used by the teacher represents the game, with some buttons for a sub part of the action set, but the majority of actions is inferred by the way the teacher move items on the screen, and which items have been selected as relevant. An alternative way could be to use natural language. Humans are expert in using natural language to communicate and the open-endedness of this tool makes it suited for applications of \gls{sparc} where the teacher can speak and the time required to vocalise the commands is not critical.

\subsection{Experimental Limitations} \label{sec:disc_experiments} 
%Might just go in each parts

Similarly to the method proposed in the research, the study evaluating \gls{sparc} present some limitation. 

The first study presented in Chapter \ref{chap:woz} used participants familiar with robots. One could argue that this population is not representative of the general population expected to interact with robot. However, and as explained in that chapter, in many case in \gls{hri}, the wizards teleoperating robots are trained experts knowing the robot used in the interaction and how to control it. As such, the participants involved in that study represent a significant part of the population expected to supervise a robot. Additionally, the small sample of the study (N=10 for a within design) explains why no significance testing have been done in the study. But valuable information have still been collected in that study, and helped us to design the two other evaluation of \gls{sparc}. A last potential limitation of the evaluation relates to the discretisation of time. In that case, the \gls{woz} condition was not a real \gls{woz} as the participant needed to select an action (or accept a random action) at each step. However, this made the two conditions more comparable and the amount of workload on the supervisor in the \gls{woz} condition should be similar to a real \gls{woz}.

The second study, presented in Chapter \ref{chap:control} aimed to address some limitation of the first one by using a larger sample (N=40) and drawing participants from a more general population non-expert in robotics or \gls{ml}. However by using the tool provided by the university, a majority of the participants were students from the university, and as such it produces another bias of the participants. However, as robots are by essence technological tools, they will probably be mostly from people used to technology, and as such a student in university might be a good representation of the population expected to use robots. Another limit of the study is that it did not exactly replicate the study presented in \cite{thomaz2008teachable}, due to the absence of access to the source code, the experiment had to be reimplemented, leading to differences in the experimental setup between the original paper and our. Additionally, this study compared \gls{sparc} only to a single other method used in \gls{iml}, other methods exist and could provide additional comparisons between \gls{sparc} and other approaches. However, while \gls{sparc} has only be compared with one approach and differences remained between our implementation and the original one, the results obtained by the study still provide us with useful insight about how \gls{sparc} could be combined with \gls{rl} and how the amount of control provided to the teacher impacts the teaching process.

The last study presented in Chapter \ref{chap:tutoring} evaluated how \gls{sparc} could be applied to teach robot to interact socially with children. In this study, the robot's teacher was a single human following the robot through many teaching sessions. Furthermore, this supervisor was a PhD student in psychology and not a trained teacher for children. As such, there might be a potential bias of her behaviour on the success of the teaching process. However, she was not knowledgeable of \gls{ml} and was no aware of the type of algorithm used in the study or the inputs the algorithm used for the learning. As such, she possess many similarities with the type of users with the potential of using robots in education contexts. 
children from different schools and game not ideal

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Ethical Questions} \label{sec:disc_ethics}

Having robots interacting in human environment and teaching them to behave raises multiple ethical questions.

A first ethical questions arising when deploying robots in human environment and expecting them to interact socially concerns people's jobs. Throughout robot and machines history, many jobs have been automated and more are expected to disappear in the next years \citep{frey2017future}. As such deploying robots in social environment, such as education, might lead teacher or social worker to fear for their jobs. However, in many of such social environments with no direct visible return on investment, workforce is already lacking (e.g. nurses in the US; \citealt{nevidjon2001nursing}) and this shortage is expected to grow in the future. As such, roboticists aim not to replace a workforce already in shortage of workers, but on the other end to support them in their job, making it safer and more pleasant for the clinicians or providing additional support for the patients \citep{belpaeme2012multimodal}. However, the community as a whole needs to be aware of these fears and ensure by their work that robots have a positive impact on society.

As robot interact with the general population, and especially learn from and about them, issues concerning privacy arise. To have meaningful interaction with users, robots need to collect information about them, and the type of information collected, the storage and the use by third parties in important. This effect is further increased when robots learn from humans, they are not anymore passively collecting data, but the interaction itself aims to gather more information about the user, about their preferences, their desires and their needs. The question of sharing these information and this knowledge between robots and beyond: to the manufacturer or to the governments needs to be addressed.

A last concerns resides in the responsibility for actions executed by the robot. In a mixed-initiative interaction when both the autonomous agent and the human supervisor can impact the robot action policy, the responsibility of actions is complex to analyse. This effect is even increased when the robot can learn from its user. In that case, the role of the company or the entity distributing this robot and the role of the user have to be considered when looking for a legal entity accountable for the robot actions. \gls{ai} applied for robotics would need to be more transparent for accountability to be clearer \citep{wachter2017transparent}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Impact}

How can sparc help hri/be used in the world

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Future Work}

The work conducted in the thesis explored how robot could be taught to interact with humans and proposed and evaluated through three study an novel interaction framework, \gls{sparc}, to enable such a learning. However, \gls{sparc} and its evaluation could be extended in many ways.

\subsection{Application Domains}

Through this thesis, \gls{sparc} has only been applied to tutor robots, to teach them a social and technical action policy. However, the principles underlying \gls{sparc} could be applied to a much wider ranges of application in robotics and other \gls{ai}. In robotics, and especially \gls{hri}, \gls{sparc} would show promises in numerous fields of social \gls{hri} from assistant robot at home to collaborative robotics including robots in hospitality of military. For example a robot could learn the preferences of a user and act as an embodied personal assistant, connected to  devices in the house, calendar on the internet and supporting its users in routine tasks. Such a robot could learn to anticipate its user's need and propose to provide support proactively. In \gls{hrc}, similarly to the work presented in \cite{munzer2017efficient}, a robot could learn its partner preference, informing them of its actions and help them to complete the task faster. Similarly to work presented in \cite{feil2005defining}, \gls{iml} approaches, and as such \gls{sparc} could be applied to classification tasks, maintaining the user informed about the state of the algorithm's knowledge and involving them in the learning process. For example, for image classification, the algorithm could automatically present a subset of classified images to the user who could step in when a misclassification happen. In this case, where incorrect actions have limited impacts, the correction can happen in hindsight, as proposed in \cite{chernova2009interactive}. Alternatively, the principles of \gls{sparc} could be also used to support agents using \gls{rl} in the real world. The supervisor could provide a safeguard preventing the agent to make errors, bringing it back to a correct action policy or guiding the agent to relevant actions or features in complex environments.

%Mention autonomous driving?

\subsection{Learning Beyond Imitation}

Another potential feature of \gls{sparc}, and other methods based on demonstrations, not evaluated in this research is reaching capabilities beyond the demonstrations. \gls{sparc} uses demonstrations and corrections from a human teacher to learn. By applying \gls{sl}, the optimal outcome would be to match the performance of the teacher. However, if the algorithm learn a value function or the teacher's goal, instead of reproducing the teacher policy, the agent could learn around the demonstrated policy and potentially become better than the teacher themselves. This capability has been reached in \cite{abbeel2004apprenticeship}, by using Inverse Reinforcement Learning. The agent learns a reward function around human demonstrations and then, by applying \gls{rl} around the demonstrated policy, the agent can improve beyond the demonstration and reach super-human capabilities. Similarly to their methods, using the shared control provided by \gls{sparc}, a robot could interact under supervision, progressively proposing action to the supervisor, the teacher could still direct the exploration, preventing the robot to reach dangerous parts of the world, while being open to better propositions than the ones expected. An example could be an agent learning go under supervision, which could propose moves to a human player. That way, the agent could propose moves the human would not have thought about, and then the human could let them be tried or if they appear to risky, override them. 

However, to reach these super-human behaviours, the agents requires a way to learn in addition to the human. Learning only from the human would not be sufficient to improve the policy beyond the one demonstrated. The agent needs to have access to a second level learning, such as a reward function directly from the environment or learnt from the human demonstrations (such as with Inverse Reinforcement Learning). Alternatively, the agent could learn in simulation, only using the human supervision when action in the real world to prevent potential mistakes.

\subsection{Sustained Learning}

In the two first studies presented in this research, the robot learn an action policy from a short term interaction (inferior to 25 minutes). In contrast, robots deployed to interact with humans on a daily basis will learn over much longer periods of time. This aspect, repeated learning, has been partial explored in Chapter \ref{chap:tutoring}, where the robot learns through 25 sessions. However, challenges are still present to learn over the robot all life. For example, the STRANDS project \cite{hawes2017strands}, explores this challenge by taking into account the spaciotemporal representation of the world and using it to select the robot actions.

\subsection{Teacher Interface}

Another axis to improve \gls{sparc} is the interface with the teacher. As mentioned in the previous section, one of the main limitation of \gls{sparc} is also what provide \gls{sparc} its strength: the inclusion of a human in the action selection process. Including this human and giving them the opportunity to pre-empt actions and select any actions come with limits the environments where \gls{sparc} is applicable. The robots needs to communicate its intentions and the human needs enough time to correct them before they impact negatively the environment or the humans interacting with the robot. Despite these limits in time ---

However, the interface used by the teacher to control the robot could mitigate these limitations, and further could explore how to provide the best communication between the teacher and the robot. For example, in the case of a \gls{gui} in a tablet or a phone, the interface can combine buttons and a representation of the world where the robot could describe how it plans to act. Similarly, the teacher could use this representation of the world to select actions for the robot to execute (as used in Chapter \ref{chap:tutoring}). The type of \gls{gui} to control \gls{sparc} could use the knowledge obtained from designing application for phone for example. Alternatively, future work could explore how natural language could be used to control a robot using \gls{sparc}. Many challenges would have to be tackle such as natural language understanding, or creating a string to describe clearly yet concisely, actions the robot intend to execute. Despite this challenges, language possess the qualities required to communicate between a teacher and a robot: high bandwidth, familiarity for humans, openendness of description. As such using combining language and \gls{sparc} is a relevant direction of work...

\subsection{Algorithms}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Summary} \label{sec:disc_summary}

This chapter presented the main limitations identified for \gls{sparc} and the limitations of the studies designed to evaluate it.