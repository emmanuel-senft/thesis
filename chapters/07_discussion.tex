%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Discussion} \label{chap:discussion}
\glsresetall

Chapter \ref{chap:sparc} presented \gls{sparc}, an interactive teaching framework designed to allow robots to learn a social interactive behaviour by begin supervised by humans. Then Chapters \ref{chap:woz}, \ref{chap:control} and \ref{chap:tutoring} evaluated this approach in three studies. The last study evaluated \gls{sparc} in real \gls{hri} and involved 75 children. These chapters seek support for the thesis of this research:

\begin{quote}
	A robot can learn how to interact meaningfully with humans by receiving supervision from a human teacher in control of the robot's behaviour, this supervision will lead to an efficient, safe and low human-workload teaching and autonomous behaviour.	
\end{quote}

This chapters gather results from the different studies presented in the research work to discuss the findings of this research, present the limitations of the approach presented, \gls{sparc} and the evaluation in the three studies. Ethical questions raised with teaching robots to interact with humans will be discussed in Section \ref{sec:disc_ethics}. And finally, the last section will present four axis where \gls{sparc} could be extended to increase its used and application to \gls{hri}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Limitations} \label{sec:disc_limitations}

\subsection{Limitations of SPARC}

\gls{sparc}, as presented in Chapter \ref{chap:sparc} presents several limitations restricting the range of domains it could be applied to.

The first of these limitations is the requirement of a human attentive to the interaction and able to correct the robot's actions before their executions. As stated earlier, \gls{sparc} aims to move away from \gls{woz} or other operation method by learning from the human commands and obtaining an efficient action policy. In its original framing, \gls{sparc} does not aim create a fully autonomous agent behaving without supervision, but to smooth the teaching phase and the use phase in one single interaction where the workload on the supervisor decreases as the robot learns. However, this type of interaction, \gls{sa}, still require a human involved in the supervision and as such presents limited applicability where robots are expected to fill gaps in human workforce to accomplish a task. However, while not being the original goal, \gls{sparc} can still be applied to create a fully autonomous behaviour, as demonstrated in Chapter \ref{chap:tutoring}. While the autonomous behaviour is different from the taught one, it still presents many similarities in the distribution of actions executed by the robot and the children's reaction to this behaviour. More training data and a better learning algorithm should even smoothen this difference of behaviour between the supervised and the autonomous robot. Furthermore, the concepts of \gls{sparc} and the \gls{sa} could still be applied for specific cases where a human supervisor needs to be included in the control process, such as when interacting with children with special needs for example.

A second limitation corresponds the attention of the supervisor. Throughout this research, we assumed that even if the robot behaviour is mostly correct, the supervisor would be attentive to the robot suggestions and ready to correct them at any time. This assumpltion is similarly to autonomous car using safety driver or the `AutoPilot' of Tesla requiring constant human supervision. The agent is fully autonomous but might make mistakes and as such, a human needs to be ready to correct these errors before they impact the world. However, as demonstrated by the few crashes in 2017 and early 2018 involving these partially autonomous vehicles, this assumption is often violated. By observing seemingly correct agent's behaviour, the human supervisor might start to overtrust the agent, missing the occasion to react in time to anticipable errors potentially leading to frustration or death in the case of autonomous vehicles. Some ways exist to potentially mitigate this limitation. For example, the agent could inform more in advance the supervisor about its actions, or expected trajectory. This would provide the supervisor with more time to react and might allow the human react in time, negating this effect. Alternatively, the agent could communicate a lack of confidence in its actions, or interpretation of the environment, informing the supervisor that attention is specially requested in that specific case.

Similarly, as pointed already in Section \ref{ssec:sparc_time}, with \gls{sparc}, the presence of the correction window and the auto execution of actions leads to several issues. To be applicable and make use of the autoexecution of actions as a way to reduce cognitive workload, the validity window of action should be wider than the correction windows. That way, actions approved passively are still valid when executed. Two main effects have different requirements on this correction window. On one hand, to improve the applicability of \gls{sparc}, these corrections windows need to be as narrow as possible. On the other, a correction window too narrow would put additional pressure on the supervisor to react in time. And as observed in Chapter \ref{chap:tutoring}, the supervisor might prefer to cancel every action coming just for not having to face the pressure to react in a short time. Alternatively, the supervisor might also find ways to prevent the suggestions of actions to reduce (as preparing an action in Chapter \ref{chap:tutoring}) this pressure. This limitation is probably on of the main limits of \gls{sparc}, the requirement of providing the teacher with enough time to correct undesired actions impacts the usability of \gls{sparc}. One way of mitigating it, could be to adapt the time window to the type of actions executed or to the algorithm's confidence in the validity of actions. As such, specific actions with a short validity window (such as emergency breaking for autonomous vehicles) could be executed in time; and actions the robot is unsure about would have more time to be corrected. 

Another potential issue is overloading the supervisor with suggested actions. If the supervisor has to correct more actions than it would have selected, the workload is not reduced but increased. While still providing useful information for the learning algorithm (and more than only the actions selected by the supervisor), this supplementary workload on the teacher is not desired. The algorithm needs to adapt the rate of suggestion to the pace of the interaction not to overload the teacher.

A last limit of \gls{sparc} is the interface between the learning algorithm and the teacher. Other \gls{iml} methods using only feedback require a single scalar to evaluate actions. On the other hand, to provide full control and accountability on the robot's actions, \gls{sparc} requires a way to inform the teacher about the robot intention and allow them to pre-empt actions. Furthermore, the teacher needs to be able demonstrate any action the robot should do, they require a way to transmit any action to the robot. As in some applications, the robot has access to hundreds of actions, assigning a button per action is not feasible, other ways of commanding the robot need to be found. Furthermore, as mentioned in Chapter \ref{chap:sparc}, the teacher can also provide additional information to the learning algorithm to speed up the learning. In summary, the interface needs to provide the teacher with the robot's intention, allow them to pre-empt actions, select any action and provide additional information to the learning algorithm. An interface providing all these features can be bloated and difficult to use by the teacher, increasing even more the workload. For applying \gls{sparc} to complex environment, efforts need to be invested in the interface to make it intuitive and clear. In Chapter \ref{chap:tutoring}, the \gls{gui} used by the teacher represents the game, with some buttons for a sub part of the action set, but the majority of actions is inferred by the way the teacher move items on the screen, and which items have been selected as relevant. An alternative way could be to use natural language. Humans are expert in using natural language to communicate and the open-endedness of this tool makes it suited for applications of \gls{sparc} where the teacher can speak and the time required to vocalise the commands is not critical.

\subsection{Experimental limitations} \label{sec:disc_experiments} 
%Might just go in each parts

Similarly to the method proposed in the research, the study evaluating \gls{sparc} present some limitation. 

The first study presented in Chapter \ref{chap:woz} used participants familiar with robots. One could argue that this population is not representative of the general population expected to interact with robot. However, and as explained in that chapter, in many case in \gls{hri}, the wizards teleoperating robots are trained experts knowing the robot used in the interaction and how to control it. As such, the participants involved in that study represent a significant part of the population expected to supervise a robot. Additionally, the small sample of the study (N=10 for a within design) explains why no significance testing have been done in the study. But valuable information have still been collected in that study, and helped us to design the two other evaluation of \gls{sparc}. A last potential limitation of the evaluation relates to the discretisation of time. In that case, the \gls{woz} condition was not a real \gls{woz} as the participant needed to select an action (or accept a random action) at each step. However, this made the two conditions more comparable and the amount of workload on the supervisor in the \gls{woz} condition should be similar to a real \gls{woz}.

The second study, presented in Chapter \ref{chap:control} aimed to address some limitation of the first one by using a larger sample (N=40) and drawing participants from a more general population non-expert in robotics or \gls{ml}. However by using the tool provided by the university, a majority of the participants were students from the university, and as such it produces another bias of the participants. However, as robots are by essence technological tools, they will probably be mostly from people used to technology, and as such a student in university might be a good representation of the population expected to use robots. Another limit of the study is that it did not exactly replicate the study presented in \cite{thomaz2008teachable}, due to the absence of access to the source code, the experiment had to be reimplemented, leading to differences in the experimental setup between the original paper and our. Additionally, this study compared \gls{sparc} only to a single other method used in \gls{iml}, other methods exist and could provide additional comparisons between \gls{sparc} and other approaches. However, while \gls{sparc} has only be compared with one approach and differences remained between our implementation and the original one, the results obtained by the study still provide us with useful insight about how \gls{sparc} could be combined with \gls{rl} and how the amount of control provided to the teacher impacts the teaching process.

The last study presented in Chapter \ref{chap:tutoring} evaluated how \gls{sparc} could be applied to teach robot to interact socially with children. In this study, the robot's teacher was a single human following the robot through many teaching sessions. Furthermore, this supervisor was a PhD student in psychology and not a trained teacher for children. As such, there might be a potential bias of her behaviour on the success of the teaching process. However, she was not knowledgeable of \gls{ml} and was no aware of the type of algorithm used in the study or the inputs the algorithm used for the learning. As such, she possess many similarities with the type of users with the potential of using robots in education contexts. 
children from different schools and game not ideal

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Ethical Questions} \label{sec:disc_ethics}

Having robots interacting in human environment and teaching them to behave raises multiple ethical questions.

A first ethical questions arising when deploying robots in human environment and expecting them to interact socially concerns people's jobs. Throughout robot and machines history, many jobs have been automated and more are expected to disappear in the next years \citep{frey2017future}. As such deploying robots in social environment, such as education, might lead teacher or social worker to fear for their jobs. However, in many of such social environments with no direct visible return on investment, workforce is already lacking (e.g. nurses in the US; \citealt{nevidjon2001nursing}) and this shortage is expected to grow in the future. As such, roboticists aim not to replace a workforce already in shortage of workers, but on the other end to support them in their job, making it safer and more pleasant for the clinicians or providing additional support for the patients \citep{belpaeme2012multimodal}. However, the community as a whole needs to be aware of these fears and ensure by their work that robots have a positive impact on society.

As robot interact with the general population, and especially learn from and about them, issues concerning privacy arise. To have meaningful interaction with users, robots need to collect information about them, and the type of information collected, the storage and the use by third parties in important. This effect is further increased when robots learn from humans, they are not anymore passively collecting data, but the interaction itself aims to gather more information about the user, about their preferences, their desires and their needs. The question of sharing these information and this knowledge between robots and beyond: to the manufacturer or to the governments needs to be addressed.

A last concerns resides in the responsibility for actions executed by the robot. In a mixed-initiative interaction when both the autonomous agent and the human supervisor can impact the robot action policy, the responsibility of actions is complex to analyse. This effect is even increased when the robot can learn from its user. In that case, the role of the company or the entity distributing this robot and the role of the user have to be considered when looking for a legal entity accountable for the robot actions. \gls{ai} applied for robotics would need to be more transparent for accountability to be clearer \citep{wachter2017transparent}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Future Work}

\subsection{Application domains}

\subsection{Learning beyond imitation}

\subsection{Correction window}

\subsection{Sustained learning}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Summary} \label{sec:disc_summary}
