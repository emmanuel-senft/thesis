%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Discussion} \label{chap:discussion}
\glsresetall

Chapter~\ref{chap:background} put the light on the absence of robot controllers today providing adaptivity to a robot with a low workload from humans and while ensuring that the robot's behaviour is constantly appropriate. Based on this observation, Chapter~\ref{chap:sparc} presented the \gls{sparc}, an interactive teaching framework designed to allow robots to learn a social interactive behaviour by being supervised by humans. Then Chapters~\ref{chap:woz},~\ref{chap:control} and~\ref{chap:tutoring} evaluated this approach in three studies, the last of which evaluated \gls{sparc} in real \gls{hri} consisting of a learning activity involving 75 children. Combined together, these chapters sought support for the thesis of this research:

\begin{quote}
	A robot can learn to interact meaningfully with humans in an efficient and safe way by receiving supervision from a human teacher in control of the robot's behaviour.
\end{quote}

This chapter combines the results from the different studies presented in this research to discuss the findings. It starts by presenting the limitations of the approach proposed in this work, \gls{sparc}. Then, Section~\ref{sec:disc_rq} presents how the three studies executed in this research work answered the research questions raised in Chapter~\ref{chap:intro}. Section~\ref{sec:disc_impact} discusses the more general impacts this research may have and the ethical questions raised by teaching robots to interact with humans.
%ethical questions raised when humans teach robots to interact with humans will be discussed in Section~\ref{sec:disc_ethics}, and Section~\ref{sec:disc_impact} will presents the potential impacts of this research. 
Finally, the last section proposes axes where \gls{sparc} could be extended to increase our knowledge in how humans teach robots, and improve \gls{sparc}'s usability and application to \gls{hri} and other fields.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\section{Limitations} \label{sec:disc_limitations}

\section{Limitations of SPARC} \label{sec:disc_limitations}

\gls{sparc}, as presented in Chapter~\ref{chap:sparc}, is a promising framework to teach robots to interact with humans or in high-stakes environments, but presents several limitations restricting the range of domains it could be applied to.

\subsection{Requirement of a Human in the Loop}
The first of these limitations is the requirement of a human to supervise the robot, even once it has learn an action policy. As stated earlier, \gls{sparc} aims to move away from \gls{woz} or other teleoperation methods by learning an efficient action policy from the human commands. In its original framing, \gls{sparc} did not aim to create a fully autonomous agent behaving without supervision, but to smooth the teaching phase and the use phase into one single interaction using \gls{sa}. In this single phase, the workload on the supervisor would decrease as the robot learns while a high performance in the domain application would be maintained due to the control provided to the teacher. However, \gls{sa} still requires a human involved in the supervision and as such presents limited applicability to fields where robots are expected to fill gaps in human workforce or to reduce the requirements on humans. 

Despite not being the original goal, \gls{sparc} can still be used to create a fully autonomous behaviour (as demonstrated in Chapter~\ref{chap:tutoring}). In that case, the training process would be similar to \gls{lfd}, with a training phase using \gls{sparc} and then the testing/deployment phase of the autonomous behaviour. However, by using \gls{sparc} differences remain. First, during the training phase, instead of passively receiving commands from the teacher, the robot would proactively make suggestions to the teacher. As presented in Section~\ref{sec:tutoring_opportunities} this aims to reduce the workload on the teacher during the training phase, provide more datapoints for the learning and inform the teacher about the state of robot's knowledge, potentially creating trust between the robot and its teacher. Second, even after being deployed to interact autonomously, the teacher could still step back in control using \gls{sparc} again to refine the action policy. Alternatively, if a different behaviour has to be applied (e.g. if the robots interact with a child with special needs rather than a typical one), the teacher can take over using \gls{sa} to ensure a personalised experience for this specific interaction.

\subsection{Reliance on Human's Attention}
%\ES{reread}
A second limitation of \gls{sparc} lies in the constant need for the human's attention and the presupposition that human teachers will always ensure an appropriate robot behaviour if given the opportunity. Throughout this research, we assumed that even if the robot behaviour may be mostly correct, the supervisor would be attentive to the robot suggestions and ready to correct any error at any time. This assumption is similar to autonomous cars using a safety driver or Tesla Autopilot requiring continuous human supervision. The agent is fully autonomous but may make mistakes and as such, a human needs to be ready to correct these errors before they impact the world. However, as demonstrated by the accidents in 2017 and early 2018 involving these supervised autonomous vehicles, this assumption is often violated, and a short moment of inattention may have dire consequence\footnote{Cf. the Tempe and Mountain view accident reported by the media in early 2018.\label{foot:disc_danger}}. By observing a seemingly correct agent behaviour for an extended period of time, the human supervisor might start to overtrust the agent, missing the occasion to react in time to anticipable errors potentially leading to frustration or death in the case of autonomous vehicles\footref{foot:disc_danger}. Nevertheless, some ways exist to mitigate this limitation but have not been applied to autonomous driving or general \gls{iml}. For example, with \gls{sa} the agent informs in advance the supervisor about its actions, similarly a car could display the planned trajectory on a screen or in augmented reality. This would provide the supervisor with more time to analyse the situation potentially allowing them to react in a timely fashion. Alternatively, the agent could communicate a lack of confidence in its actions or its interpretation of the environment, informing the supervisor that attention is specially required in that moment.

\subsection{Time Pressure}
%\ES{check}
As pointed already in Section~\ref{ssec:sparc_time}, with \gls{sparc}, the presence of the \gls{cw} and the auto-execution of actions may lead to issues. To be applicable and make use of the auto-execution of actions as a way to reduce workload, the \gls{vw} of an action needs to be wider than its \gls{cw}. That way, actions approved passively are still valid when executed. To increase the application of \gls{sparc} to a wider range of situations, the \gls{cw} has to be as narrow as possible. In contrast, the supervisor needs a \gls{cw} as wide as possible, to provide them with enough time to process the action and cancel it if required. Correction windows too narrow would put additional pressure on the supervisor to react in time or even prevent them to avert undesired actions. %To cope with this additional time pressure, the supervisor might find ways to prevent the suggestions of actions or simply refuse each action proposed by the robot. 
This results in two effects having opposite requirements on the \gls{cw}. However, while a longer \gls{cw} would only limits \gls{sparc}'s applicability in some situations, one too short could have negative consequences. As such, this need of a \gls{cw} wide enough to allow the teacher to react is probably on of the main limits of \gls{sparc} as it produces a significant delay in the robot's actions and reduces the range of domains \gls{sparc} can be applied to.

%\ES{reread}
However, this requirement of a \gls{cw} wide enough for the teacher can be mitigated in multiple ways. For example, instead of communicating the action the robot is directly about to do, the robot could communicate a plan with multiple steps announced in advances. That way, the teacher would be informed beforehand of the next few steps and could anticipate their impact and react to any future actions instead of limiting their evaluation to the next one. Alternatively, the robot could adapt the length of the \gls{cw} to its confidence in the proposed action; for instance, an action with a low confidence would be given more time to be corrected. Likewise, each type of action could have a dedicated value for the \gls{cw}, for example actions needing to be executed quickly (such as emergency breaking for example) would have a much shorter \gls{cw} than other actions with less time constraints. Finally, a last possibility could be to allow the teacher to manually select the duration of this \gls{cw}, consequently letting them be in control of the pace of the interaction. In that case, the teacher might prefer to start with a long \gls{cw} and make a limited use of the auto-execution in the early phases of the interaction to be able to focus on the teaching process; but in later stage of the interaction, they might reduce this \gls{cw} to profit for the auto-execution of actions and reduce their workload.


\subsection{Overloading the Teacher}

By giving an active role to the robot in the teaching process (through proposing actions), \gls{sparc} requires from the teacher to  monitor simultaneously two autonomous agents: the human target and the robot. This requirement might lead to another risk with \gls{sparc}: overloading the teacher with suggested actions. If the teacher has to correct more actions than they would have selected, their workload would not be reduced but increased. While still providing useful information for the learning algorithm (and more than only the actions selected by the supervisor), this supplementary workload on the teacher is not desired. As explained before, this could lead to erroneous teacher's behaviours hindering the learning and potentially increasing the risks in the interaction. For example at some points in the study presented in Chapter~\ref{chap:tutoring}, the teacher just cancelled actions as soon as they arrived, even before she had time to evaluate them. While reducing the workload on the teacher by not requiring them to evaluate the proposed action, this behaviour might limit the efficiency of the learning algorithm by giving it incorrectly labelled datapoints. 

Overloading the teacher is a serious issue and might have negative consequences both on the robot's learning and the experience of the human involved in the application interaction. As such, mechanisms must be present to ensure that this does not happen. The learning algorithm needs to have the right balance between suggestions and waiting periods to allow the teacher to analyse and provide a correct evaluation of the proposed actions. Alternatively, the teacher could be provided a direct way to impact on this suggestions and on the auto-execution of actions to give them control over their workload and allow them to teach the robot efficiently.

\subsection{Interface}\label{sec:disc_lim_interface}

The interface between the teacher and the robot is key when applying \gls{sparc} or other \gls{iml} methods. Simple interfaces can be easy to create and used by the teachers, however they might only have limited efficiency in the learning process. For instance, approaches using only feedback require a single one-way communication channel between the teacher and the robot, and this channel only needs to send a scalar evaluation of the agent's actions. Consequently, both the design of the interface for the teacher and the communication are simple but their efficiency is limited. On the other hand, to provide full control and accountability over the robot's actions, \gls{sparc} requires both ways communication. First, the teacher needs to receive inputs from the robot, such as its intentions, to decide if the action is valid or not. Second, the teacher needs to send information to the robot: feedback about the intentions, to preempt actions if required, but also demonstrations by selecting actions for the robot to execute. As the robot may have access to hundreds of actions, assigning one button per action is not feasible, other ways of commanding the robot need to be found. Furthermore, as mentioned in Chapter~\ref{chap:sparc}, with \gls{sparc} the teacher can also provide additional information to the algorithm to speed up the learning. In summary, the interface between the robot and the teacher needs to provide the teacher with the robot's intentions and allow the teacher to preempt actions, select any action and provide additional information to the learning algorithm. An interface providing all these features can easily be bloated and difficult to use by humans, increasing even more the required workload to control and teach the robot. As such, for applying \gls{sparc} to complex environments, efforts need to be invested in the interface to make it intuitive and clear. 

For instance, in Chapter~\ref{chap:tutoring}, the \gls{gui} used by the teacher represented the current state of the game, with some buttons for accessing a subpart of the action space, but the majority of actions was inferred by the way the teacher moved items on the screen and the items selected. An alternative way could be to use natural language. Humans are experts in using natural language to communicate and the open-endedness of this tool makes it suited for applications of \gls{sparc} where the teacher can speak and where the time required to vocalise commands is not critical, such as a robot assistant at home.

\subsection{Learning with Humans}

%As justified in Chapter~\ref{chap:background}, today, there is no simulator of humans and social norms precise enough to be used to train robot to interact with humans. Consequently, learning for \gls{hri} has to happen in the real world. In addition to this \emph{space} constraint, learning with humans also adds a \emph{time} constraint. B

\gls{sparc} has been designed to enable agents to learn safely in situ for high-stakes environments where no simulator can be used to learn in a virtual world (e.g. \gls{hri}). However, learning in situ for interaction with humans adds a serious time constraint: by including a human in the learning interaction, this interaction needs to go at a human pace. This implies that gathering datapoints through \gls{sparc} is a relatively slow process. And recent progress \gls{ai} being linked to the availability of large datasets or the ease to collect large amount of data\footnote{For instance ImageNet contains more than 14 millions of images~\citep{russakovsky2015imagenet} and \gls{rl} methods still rely on millions or more of datapoints (cf. Section \ref{ssec:back_rl})}, many recent algorithms are not applicable when learning online in the real world. As such, algorithms using \gls{sparc} need to be data efficient, be able to learn from a low number of datapoint and generalise quickly. However, as mentioned previously, by including the human in the loop and learning in the real world, we have access important additional features. First, the points accumulated should be relevant to the learning process: they come from the desired interaction and as they have been validated by a human, their label should be correct. And secondly, the human can also provide additional information on the selection to quicken the learning (as implemented in Chapter \ref{chap:tutoring}). This deeper uses of the human teacher might help to mitigate the limited number of datapoints available compared to other types of learning frameworks.

%\subsection{Experimental Limitations} \label{sec:disc_experiments} 
%\ES{Need to be sure that all these limitations are mentioned in the corresponding parts}
%Similarly to the method proposed in this research, the studies evaluating \gls{sparc} also presented some limitation. 

%The first study presented in Chapter~\ref{chap:woz} used participants familiar with robots. One could argue that this population is not representative of the general population expected to interact with robots. However, and as explained in that chapter, in many case in \gls{hri}, the wizards teleoperating robots are trained experts knowing the robot used in the interaction and how to control it. Thus, the participants involved in that study are representative of a significant part of the population expected to supervise a robot to interact with other humans. Additionally, the small sample of the study (N=10 for a within participants design) explains why no significance testing have been done in the study. However, valuable information have been collected and helped us to design the two other evaluation of \gls{sparc}. A last potential limitation of the evaluation relates to the discretisation of time. In this study, the \gls{woz} condition was not a real \gls{woz} as the participants needed to select an action (or accept a random action) at each step. However, this made the two conditions more comparable and the amount of workload on the supervisor in the \gls{woz} condition should have been similar to a real \gls{woz}.
%\ES{low number of datapoint and slower interaction}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Research Questions} \label{sec:disc_rq}

This section will revisit the research questions identified in Section~\ref{sec:intro_thesis} and explain how the work presented in this thesis addressed them.
\begin{itemize}
\item [RQ1] \textbf{What are the requirements of a robot controller to ensure a behaviour suited to \gls{hri}?} 
Based on a review of the different fields of application of social \gls{hri}, we defined three requirements a robot controller should follow to ensure an efficient interaction. First and foremost, the robot's behaviour needs to be constantly appropriate: as robots often interact with vulnerable populations, their behaviour needs to be constantly safe for the humans they interact with. Secondly, the robot should be adaptive, be able to generalise to unexpected situations, but also personalise its behaviour to the different humans it interacts with and be able to learn, improving and extending its action policy. Finally, the robot needs to be as autonomous as possible, or at least require a low human workload to interact in the world. This implies that robots needs to find ways to learn about their environment and reach autonomy without relying on random exploration as this would violate the first principle. We think the robotic community, and especially \gls{hri}, should strive toward more autonomous robots, and could take a stronger inspiration from \gls{iml} as it shows strong promises for teaching robots and could enable them to learn complex tasks such as interacting with humans.

\item [RQ2] \textbf{What interaction framework would allow a human to teach a robot while validating the requirements from RQ1?}
To validate the three principles expressed as answer to RQ1, we proposed \gls{sparc}, a new teaching framework for robots which provides control over the robot's actions to a teacher and use this control to learn in a safe way, validating the first and second requirements. Secondly, by allowing the teacher to passively accept the robot's propositions, we aim to decrease the workload on the teacher over time and progressively provide the robot with autonomy. By taking inspiration from \gls{iml}, \gls{sparc} aims to fill a void in the \gls{hri} research: online learning for interaction with humans.

\item [RQ3] \textbf{Could a robot decrease its supervisor's workload by learning from their supervision?}
Study 1 showed that providing a supervised robot with learning can reduce the workload on its supervisor. Furthermore, study 2 demonstrated that, compared to other methods, \gls{sparc} is an efficient way to enable a safe teaching and requires a comparatively low workload. Similarly to methods from \gls{lfd}~\citep{liu2014train,sequeira2016discovering}, \gls{sparc} provides an alternative to \gls{woz} and opens new opportunities to have robots displaying social behaviours after observing humans demonstrations. However, unlike classic \gls{lfd} methods, by including an online learning component, \gls{sparc} could produces results supporting the teacher during the learning process, inform them about the robot knowledge and potentially create trust between the robot and its teacher.

\item [RQ4] \textbf{How does providing the teacher with control over the learner's actions impacts the teaching process?} 
Results from study 1 and 2 indicated that by informing the teacher in advance of its actions, the robot ensures that its final behaviour is correct. This implies that even in early phases of the learning, when the robot behaviour is not adequate yet, the teacher can prevent the robot's lack of knowledge to negatively impact the world. Furthermore, this control helps the teacher to steer the robot toward useful parts of the environment and demonstrate it a good policy, making the teaching faster, safer, more efficient and lighter (as requiring a lower workload) than other methods lacking control. This finding is especially relevant to \gls{iml}, as often human teachers are provided limited control over the robot's action policy~\citep{thomaz2008teachable,knox2009interactively}. While being a challenge, providing the teacher with this control can have significant positive results on the learning progress.

\item [RQ5] \textbf{How does teaching a robot to interact socially impacts the two humans involved in the overall triadic interaction?}
When being used to teach a robot to interact with a human, \gls{sparc} does allow the robot to constantly have an efficient behaviour (as demonstrated by the improvement of children's behaviours in the supervised condition in Chapter~\ref{chap:tutoring}). However, this performance in the application interaction might come with a cost for the teaching interaction. As the teacher needs to react to the robot's suggestions, they also have to monitor a second autonomous agent, which might lead to a heavier workload than classic teleoperation methods. This is one of the drawbacks of \gls{sparc} compared to methods based on \gls{lfd} to gather information, but as mentioned earlier, ways exist to mitigate this issue and allow this interaction between the teacher and the robot to lead to positive results for the teacher.

\item [RQ6] \textbf{After receiving supervision from a human, could a robot behave autonomously in a social context?}
In Chapter~\ref{chap:tutoring}, we used \gls{sparc} to teach the robot in the supervised condition, and then we deployed the robot to interact autonomously. During this autonomous interaction, the robot applied a policy similar to the demonstrated one, and the impact on the children's behaviour was close to the one in the supervised condition. Consequently, in this study, the robot could behave socially in an autonomous fashion in a complex and multimodal environment after having been supervised by a human in a learning phase. This finding is one of the most important of the thesis as it demonstrates the potential of \gls{sparc} to teach robot complex social autonomous behaviours.

\end{itemize}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Impact} \label{sec:disc_impact}

\subsection{Pushing the State of the Art}

Before the start of this research work or even during its realisation, few other methods have been applied to teach robot to interact with humans. As mentioned in Chapter~\ref{chap:background}, the main other approaches were \gls{lfw} and \gls{lfd}, they used demonstrations from a \gls{woz} setup or interactions between humans and from these demonstrations learned offline an action policy~\citep{knox2014learning,liu2014train,sequeira2016discovering}. Due to the challenges when learning during the interaction (high stakes of actions, limited datapoints and complexity to maintain a policy providing useful data), online learning and \gls{iml} were seldom used to teach robot to interact with humans, these approaches were mostly focused on teaching agents to interact in other non-social environments. Additionally, most of the previous methods in \gls{iml} only provided limited control to the human teachers, reducing them to feedback providers while humans could and should provide much more information to learning agents~\citep{amershi2014power}
	
By proposing \gls{sparc}, we wanted to push \gls{iml} to give more power to robot teacher, to make a better use of humans abilities to improve the learning process, making it safer and more comfortable for these human teachers. By keeping a human in control of the robot's actions, \gls{sparc} enables robots to learn online in sensitive environments. %, we pushed the state of the art to new boundaries in teaching robots. 
%With \gls{sparc} we proposed a new way to teach agents. By following the principles presented in Section~\ref{sec:sparc_principles} (combining \gls{sa} and \gls{ml}), this method provides a robot with an adaptive policy while ensuring that its behaviour remains constantly appropriate. 
Furthermore, with the combination of proposition, correction and selection of actions, \gls{sparc} has the opportunity to reduce the workload on the teacher. Hence, this approach is fit to control and teach robots to interact with humans as it follows the requirements presented in Section~\ref{ssec:back_constraints}. With this method, we successfully demonstrated in Chapter~\ref{chap:tutoring} that robots can be taught online to interact safely and efficiently with humans, while ensuring a a constantly appropriate action policy. 

By demonstrating its applicability to teach robots to interact with humans from in-situ supervision, \gls{sparc} pushes the state of the art both in \gls{hri} and \gls{iml}. Moreover, it opens up new opportunities to provide robots with action policies complex to define or not known in advance. This new method to teach robot safely to interact with humans might allow robots to be deployed in new contexts where they are absent today.

%\ES{presents goods and bads: advantages of both approaches, learning offline and online}
%; therefore, with \gls{sparc} the resulting autonomous behaviour can be adapted to each situation and represents the teacher's desired policy. %\gls{sparc} can be a way to allow non-experts in robotics to personalise their robot, to have it learn to behave in the way they each desire.

\subsection{Empowering Non-Experts in Robotics}

%\ES{other methods require a lot of engineering between the demonstrations and the applications, sparc smoothes the process, allowing the engineering work to happen before, hence allow people to teach their robot themselves}

Other methods used to teach robot to interact with humans use post processing on previously gathered demonstrations to learn offline an action policy. While leading to positive results, these approaches require significant engineering work after the demonstrations have been recorded to create a robot behaviour and provide limited opportunities to refine the behaviour after the learning is over~\citep{liu2014train,sequeira2016discovering}. This implies that they cannot be used solely by end-users not knowledgeable in machine learning, experts in robotics have to interpret the data provided by the domains expert to design the behaviour. In contrast, by mixing together the collection of data and the learning, \gls{sparc} removes this barrier between data collection and use. That way, end-users can directly teach themselves a robot to interact as they desire.
%By allowing end-users non-expert in robotics to teach a robot how to interact, 
\gls{sparc} provides an opportunity for anyone to personalise their robot without requiring technical skills. As demonstrated in Chapter~\ref{chap:woz} and \ref{chap:control}, from a single algorithm and state representation, \gls{sparc} can lead to different behaviours adapted to the teacher's strategy and preferences. Combined with efficient interfaces and learning algorithms, approaches such as \gls{sparc} have the potential to democratise robotics by allowing anyone to teach a robot to interact efficiently in a wide range of domains. Robot developers and designers could use this learning ability to deploy robots as \emph{blank slates}, with just a way to perceive the world, act on it and interact with a teacher, and let their behaviour be defined by their users. These users would start filling these blank slates, creating their own robot behaviour, teaching their robot how to fulfil their personal needs. As defended by \cite{fails2003interactive} and \cite{amershi2014power}, by allowing end-users to teach an agent to behave as they desire, \gls{iml} methods have the potential to ease the deployments of technology and reach faster new application domains. This might allow users currently excluded from using robots (due to lack of interest from developers and lack of technical skills from the users), to profit from this new technology.

While providing many opportunities, deploying a blank robot able to learn complex action policies would require significant engineering pre-deployment to have a wide enough state and action space and a learning algorithm efficient enough to reach useful policies. However, as demonstrated by the study in Chapter~\ref{chap:tutoring}, this is achievable. A robot can be deployed with large state and action spaces and then using algorithms designed to learn quickly from teachers in complex environment, a non-technical human can teach a robot a complex social policy. Furthermore, by providing additional tools to the users to widen or refine the state and action space, this teaching could be applied to even more applications.

%As different teachers have various preferences, they will have different expectations for the robot's behaviour. And \gls{sparc} could be a way to allow each user to teach their robot a personalised interaction behaviour.


\subsection{Robots and Proactivity}
%\ES{bad}
Another way to interpret \gls{sparc} is as a way to provide proactivity to a robot. By using \gls{ml} and proposing to execute actions, the robot is actually taking the initiative to do an action without executing it straight-away. For instance, a proactive robot assistant would anticipate its user's needs and desires and would propose help or services without having to be asked~\citep{mason2011robot}. This capability has two uses: first it allows robot users not to have to ask a supportive behaviour for the robot every time they need it and second, it means that the robot could even support its user when they do not realise help would be useful.

By having the capability to learn new actions, or what action it should do, such a robot would move from a simple tool to use to an adaptive partner able to support its user in a large quantity of task. Finally by informing the surrounding humans of its actions, such a robot assistant would only execute action deemed useful by their users limiting the risks of negative outcomes.

\subsection{Beyond Human-Robot Interaction}\label{sec:disc_beyond}

Interactions with humans are complex: they happen in large multimodal environments, with high stakes, implying social behaviours and where gathering datapoints is a tedious process. As \gls{sparc} demonstrated its efficiency on such an environment, it should be possible to apply it to many other domains outside of \gls{hri} where constraints on the learning process are lighter. For example, it could be used to teach robot manipulation or navigation task: a simulator could present the expected trajectory leaving the teacher time to correct it if needed. Alternatively, and similarly to other \gls{iml} approaches~\citep{fails2003interactive}, \gls{sparc} could also be applied to classification tasks, maintaining the user informed about the state of the algorithm's knowledge and involving them in the learning process. For example, for semi-supervised image classification, the algorithm could automatically present to the user a subset of classified images between learning steps, and this user could step in when a misclassification happens. In these cases, where incorrect actions have limited impacts, the corrections can happen in hindsight, as proposed in \cite{chernova2009interactive}. Finally, the principles of \gls{sparc} could also be used to support agents using \gls{rl} in the real world. The supervisor could provide a safeguard preventing the agent to make errors, bringing it back to the correct parts of the environment or guiding the agent to the relevant actions in complex environments.

\subsection{Ethical Questions} \label{sec:disc_ethics}
%\ES{add more ref to support the questions}
Having robots interacting in human environment and allowing any human to teach them, raise multiple ethical questions~\citep{lin2014robot}.

The first one concerns people's jobs. Throughout robots and machines history, many jobs have been automated and more are expected to disappear in the next years~\citep{frey2017future}. As such, deploying robots in social environments, such as schools or care facilities, might lead teachers or social workers to fear for their jobs. However, in many of such social environments with no direct quantifiable return on investment, workforce is already lacking (e.g. nurses in the US; \citealt{nevidjon2001nursing}) and this shortage is expected to grow in the future. Consequently, robots provide an opportunity not to replace a workforce already in shortage of workers, but on the other hand to support these people in their job, making these jobs safer and more pleasant for the workers and providing additional support for the clients or patients~\citep{wada2005psychological}. However, the \gls{hri} community as a whole needs to be aware of these fears and ensure by their work that robots have a positive impact on society and communicate their vision of robots helping the human population.

By allowing robots to learn, we might increase their range of application, potentially leading to more interactions with vulnerable populations, such as elderly people or children in school. However, having robots interacting in these environments raises multiple questions. \cite{sharkey2012granny} identify that using robots in elder care might ``reduce the amount of human contact'', ``increase the feelings of objectification'', create ``a loss of privacy'' and ``personal liberty'' and elicit ``deception and infantilisation''. They also insist that the conditions were an elderly should be in control of a robot are to be carefully identified. Similarly, \cite{sharkey2016should} expresses concerns about deploying robots in classrooms. As such, roboticists need to work with domain experts to ensure that robots do help their users and have positive impacts where they are deployed.

Another major ethical questions concerning learning robots is privacy. As robots will interact with the general population, and especially will learn from and about individuals, issues concerning privacy arise. To have meaningful interactions with their users, robots need to collect information about them. And the type of information collected, the storage, the use by third parties and the users' perceptions about this collection have to be carefully considered before deploying a robot in the real world~\citep{syrdal2007he}. This effect is further increased when robots learn from humans. These learning robots are not any more passively collecting data, but the interaction itself is designed to gather more information about the user, about their preferences, their desires and their needs. Finally, this effect is even amplified when interacting with vulnerable populations. If robots take an important role in education and care, humans interacting with them will tend to be children or patient, and these people might not be able to ensure their privacy alone. The question of sharing these information and this knowledge between robots and beyond, to the manufacturer or to the governments, needs to be addressed before robots are deployed on large scales. An additional ethical question linked with privacy is security. This data accumulated by robots needs to be protected from malicious attacks. This issue is even more visible with the recent world wide hacks of the Internet of Things devices (such as Mirai\footnote{\url{https://en.wikipedia.org/wiki/Mirai_(malware)}}). This year, \cite{giaretta2018adding} presented a report of numerous basic security flaws in the Pepper robot and expressed the idea that robot have moved too quickly from research to market product and that they often do not present the required security to ensure they users' privacy and security.

A last concern resides in the responsibility for the actions executed by the robot~\citep{asaro2007robots}. In a mixed-initiative interaction when both the autonomous agent and the human supervisor can impact the robot action policy, the responsibility of actions is complex to analyse. This effect is even increased when the robot can learn from its user. In that case, the role of the company or the entity distributing this robot and the role of the user can be hard to define when looking for a legal entity accountable for the robot actions. As argued by \cite{wachter2017transparent}, \gls{ai} applied for robotics needs to be more transparent, this could both help humans to interact and understand robots and provide a clearer accountability for robots' actions.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Future Work}

%\ES{overlap with limitation...}

The work conducted in this thesis explored how robots could be taught to interact with humans and proposed a novel interaction framework, \gls{sparc}, to enable such a learning. However, \gls{sparc} could be extended in many ways and its principles applied to other applications.

\subsection{Application Domains}

Through this thesis, \gls{sparc} has only been applied to \gls{hri} in the context of tutor robots, to teach a robot to support child learning. However, the principles underlying \gls{sparc} could be applied to a much wider range of applications in \gls{hri} and other in other fields (cf. Section~\ref{sec:disc_beyond}). For instance, \gls{sparc} would show promises in numerous applications in social \gls{hri}: from assistant robot at home to collaborative robotics including robots in hospitality, military or industry. For example a robot could learn the preferences of a user and act as an embodied personal assistant, connected to devices in the house, calendar on the internet and supporting its users in routine tasks. Such a robot could learn to anticipate its user's needs and propose to provide support proactively. In \gls{hrc}, similarly to the work presented in \cite{munzer2017efficient}, a robot could learn its partner preferences, informing them of its actions and helping them to complete the task faster and easier. As such, future work should apply \gls{sparc} to other use cases of \gls{hri} and explore how it could be used to teach agents to interact in other complex or high-stakes environments.

%\ref{sec:disc_beyond}
%\subsection{Multiple Teachers}

\subsection{Learning Beyond Imitation}\label{sec:disc_beyond}

Another potential feature of \gls{sparc}, and other methods based on demonstrations, not evaluated in this research is reaching capabilities beyond the demonstrations. As \gls{sparc} uses demonstrations and corrections from a human teacher to learn, by applying \gls{sl}, the optimal outcome would be to match the teacher's performance. However, if the algorithm possesses or learns a value function or the teacher's goal, instead of reproducing the teacher policy, the agent could improve its action policy around the demonstrated one and potentially become better than the teachers themselves. This achievement have been accomplished by \cite{abbeel2004apprenticeship}, by using Inverse Reinforcement Learning. In their work, the agent learned a reward function and a basic action policy from human demonstrations and then, by applying \gls{rl} around the demonstrated policy, the agent improved its behaviour beyond the demonstrations reaching super-human capabilities. 

Alternatively, instead of having the robot reaching capabilities beyond human ones on its own, a human-robot team could also together reach this kind of performance. For examples, \cite{kasparov2010chess} proposed ``Advance Chess'', a new type of chess were players have access to a computer to help them during their decisions. This combination of human and machine, aims at profiting from the best of both worlds and would allow humans to make better use of their intuition and creativity while using computer's certainty to save human calculations. Similarly, a learning agent could interact with the human in a mixed initiative framework, such as \gls{sparc}, where the agent could suggest actions (such as moves in a Go or Chess game) and the human could accept them or refuse them. That way the human would be in control of the interaction, preventing potential errors from the artificial agent to have negative impact, while still being open to opportunities they did not anticipate. Hence, the team could reach together super-human capabilities, while ensuring with the presence of the human in control of the interaction that the behaviour would be at least of human performance. Additionally, this mixed initiative interaction might provide the human with the opportunity to learn from the robot, if the robot proposes better than anticipated actions.

%Similarly to their methods, using the shared control provided by \gls{sparc}, a robot could interact under supervision, progressively proposing action to the supervisor. And the teacher could still direct the exploration, preventing the robot to reach dangerous parts of the world, while being open to better propositions than the ones expected.

%An example could be an agent learning a game such as Go with a combination of exploration in simulation and action in the real world under supervision. The agent could optimise the policy offline and propose moves to the human. Based on these suggestions, the human could correct them or accept them. That way, the agent could propose moves the human would not have thought about, and then the human could let them be tried or if they appear to risky, override them. 

%The approach proposed above goes further by mixing the initiative between both agents and having the computer continuously learning. This mixing of optimisation in simulation and actions under supervision in the real world could lead to efficient learning while ensuring that the executed behaviours are correct and be applied to a wide range of other problems such as navigation, manipulation or \gls{hri}. This could even further and potentially allow humans to learn online from the autonomous agent. %Alternatively, the agent could learn in simulation, only using the human supervision when acting in the real world to prevent potential mistakes.

However, to reach these super-human behaviours, the agent requires a way to learn in addition to the human. The agent needs to have access to a second level learning, such as a reward function directly from the environment or learned from the human demonstrations (such as with Inverse Reinforcement Learning). For example, the human could provide a mixture of demonstration, rewards and high levels goals which could be used to learn such a reward function or to update a planner with more correct models allowing them to reach the goals faster. Alternatively, the robot could learn simultaneously from the human and in simulation, while leaving the human in control when interacting in the real world and using their feedback during these real interactions to refine the simulation.

\subsection{Sustained Learning}

This work, and most of the general research in robotics, consider the problem of learning single tasks in isolation. However, once deployed, robots need to address the challenges of lifelong learning~\citep{thrun1995lifelong}, being able to continuously learn in different domains and transfer knowledge from one situation to another. Methods such as \gls{sparc} could help framing this learning and potentially helping the robot to know which parts of the policy are transferable.

Another challenge of learning over long periods of time is the action policy's dependency in time. For example, in Chapter~\ref{chap:tutoring}, the temporal aspects of the interaction were taken into account only by including some notions of time since events in the state definition. The robot was not doing any temporal planning or explicitly taking time into account when behaving. However, to sustain continuous long term interaction, spanning multiple hours or days, the dependency in time of the action policy has to be taken into account through other means. One way is to learn spacio temporal features relevant to the human's behaviours and expectations (cf. STRANDS project; \citealt{hawes2017strands}). For instance, a robot assistant at home should know its users are typically going to work every morning, except weekends and holidays, and a human teacher could help the robot to interpret the elements related to time. Finally, to sustain learning over long periods of time, robots also need algorithms that can scale well with a large number of data. 

%In the two first studies presented in this research, the robot learned an action policy from a single short term interaction (inferior to 25 minutes) and in the last study, the robot learned from repeated 
%In contrast, robots deployed to interact with humans on a daily basis will learn over much longer and repeated periods of time. 
%Part of this aspect, repeated learning, has been partially explored in Chapter~\ref{chap:tutoring}, where the robot learned through 25 sessions. While progressing toward repeated learning, this study did not explore the challenges of life long learning~\citep{thrun1995lifelong}. In the implementation of \gls{sparc} in Chapter~\ref{chap:tutoring}, the temporal aspects of the interaction were taken into account only by including some notion of time since events in the state definition. The robot was not doing any temporal planning or explicitly taking time into account when behaving.% and using a feature based algorithm does not scale well with large quantity of data, limiting the efficiency in case of longer or more numerous teaching sessions. 
%To sustain continuous long term interaction, spanning multiple hours or days, the dependency in time of the action policy has to be taken into account through other means. The robot needs to learn spacio temporal features relevant to the human's behaviours and expectations (cf. STRANDS project; \citealt{hawes2017strands}). For instance, a robot assistant at home should know its users are typically going to work every morning, except weekends and holidays. To sustain learning over long periods of time, robots needs algorithm that can scale well with large number of data and take into account the impact of time on the action policy.
%Additionally, such robots should be able to learn from their users, but also when they are absent, as it generally happens a major part of the day, but humans might still have expectations for the robot during that time.

\subsection{Interface With the Teacher}

Another axis to improve \gls{sparc}, and which is critical to reach when tackling new applications, is the interface with the teacher. One of the main limitation of \gls{sparc} is also what provides it its strength: the inclusion of a human in the action selection process. Including this human and giving them the opportunity to preempt and select any actions comes with limits on the interaction. The robots needs to communicate its intentions and the human needs enough time to correct them before they impact negatively the environment or other humans interacting with the robot and the teacher needs to inform which actions the robot should execute. %Despite these limits in time ---

Consequently, the interface used by the teacher to control the robot is key and should mitigate these limitations. Further work should explore how to provide the best communication between the teacher and the robot. For example, in the case of a \gls{gui} on a tablet or a phone, the interface could combine buttons and a representation of the world where the robot could describe how it plans to act or its expected trajectories. Similarly, the teacher could use this representation of the world to select actions for the robot to execute (as used in Chapter~\ref{chap:tutoring} but including long-term information). Designing these \gls{gui} for \gls{sparc} could use the knowledge obtained from designing application for phone for example~\citep{joorabchi2013real}. 

However, \gls{gui} might not be the optimal way to communicate with the robot, as they might scale difficulty with a high number of actions and require additional hardware. Future work should explore alternatives interfaces, such as natural language, to control a robot through \gls{sparc}. This would raise many challenges, such as natural language understanding, or creating verbal commands describing the robot's actions, intentions or explanation clearly yet concisely~\citep{hayes2017improving}. Despite these challenges, language possesses the qualities required to communicate between a teacher and a robot: familiarity for humans, open-endedness of description and precision for example. Another area of research which would improve \gls{sparc} is Brain Computer Interfaces. For example, when witnessing an error, the brain creates a specific pattern which can be detected using EEG~\citep{gehring1993neural}, that way instead of an explicit \gls{cw}, such an interface could automatically detect errors in robot suggestions without requiring the teacher to explicitly cancel the action. By having a much smaller delay between the proposition and its execution, \gls{sparc} could be applied to more applications.

\subsection{Algorithms}

In the future, \gls{sparc} should be combined with richer learning algorithms. The three examples provided in this thesis represent only a small part of the algorithms \gls{sparc} can be combined with. More advanced \gls{ml} would allow \gls{sparc} to learn faster and more efficiently; and, as mentioned previously, potentially reach super-human performance in the task. However, as mentioned in Chapters~\ref{chap:background} and~\ref{chap:sparc}, many challenges remain when using learning for \gls{hri}. The first one, the main one tackled by \gls{sparc}, is the high stakes of the interaction, incorrect errors might lead to disastrous consequences. In addition, the data efficiency is fundamental: when interacting with humans, collecting information about human's reaction can be costly or take a large amount of time. As such, each datapoint should be used with high efficiency and the human included in the loop should provide additional knowledge to deal with this scarcity of data. Alternatively, the algorithm could combine data accumulated from different teachers to learn a more general policy. However, this could lead to a transfer problem, assumptions and policies correct with one user might not be valid anymore for another one. And doing this generalisation might decrease the potential for personalisation that \gls{ml} provide. One way to address this personalisation vs generalisation issue is to group people by similarity and learn to detect the group of a person and an action policy adapted to this group to reach a better action policy~\citep{brunskill2014pac}. Alternatively, the robot could learn or already possess a general action policy and then use \gls{sparc} to refine it and adapt it to its user. 

\gls{sparc} and humans in general could also be used to teach hierarchical strategies \citep{botvinick2012hierarchical}. With hierarchical learning, agents learn subpolicies used to complete subgoals, and then combine these subpolicies to reach higher goals. By helping an agent to create subpolicies and informing it which ones are relevant to specific context, a human could allow an agent to learn to solve complex task much quicker. This teaching on multiple levels presents a challenge for \gls{sparc}, as in the current implementation, it only considers actions one by one and the teacher cannot inform about goals. This would require a way for the teacher to create higher level actions, organise them and switch between them. However, using a human provides a strong potential to quicken the learning of complex and rich policies.

%Additionally, as mentioned earlier, \gls{sparc} could be combined with \gls{rl} in simulation, using the human only to ensure a correct behaviour in the real world, while including the humans commands back in the simulation to improve the learning process.
Additionally, instead of providing demonstrations of a policy to follow, the teacher could also give symbolic rules defining a behaviour for the robot. This alternative way of teaching could generalise faster than simple demonstrations and might allow teachers to define complex behaviours easily and without having to encounter each situations to show the robot how to behave. %However, relying to much on symbolic description of the world might lead to similar issues that the ones faced by expert systems.

Another challenge for algorithms used with humans is the fact that humans are not static entities. As mentioned in Section~\ref{ssec:back_feedback}, different humans will use different teaching strategies. Furthermore, as seen in Chapters~\ref{chap:woz},~\ref{chap:control} and~\ref{chap:tutoring}, in \cite{thomaz2008teachable} and \cite{macglashan2017interactive}, human teachers adapt their teaching strategy overtime. Human policies and feedback are moving targets, and algorithms used to learn from humans need to take into account these variations and evolutions of humans' behaviours.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Summary} \label{sec:disc_summary}

%\ES{I should probably state the things - difference with introduction}

This chapter started by presenting the main limitations identified for \gls{sparc} (requirement of a \gls{cw} and attention from the teacher, potential increase of workload on the teacher, complexity of the interface, low number of datapoint and slower interaction). Each limitation was described and we presented potential ways to address them when designing interactions involving human teachers. We then revisited the research questions identified in Section~\ref{sec:intro_thesis} and presented how this work addressed them. We continued with discussing the potential impacts of \gls{sparc} on the wider fields of \gls{hri} and \gls{iml} and for deploying robots in the real world, as well as the ethical questions raised by having humans teach robots. Finally, we proposed directions to extend the efficiency and applicability of \gls{sparc} that could be addressed in future work.
